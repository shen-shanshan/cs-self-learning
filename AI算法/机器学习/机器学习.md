# 引言

## 1  机器学习的流程

1. 数据收集：
   - 训练数据：图片、声音、文本、数据；
   - 标签数据。
2. 数据清洗（格式化）
3. 特征工程（向量化 -> 特征向量）
4. 数据建模（算法 -> 模型）

## 2  机器学习的分类

- 有监督学习：
  - 回归
  - 分类
- 无监督学习（训练数据没有标签）：
  - 聚类
  - 降维
- 强化学习

## 3  预备知识

- 高等数学
- 线性代数
- 概率论
- Python

# 线性回归

## 1  基本概念

### 1.1  假设函数

假设函数（hypotheses function）：预测函数。
$$
h_{θ} = \sum_{i=0}^{n}θ_{i}x_{i} = θ^Tx
$$

> y = ax + b  ->  h(x):   b -> θ<sub>0</sub>x<sub>0</sub> (x<sub>0</sub> = 1);   a -> θ<sub>1</sub>;   x -> x<sub>1</sub>。

- 当 *X* 为二维时，假设函数为一条直线；
- 当 *X* 为三维时，假设函数为一个平面；
- 当 *X* 大于三维时，假设函数为一个超平面。

### 1.2  损失函数

损失函数（loss function）：预测值（*h*）与实际值（*y*）之间的差距。
$$
L(θ) = (h_{θ}(x)-y)^2
$$

### 1.3  代价函数

代价函数（cost function）：损失函数的均值。
$$
J(θ) = \frac{1}{2m}\sum_{i=1}^{m}(h_{θ}(x^{(i)})-y^{(i)})^2
$$

## 2  梯度下降

### 2.1  算法原理

*J*(*θ*) 的梯度（导数）为 ∇*J*(*θ*)：

- 随机初始化 *θ*；
- 设置步长 *α*；
- 设置迭代次数 *m*。

```
for i = 0 to m:
	θ = θ - α∇J(θ)
```

> 缺点：可能会出现只下降到局部最低，而达不到全局最低的情况。

### 2.2  代码实现

```python
import numpy as np
import matplotlib.pyplot as plt

x = np.linspace(-6, 4, 100)
y = x**2 + 2*x + 5

plt.plot(x, y)

# 初始化起点、步长、迭代次数
x = 3
alpha = 0.8
iteratorNum = 10

# 迭代
for i in range(iteratorNum):
    # y的导数为2x+2
    x = x - alpha*(2*x + 2)

print(x)
```

- 若 α 太小，则会导致迭代次数过多，下降速度慢；
- 若 α 太大，则会导致在最低点附近来回震荡，收敛速度慢；

## 3  线性回归

### 3.1  算法原理

使用梯度下降法求解线性回归问题：使代价函数的损失值最小。

迭代公式：

![image-20230910144123253](机器学习.assets/image-20230910144123253.png)

> - m 为样本数量；
> - j 为 n 维特征的其中一个。

### 3.2  代码实现

```python
# 梯度下降
def gradientDescent(X, y, theta, iterations, alpha):
    # 向原始数据手动添加一个全为1的列
    c = np.ones(X.shape[0]).transpose()
    X = np.insert(X, 0, values=c, axis=1)
    m = X.shape[0]  # 行数
    n = X.shape[1]  # 列数
    
    for num in range(iterations):
        for j in range(n):
            theta[j] = theta[j] + (alpha/m)*(np.sum((y-np.dot(X, theta))*X[:, j].reshape(-1, 1)))


# 特征归一化（减去平均值，除以方差）
def featureNormalize(X):
    mu = np.average(X, axis=0)
    sigma = np.std(X, axis=0, ddof=1)  # ddof=1，代表除的是n-1，否则除的是n
    X = (X-mu)/sigma
    return X, mu, sigma


X, y = loaddata()
X, mu, sigma = featureNormalize(X)
theta = np.zeros(X.shape[1]+1).reshape(-1, 1)
iterations = 400
alpha = 0.01
theta = gradientDescent(X, y, theta, iterations, alpha)
```

## 4  算法分类

### 4.1  批量梯度下降

每一次 *θ* 的更新，都会用到所有的数据进行计算。

### 4.2  随机梯度下降

每一次 *θ* 的更新，只选择其中一个样本进行计算。

> 缺点：效果不稳定，可能会产生震荡。

### 4.3  小批量梯度下降

每一次 *θ* 的更新，只选择样本中的一批（batch_size）数据进行计算。

## 5  评价指标

均方误差（MSE）：用每一个样本的实际值减去预测值，再对所有的样本取平均。

![image-20230910162351711](机器学习.assets/image-20230910162351711.png)

均方根误差（RMSE）：

![image-20230910162440635](机器学习.assets/image-20230910162440635.png)

平均绝对误差（MAE）：

![image-20230910162449513](机器学习.assets/image-20230910162449513.png)

## 6  正则化

### 6.1  岭回归

L2 范数正则化解决过拟合（Ridge Regression，岭回归）：

![image-20230910165601545](机器学习.assets/image-20230910165601545.png)

迭代公式：

![image-20230910165955770](机器学习.assets/image-20230910165955770.png)

λ 越大，对模型的惩罚就越大，模型就越简单（起主要作用的特征数量就越少），就越可以减少过拟合的风险。

### 6.2  LASSO 回归

L1 范数正则化解决过拟合（LASSO 回归）：

![image-20230910165613448](机器学习.assets/image-20230910165613448.png)

### 6.3  弹性网

L1 与 L2 结合解决过拟合（Elastic Net，弹性网）：

![image-20230910165627959](机器学习.assets/image-20230910165627959.png)

## 7  最小二乘法

$$
θ = (X^TX)^{-1}X^Ty
$$

> 常用公式：
>
> ![image-20230924110838971](机器学习.assets/image-20230924110838971.png)

# 逻辑回归

> 线性回归解决的是预测问题，逻辑回归解决的是二分类问题（通过扩展也可以解决多分类问题）。

## 1  基本概念

![image-20230924113433377](机器学习.assets/image-20230924113433377.png)

假设函数：预测值。

损失函数：

- 当 y（实际值）为 1 时，预测值越接近 1，损失函数越小；
- 当 y（实际值）为 0 时，预测值越接近 0，损失函数越小。

代价函数：对所有样本的损失函数取平均。

> 代价函数的值越小，分类的效果就越好。

## 2  迭代公式

![image-20230924115159799](机器学习.assets/image-20230924115159799.png)

## 3  代码实现

## 4  正则化

![image-20230924171227392](机器学习.assets/image-20230924171227392.png)

## 5  多分类

### 5.1  OvR（one over rest，一对多）

针对 k 个类别，训练 k 个分类器（预测函数），每次只区分是当前类别还是其它类别，最后取其中预测值最高的类别。

> 缺点：训练集有偏（不同类别的数据数量差别很大），对结果会造成影响。

### 5.2  OvO（one over one，一对一）

针对 k 个类别，训练 k(k-1)/2 个分类器（预测函数），每次只任意取其中两个类别来训练分类器。预测新数据时，将新数据带入每一个分类器，统计每个分类器的结果，出现次数最多的。

> - 缺点：训练开销大；
> - 优点：相对 OvR 准确率高。

### 5.3  MvM（multi over multi，多对多）

![image-20230924173433609](机器学习.assets/image-20230924173433609.png)

> 这里的距离为欧氏距离，即将每一维的坐标差分别平方，再求和并开根号。

# 决策树

## 1  基本概念

### 1.1  熵

![image-20230924174805141](机器学习.assets/image-20230924174805141.png)

### 1.2  条件熵

![image-20230924180423895](机器学习.assets/image-20230924180423895.png)

> 这里的 n 代表当前特征一共有 n 种取值，我们需要分别计算每种取值的熵，再加权求和，即为当前特征的条件熵。
>
> 每个特征下每种取值的熵，需要根据最后的类别去进行计算。

### 1.3  信息增益

![image-20230924181602323](机器学习.assets/image-20230924181602323.png)

> 若能根据某一个特征取值的不同，就能较为准确地区分出最后的类别，则说明该特征对最后的分类结果影响很大（与分类结果强相关），并且该特征下不同取值的熵会较小（对应最后的结果越统一），该特征对应的信息增益也越大，因此我们应选择信息增益较大的特征（分类效果最好）。

信息增益的计算方法：

![image-20230924181730633](机器学习.assets/image-20230924181730633.png)

## 2  构建算法

### 2.1  ID3 算法

![image-20230924182004385](机器学习.assets/image-20230924182004385.png)

### 2.2  C4.5 算法

![image-20230924225524160](机器学习.assets/image-20230924225524160.png)

### 2.3  基尼指数

![image-20230924225903034](机器学习.assets/image-20230924225903034.png)

> 每一个节点只划分出两个类别，选择一类，其余的类别都归为另一类。
>
> - 当该节点处只有两个类别时，只有一种划分方式；
> - 当该节点处有三个类别时，有三种划分方式。

示例：

![image-20230924230711331](机器学习.assets/image-20230924230711331.png)

## 3  代码实现

递归结束条件（满足其一即可）：

- 当前节点下的所有数据中，只剩一个类别，则该节点为该类别；
- 当前节点下的所有数据中，只剩一个特征，选择数量最多的类别作为当前节点的类别。

## 4  剪枝

### 4.1  预剪枝

![image-20230924233809848](机器学习.assets/image-20230924233809848.png)

### 4.2  后剪枝

![image-20230924233824977](机器学习.assets/image-20230924233824977.png)

## 5  处理连续值与缺失值

### 5.1  处理连续值

处理方法：将连续值离散化，通过计算信息增益来判断划分是否得当（取信息增益最大时的划分方法）。

![image-20230924233031814](机器学习.assets/image-20230924233031814.png)

### 5.2  处理缺失值

![image-20230924233725114](机器学习.assets/image-20230924233725114.png)

## 6  多变量决策树

![image-20230924234430469](机器学习.assets/image-20230924234430469.png)

![image-20230924234448821](机器学习.assets/image-20230924234448821.png)
