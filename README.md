# Computer Science Self-Learning Notes

<div align='left'>
  <img src=https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg >
  <img src=https://img.shields.io/badge/License-MIT-turquoise.svg >
  <img src=https://img.shields.io/badge/Maintained%3F-yes-green.svg >
  <img src=https://img.shields.io/badge/Contributions-welcome-blue >
  <img src=https://img.shields.io/github/watchers/shen-shanshan/cs-self-learning?color=9cc >
  <img src=https://img.shields.io/github/forks/shen-shanshan/cs-self-learning.svg?style=social >
  <img src=https://img.shields.io/github/stars/shen-shanshan/cs-self-learning.svg?style=social >
  <p>
</div align='left'>

<p align="center">
    <img src="https://i.imgur.com/waxVImv.png" alt="Oryx Video-ChatGPT">
</p>

## ğŸ“Œ Overview

This repository archives my notes and materials during my computer science self-learning jouney. Currently, I mainly focus on LLM/VLM inference engine and GPU/NPU computing, thus I have gathered many technical blogs for AI infra beginners and MLSys papers for researchers.

Contents:

- ğŸ“š [<u>Learning Notes</u>](#-learning-notes)
- ğŸ“š [<u>Technical Blogs</u>](#-technical-blogs)
- ğŸ“š [<u>Papers</u>](#-papers)
- ğŸ“š [<u>Learning Projects</u>](#-learning-projects)

In addition, I have also published some blogs and papers on the internet, you can read them at links below.

- ğŸ“– My technical blogs: [zhihu](https://www.zhihu.com/people/sss-53-26), [personal website](https://shen-shanshan.github.io/).
- ğŸ“– My publications: [papers](./Research/Publications/).

## ğŸ“š Learning Notes

### ğŸ§± Basic Knowledges

- Programming Languages:
  - [<u>Python</u>](./Languages/Python/Notes/)
  - [<u>C/C++</u>](./Languages/C&C++/Notes/)
  - [<u>Java</u>](./Languages/Java/Notes/)
  - [<u>Go</u>](./Languages/Go/Notes/)
- Data Structure & Algorithm:
  - [<u>Data Structure & Algorithm</u>](./Data_Structure&Algorithm/Notes/)
  - [<u>LeetCode Practices</u>](./Data_Structure&Algorithm/Codes/)
- [<u>Network</u>](./Network/Notes/)
- [<u>Operating System</u>](./Operating_System/Notes/)
- [<u>Design Pattern</u>](./Design_Pattern/Notes/)

### ğŸ¤– AI

- Mathematics:
  - [<u>Algebra</u>](./AI/Mathematics/Algebra/)
  - [<u>Differentiation</u>](./AI/Mathematics/Differentiation/)
  - [<u>Probability</u>](./AI/Mathematics/Probability/)
  - [<u>Optimization</u>](./AI/Mathematics/Optimization/)
- Deep Learning:
  - [<u>Basic Knowledges</u>](./AI/Deep_Learning/Basic/)
  - [<u>PyTorch Tutorial</u>](./AI/Deep_Learning/PyTorch/Notes/)
- LLM:
  - [<u>Basic Knowledges</u>](./AI/LLM/Basic/)
    - [<u>Transformer</u>](./AI/LLM/Basic/Transformer/)
  - [<u>MoE</u>](./AI/LLM/MoE/)
  - [<u>Multi-Modality</u>](./AI/LLM/Multi-Modality)
    - [<u>ViT</u>](./AI/LLM/Multi-Modality/ViT/)
- AI Infra:
  - [<u>Environment Preparation</u>](./AI/AI_Infra/Environment_Preparation/README.md)
  - [<u>Basic Knowledges</u>](./AI/AI_Infra/Basic/)
  - [<u>Inference Engine</u>](./AI/AI_Infra/Inference_Engine/Notes/)
    - [<u>vLLM</u>](./AI/AI_Infra/Inference_Engine/vLLM/Notes/)
  - [<u>HPC</u>](./AI/AI_Infra/HPC/)
    - [<u>CUDA</u>](./AI/AI_Infra/HPC/CUDA/README.md)
    - [<u>CANN</u>](./AI/AI_Infra/HPC/CANN/README.md)
  - [<u>Communication</u>](./AI/AI_Infra/Communication/)
    - [<u>NCCL</u>](./AI/AI_Infra/Communication/NCCL/)
  - [<u>Distributed System</u>](./AI/AI_Infra/Distributed_System/)
    - [<u>Ray</u>](./AI/AI_Infra/Distributed_System/Ray/)

### ğŸš€ Backend & Big Data

- [<u>Roadmap</u>](./Backend&Big_Data/Roadmap/)
- Backend Development:
  - [<u>Spring</u>](./Backend&Big_Data/Spring/)
  - [<u>MySQL</u>](./Backend&Big_Data/MySQL/)
  - [<u>Redis</u>](./Backend&Big_Data/Redis/)
  - [<u>Oracle</u>](./Backend&Big_Data/Oracle/)
- Big Data Development:
  - [<u>ElasticSearch</u>](./Backend&Big_Data/ElasticSearch/)
  - [<u>Flink</u>](./Backend&Big_Data/Flink/)
  - [<u>Hudi</u>](./Backend&Big_Data/Hudi/)

### ğŸ› ï¸ Tools

- [<u>Git</u>](./Tools/Git/Notes/)
- [<u>Docker</u>](./Tools/Docker/README.md)
- [<u>CMake</u>](./Tools/CMake/)
- Documentation:
  - [<u>Markdown</u>](./Tools/Markdown/)
  - [<u>reStructuredText</u>](./Tools/reStructuredText/Notes/)
- IDE:
  - [<u>VSCode Shortcut Key</u>](./Tools/VSCode/VSCodeå¸¸ç”¨å¿«æ·é”®.md)
- AI Agent:
  - [<u>Skills</u>](./Tools/Agent/Skills/)

### ğŸ”— Others

- [<u>Open Source Best Practices</u>](./Open_Source/)
- Research:
  - [<u>Research Notes</u>](./Research/Notes/)
  - [<u>Popular MLSys Papers</u>](./Research/Papers/)
- Employment:
  - [<u>Interview Experience</u>](./Employment/Interview/Experience/)
  - [<u>Working Insights</u>](./Employment/Working/)

## ğŸ“š Technical Blogs
<!--
| [<u>Title</u>][] | Category | [<u>@Author</u>][] | Note | â­ï¸ | âœ… |

### ğŸ“š Roadmap
| Title | Category | Author | Note | Rec | Read |
| :---- | :------- | :----- | :--- | :-- | :--- |
| [<u>äº†è§£/ä»äº‹æœºå™¨å­¦ä¹ /æ·±åº¦å­¦ä¹ ç³»ç»Ÿç›¸å…³çš„ç ”ç©¶éœ€è¦ä»€ä¹ˆæ ·çš„çŸ¥è¯†ç»“æ„ï¼Ÿ</u>][2036] | MLSys | [<u>@æœˆçƒå¤§å”</u>][3016] | | â­ï¸â­ï¸ | âœ… |
| [<u>æ— ç—›çš„æœºå™¨å­¦ä¹ ç³»ç»Ÿå…¥é—¨æŒ‡å—ï¼ˆä¸€ï¼‰</u>][2037] | MLSys | [<u>@æœˆçƒå¤§å”</u>][3016] | | â­ï¸â­ï¸ | âœ… |

### ğŸ“š Basic Knowledge
| [<u>å¤§æ¨¡å‹ - MoE æ··åˆä¸“å®¶ç³»ç»Ÿ</u>][2030] | MoE | [<u>@é©¬é˜Ÿä¹‹å£°</u>][3009] | MoE ç®€ä»‹ | â­ï¸â­ï¸ | âœ… |
| [<u>KV Cache - é«˜æ•ˆæ¨ç†å¿…å¤‡æŠ€æœ¯</u>][2025] | KV Cache | [<u>@é©¬é˜Ÿä¹‹å£°</u>][3009] | | â­ï¸â­ï¸ | âœ… |
| [<u>å¤§æ¨¡å‹æ¨ç† - ä¸ºä»€ä¹ˆè¦ PD åˆ†ç¦»ï¼Ÿ</u>][2017] | PD Disaggregation | [<u>@é©¬é˜Ÿä¹‹å£°</u>][3009] | PD åˆ†ç¦»ç®€ä»‹ | â­ï¸â­ï¸ | âœ… |

### ğŸ“š Dive into vLLM
| [<u>vLLM V1 æºç é˜…è¯»</u>][2014] | Architecture | [<u>@BoLi2001</u>][3007] | | â­ï¸â­ï¸â­ï¸â­ï¸ | âœ… |
| [<u>vLLM V1 Scheduler çš„è°ƒåº¦é€»è¾‘ & ä¼˜å…ˆçº§åˆ†æ</u>][2015] | Scheduler | [<u>@kaiyuan</u>][3005] | | â­ï¸â­ï¸â­ï¸ | âœ… |
| [<u>Shared Memory IPC Caching: Accelerating Data Transfer in LLM Inference Systems</u>][2058] | Multi-Modal | [<u>@vLLM Blog</u>][3019] | | | |
-->

### ğŸ“– Basic Knowledges

| Title | Category | Author | Note | Rec | Read |
| :---- | :------- | :----- | :--- | :-- | :--- |
| [<u>The Illustrated Transformer</u>][2016] | Transformer | [<u>@Jay Alammar</u>][3008] | Transformer åŸç†è¯¦è§£ | â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸ | âœ… |
| [<u>The Illustrated GPT-2 (Visualizing Transformer Language Models)</u>][2021] | Transformer | [<u>@Jay Alammar</u>][3008] | Transformer æ¨ç†è¿‡ç¨‹ | â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸ | âœ… |
| [<u>Mixture of Experts Explained</u>][2038] | MoE | [<u>@HuggingFace Blog</u>][3017] | MoE ç»¼è¿° | â­ï¸â­ï¸â­ï¸â­ï¸ | âœ… |
| [<u>ViT è®ºæ–‡é€Ÿè¯»</u>][2018] | Multi-Modal | [<u>@Zhang</u>][3010] | ViT åŸç†è¯¦è§£ | â­ï¸â­ï¸ | âœ… |
| [<u>LLaVA ç³»åˆ—æ¨¡å‹ç»“æ„è¯¦è§£</u>][2019] | Multi-Modal | [<u>@Zhang</u>][3010] | | â­ï¸â­ï¸â­ï¸ | âœ… |
| [<u>å›¾æ–‡è¯¦è§£ LLM inferenceï¼šKV Cache</u>][2024] | KV Cache | [<u>@å­£å¶</u>][3011] | | â­ï¸â­ï¸â­ï¸ | âœ… |
| [<u>å›¾è§£å¤§æ¨¡å‹è®¡ç®—åŠ é€Ÿç³»åˆ—ï¼šåˆ†ç¦»å¼æ¨ç†æ¶æ„ 2ï¼Œæ¨¡ç³Šåˆ†ç¦»ä¸åˆå¹¶è¾¹ç•Œçš„ chunked-prefills</u>][2044] | Schedule | [<u>@çŒ›çŒ¿</u>][3003] | | â­ï¸â­ï¸â­ï¸â­ï¸ | âœ… |
| [<u>LLM æ¨ç†å¹¶è¡Œä¼˜åŒ–çš„å¿…å¤‡çŸ¥è¯†</u>][2032] | Parallel Strategy | [<u>@kaiyuan</u>][3005] | | | |
| [<u>åˆ†å¸ƒå¼æ¨ç†ä¼˜åŒ–æ€è·¯</u>][2051] | Parallel Strategy | [<u>@kaiyuan</u>][3005] | | | |
| [<u>MoE å¹¶è¡Œè´Ÿè½½å‡è¡¡ï¼šEPLB çš„æ·±åº¦è§£æä¸å¯è§†åŒ–</u>][2040] | Parallel Strategy | [<u>@kaiyuan</u>][3005] | | â­ï¸â­ï¸â­ï¸ | âœ… |
| [<u>The Ultra-Scale Playbook: Training LLMs on GPU Clusters</u>][2039] | Parallel Strategy | [<u>@HuggingFace Blog</u>][3017] | | | |
| [<u>å›¾è§£å¤§æ¨¡å‹è®¡ç®—åŠ é€Ÿç³»åˆ—ï¼šåˆ†ç¦»å¼æ¨ç†æ¶æ„ 1ï¼Œä» DistServe è°ˆèµ·/u>][2049] | PD Disaggregation | [<u>@çŒ›çŒ¿</u>][3003] | PD åˆ†ç¦»åŸç†è¯¦è§£ | â­ï¸â­ï¸â­ï¸â­ï¸ | âœ… |
| [<u>LLM æ¨ç†æé€Ÿï¼šAttention ä¸ FFN åˆ†ç¦»æ–¹æ¡ˆè§£æ</u>][2046] | AF Disaggregation | [<u>@kaiyuan</u>][3005] | AF åˆ†ç¦»åŸç†è¯¦è§£ | â­ï¸â­ï¸â­ï¸ | âœ… |
| [<u>Step-3 AF åˆ†ç¦»æ¨ç†ç³»ç»Ÿ vs Deepseek EP æ¨ç†ç³»ç»Ÿï¼Œè°æ›´å¥½ï¼Ÿ</u>][2029] | AF Disaggregation | [<u>@ä¸å½’ç‰›é¡¿ç®¡çš„ç†ŠçŒ«</u>][3013] | AF åˆ†ç¦»ä¸å¤§ EP ä¼˜åŠ£å¯¹æ¯” | â­ï¸â­ï¸ | âœ… |
| [<u>Step-3 æ¨ç†ç³»ç»Ÿï¼šä» PD åˆ†ç¦»åˆ° AF åˆ†ç¦»ï¼ˆAFDï¼‰</u>][2026] | AF Disaggregation | [<u>@Yibo Zhu</u>][3012] | Step3 ä½œè€…æ‚è°ˆ | â­ï¸â­ï¸ | âœ… |
| [<u>GPU å†…å­˜ï¼ˆæ˜¾å­˜ï¼‰çš„ç†è§£ä¸åŸºæœ¬ä½¿ç”¨</u>][2063] | Hardware | [<u>@kaiyuan</u>][3005] | | â­ï¸â­ï¸â­ï¸â­ï¸ | âœ… |

### ğŸ“– Dive into vLLM

| Title | Category | Author | Note | Rec | Read |
| :---- | :------- | :----- | :--- | :-- | :--- |
| [<u>Inside vLLM: Anatomy of a High-Throughput LLM Inference System</u>][2045] | Overview | [<u>@vLLM Blog</u>][3019] | vLLM å…¨é¢åˆ†æ | â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸ | âœ… |
| [<u>vLLM V1 æ•´ä½“æµç¨‹ï½œä»è¯·æ±‚åˆ°ç®—å­æ‰§è¡Œ</u>][2003] | Architecture | [<u>@SSSä¸çŸ¥-é“</u>][3001] | vLLM æ¨ç†æµç¨‹ | â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸ | âœ… |
| [<u>å›¾è§£ vLLM V1 ç³»åˆ— 1ï¼šæ•´ä½“æµç¨‹</u>][2011] | Architecture | [<u>@çŒ›çŒ¿</u>][3003] | | â­ï¸â­ï¸â­ï¸ | âœ… |
| [<u>å›¾è§£ vLLM V1 ç³»åˆ— 2ï¼šExecutor-Workers æ¶æ„</u>][2012] | Architecture | [<u>@çŒ›çŒ¿</u>][3003] | | â­ï¸â­ï¸â­ï¸ | âœ… |
| [<u>å›¾è§£ vLLM V1 ç³»åˆ— 5ï¼šè°ƒåº¦å™¨ç­–ç•¥</u>][2006] | Scheduler | [<u>@çŒ›çŒ¿</u>][3003] | | â­ï¸â­ï¸â­ï¸â­ï¸ | âœ… |
| [<u>å›¾è§£ vLLM V1 ç³»åˆ— 4ï¼šåŠ è½½æ¨¡å‹æƒé‡</u>][2005] | Model | [<u>@çŒ›çŒ¿</u>][3003] | | â­ï¸â­ï¸ | âœ… |
| [<u>vLLM æ¨¡å‹æƒé‡åŠ è½½ï¼šä½¿ç”¨ setattr</u>][2007] | Model | [<u>@é£ä¹‹é­”æœ¯å¸ˆ</u>][3004] | | â­ï¸â­ï¸ | âœ… |
| [<u>ColumnParallelLinear å’Œ RowParallelLinear</u>][2008] | Model | [<u>@é£ä¹‹é­”æœ¯å¸ˆ</u>][3004] | | â­ï¸â­ï¸ | âœ… |
| [<u>å›¾è§£ vLLM V1 ç³»åˆ— 3ï¼šKV Cache åˆå§‹åŒ–</u>][2013] | KV Cache | [<u>@çŒ›çŒ¿</u>][3003] | | â­ï¸â­ï¸â­ï¸ | âœ… |
| [<u>vLLM æ˜¾å­˜ç®¡ç†è¯¦è§£</u>][2009] | Memory | [<u>@kaiyuan</u>][3005] | | â­ï¸â­ï¸â­ï¸â­ï¸ | âœ… |
| [<u>vLLM ç®—åŠ›å¤šæ ·æ€§ï½œPlatform æ’ä»¶ä¸ CustomOp</u>][2064] | Platform | [<u>@SSSä¸çŸ¥-é“</u>][3001] | | â­ï¸â­ï¸â­ï¸â­ï¸ | âœ… |
| [<u>Introducing vLLM Hardware Plugin, Best Practice from Ascend NPU</u>][2010] | Platform | [<u>@The Ascend Team on vLLM</u>][3006] | vLLM ç¡¬ä»¶æ’ä»¶åŒ–æœºåˆ¶ | â­ï¸â­ï¸â­ï¸ | âœ… |
| [<u>vLLM ç®—å­å¼€å‘æµç¨‹ï¼šâ€œä¿å§†çº§â€è¯¦ç»†è®°å½•</u>][2004] | Kernel | [<u>@DefTruth</u>][3002] | | â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸ | âœ… |
| [<u>Introduction to torch.compile and How It Works with vLLM</u>][2052] | Torch Compile | [<u>@vLLM Blog</u>][3019] | | â­ï¸â­ï¸ | âœ… |
| [<u>vLLM torch.compile Integration</u>][2060] | Torch Compile | [<u>@Jiangyun Zhu</u>][3028] | è‡ªå®šä¹‰ Pass æ–¹æ³• | â­ï¸â­ï¸â­ï¸ | âœ… |
| [<u>vLLM ç»“æ„åŒ–è¾“å‡ºï½œGuided Decoding (V0)</u>][2001] | Guided Decoding | [<u>@SSSä¸çŸ¥-é“</u>][3001] | | â­ï¸â­ï¸â­ï¸ | âœ… |
| [<u>vLLM ç»“æ„åŒ–è¾“å‡ºï½œGuided Decoding (V1)</u>][2002] | Guided Decoding | [<u>@SSSä¸çŸ¥-é“</u>][3001] | | â­ï¸â­ï¸â­ï¸ | âœ… |
| [<u>vLLM DP ç‰¹æ€§ä¸æ¼”è¿›æ–¹æ¡ˆåˆ†æ</u>][2041] | Parallel Strategy | [<u>@kaiyuan</u>][3005] | | â­ï¸â­ï¸â­ï¸â­ï¸ | âœ… |
| [<u>LLM æ¨ç†æ•°æ®å¹¶è¡Œè´Ÿè½½å‡è¡¡ï¼ˆDPLBï¼‰æµ…æ</u>][2042] | Parallel Strategy | [<u>@kaiyuan</u>][3005] | | â­ï¸â­ï¸â­ï¸ | âœ… |
| [<u>vLLM PD åˆ†ç¦»æ–¹æ¡ˆæµ…æ</u>][2022] | PD Disaggregation | [<u>@kaiyuan</u>][3005] | | â­ï¸â­ï¸â­ï¸ | âœ… |
| [<u>vLLM PD åˆ†ç¦» KV Cache ä¼ é€’æœºåˆ¶è¯¦è§£ä¸æ¼”è¿›åˆ†æ</u>][2023] | PD Disaggregation | [<u>@kaiyuan</u>][3005] | | â­ï¸â­ï¸â­ï¸ | âœ… |
| [<u>vLLM å·ç§¯è®¡ç®—åŠ é€Ÿï½œimg2col åŸç†è¯¦è§£</u>][2057] | Multi-Modal | [<u>@SSSä¸çŸ¥-é“</u>][3001] | | â­ï¸â­ï¸ | âœ… |

### ğŸ“– Dive into PyTorch

| Title | Category | Author | Note | Rec | Read |
| :---- | :------- | :----- | :--- | :-- | :--- |
| [<u>PyTorch æ˜¾å­˜ç®¡ç†ä»‹ç»ä¸æºç è§£æï¼ˆä¸€ï¼‰</u>][2061] | Memory | [<u>@kaiyuan</u>][3005] | | â­ï¸â­ï¸â­ï¸â­ï¸ | âœ… |
| [<u>PyTorch æ˜¾å­˜å¯è§†åŒ–ä¸ Snapshot æ•°æ®åˆ†æ</u>][2062] | Memory | [<u>@kaiyuan</u>][3005] | | â­ï¸â­ï¸â­ï¸â­ï¸ | âœ… |

### ğŸ“– CUDA Programming

| Title | Category | Author | Note | Rec | Read |
| :---- | :------- | :----- | :--- | :-- | :--- |
| [<u>CUDA å†…æ ¸ä¼˜åŒ–ç­–ç•¥</u>][2027] | Performance | [<u>@Zhang</u>][3010] | | â­ï¸â­ï¸â­ï¸ | âœ… |
| [<u>ä»å•¥ä¹Ÿä¸ä¼šåˆ° CUDA GEMM ä¼˜åŒ–</u>][2028] | Performance | [<u>@çŒ›çŒ¿</u>][3003] | | â­ï¸â­ï¸â­ï¸â­ï¸ | âœ… |

### ğŸ“– Communication

| Title | Category | Author | Note | Rec | Read |
| :---- | :------- | :----- | :--- | :-- | :--- |
| [<u>NCCL: Collective Operations</u>][2050] | Collective Communication | [<u>@NVIDIA Developer</u>][3022] | é›†åˆé€šä¿¡å¸¸ç”¨æ“ä½œ | â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸ | âœ… |
| [<u>ä¸€æ–‡è¯»æ‡‚ï½œRDMA åŸç†</u>][2035] | Network | [<u>@Linuxå†…æ ¸åº“</u>][3015] | | â­ï¸â­ï¸â­ï¸ | âœ… |

### ğŸ“– Development

| Title | Category | Author | Note | Rec | Read |
| :---- | :------- | :----- | :--- | :-- | :--- |
| [<u>LLM Inference é«˜æ•ˆ Debug æ–¹æ³•æ±‡æ€»</u>][2043] | Debug | [<u>@CarryPls</u>][3018] | vLLM Debug ç»éªŒ | â­ï¸â­ï¸ | âœ… |

### ğŸ“– Dive into Qwen

| Title | Category | Author | Note | Rec | Read |
| :---- | :------- | :----- | :--- | :-- | :--- |
| [<u>ä¸‡å­—é•¿æ–‡å›¾è§£ Qwen2.5-VL å®ç°ç»†èŠ‚</u>][2020] | Multi-Modal | [<u>@çŒ›çŒ¿</u>][3003] | Qwen2.5-VL æŠ€æœ¯æŠ¥å‘Šè§£è¯» | â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸ | âœ… |

### ğŸ“– Dive into DeepSeek

| Title | Category | Author | Note | Rec | Read |
| :---- | :------- | :----- | :--- | :-- | :--- |
| [<u>DeepSeek æŠ€æœ¯è§£è¯»ï¼ˆ1ï¼‰- å½»åº•ç†è§£ MLAï¼ˆMulti-Head Latent Attentionï¼‰</u>][2031] | Transformer | [<u>@å§œå¯Œæ˜¥</u>][3014] | | â­ï¸â­ï¸â­ï¸â­ï¸ | âœ… |
| [<u>DeepSeek æŠ€æœ¯è§£è¯»ï¼ˆ2ï¼‰- MTPï¼ˆMulti-Token Predictionï¼‰çš„å‰ä¸–ä»Šç”Ÿ</u>][2034] | Parallel Decoding | [<u>@å§œå¯Œæ˜¥</u>][3014] | | â­ï¸â­ï¸â­ï¸â­ï¸ | âœ… |
| [<u>DeepSeek æŠ€æœ¯è§£è¯»ï¼ˆ3ï¼‰- MoE çš„æ¼”è¿›ä¹‹è·¯</u>][2033] | MoE | [<u>@å§œå¯Œæ˜¥</u>][3014] | | â­ï¸â­ï¸â­ï¸â­ï¸ | âœ… |

<!----------------------------- Links: 2001~4000 ------------------------------>
<!------------------------------ Article Links -------------------------------->
[2001]: https://zhuanlan.zhihu.com/p/31572085999
[2002]: https://zhuanlan.zhihu.com/p/1895887395691423231
[2003]: https://zhuanlan.zhihu.com/p/1904589365151265287
[2004]: https://zhuanlan.zhihu.com/p/1892966682634473987?share_code=1lbfAKTh5A2Vr&utm_psn=1913354916832997933
[2005]: https://zhuanlan.zhihu.com/p/1908151478557839879?share_code=RlCt8lDNStds&utm_psn=1912310198112059517
[2006]: https://zhuanlan.zhihu.com/p/1908153627639551302?share_code=02jBOS1PfJxF&utm_psn=1912310252315079385
[2007]: https://zhuanlan.zhihu.com/p/714531623?utm_psn=1916989579635975888
[2008]: https://zhuanlan.zhihu.com/p/706801913
[2009]: https://zhuanlan.zhihu.com/p/1916529253169734444?share_code=aePDPg2VonBo&utm_psn=1917144770171606655
[2010]: https://blog.vllm.ai/2025/05/12/hardware-plugin.html
[2011]: https://zhuanlan.zhihu.com/p/1900126076279160869?share_code=18FtZ4wqQM3hR&utm_psn=1900940137866716878
[2012]: https://zhuanlan.zhihu.com/p/1900613601577899465
[2013]: https://zhuanlan.zhihu.com/p/1900932850829730567
[2014]: https://zhuanlan.zhihu.com/p/32045324831
[2015]: https://zhuanlan.zhihu.com/p/1900957007575511876?share_code=o9ZDfDnEpemP&utm_psn=1901069245619635086
[2016]: https://jalammar.github.io/illustrated-transformer/
[2017]: https://zhuanlan.zhihu.com/p/1897270081664300462?utm_psn=1897629970966217092
[2018]: https://www.armcvai.cn/2024-09-08/vit-paper.html
[2019]: https://www.armcvai.cn/2024-11-28/llava-structure.html
[2020]: https://zhuanlan.zhihu.com/p/1921289925552210138?share_code=oQnxmXt37SUD&utm_psn=1921301797538076351
[2021]: https://jalammar.github.io/illustrated-gpt2/
[2022]: https://zhuanlan.zhihu.com/p/1889243870430201414?utm_psn=1889596220076426760
[2023]: https://zhuanlan.zhihu.com/p/1906741007606878764?share_code=1m2xkCswqTA9N&utm_psn=1907185030842782292
[2024]: https://zhuanlan.zhihu.com/p/15949356834
[2025]: https://zhuanlan.zhihu.com/p/1899420001678647941
[2026]: https://zhuanlan.zhihu.com/p/1932920900203807997?share_code=ba1MSyrS0qF6&utm_psn=1939003570323165191
[2027]: https://www.armcvai.cn/2024-08-25/cuda-kernel.html
[2028]: https://zhuanlan.zhihu.com/p/703256080
[2029]: https://zhuanlan.zhihu.com/p/1935665533891633936
[2030]: https://zhuanlan.zhihu.com/p/1893225678108858182
[2031]: https://zhuanlan.zhihu.com/p/16730036197
[2032]: https://zhuanlan.zhihu.com/p/1937449564509545940
[2033]: https://zhuanlan.zhihu.com/p/18565423596
[2034]: https://zhuanlan.zhihu.com/p/18056041194
[2035]: https://zhuanlan.zhihu.com/p/648805252
[2036]: https://www.zhihu.com/question/315611053/answer/623529977
[2037]: https://zhuanlan.zhihu.com/p/65242220
[2038]: https://huggingface.co/blog/moe
[2039]: https://huggingface.co/spaces/nanotron/ultrascale-playbook
[2040]: https://zhuanlan.zhihu.com/p/29963005584
[2041]: https://zhuanlan.zhihu.com/p/1909265969823580330
[2042]: https://zhuanlan.zhihu.com/p/1927317160889386326
[2043]: https://zhuanlan.zhihu.com/p/1948019110530232787
[2044]: https://zhuanlan.zhihu.com/p/710165390
[2045]: https://blog.vllm.ai/2025/09/05/anatomy-of-vllm.html
[2046]: https://zhuanlan.zhihu.com/p/1952393747112367273
[2047]: https://www.zhihu.com/question/1896287526882341948/answer/1947436095278146150
[2048]: https://www.zhihu.com/question/421349039/answer/1951250277861679932?share_code=BPCzsxJfCoYl&utm_psn=1953189887630120518
[2049]: https://zhuanlan.zhihu.com/p/706761664
[2050]: https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html
[2051]: https://zhuanlan.zhihu.com/p/1937556222371946860
[2052]: https://blog.vllm.ai/2025/08/20/torch-compile.html
[2053]: https://colah.github.io/notes/taste/
[2054]: https://www.zhihu.com/question/60042037/answer/3601970421
[2055]: https://zhuanlan.zhihu.com/p/400248999?share_code=1a4eZuv2CLy0K&utm_psn=1920625675238372098
[2056]: https://www.zhihu.com/question/21278186/answer/1269255636
[2057]: https://zhuanlan.zhihu.com/p/1974125856852034422
[2058]: https://blog.vllm.ai/2025/11/13/shm-ipc-cache.html
[2059]: https://www.zhihu.com/question/640021523/answer/3366909228?share_code=UG5NXJZlmobv&utm_psn=1976998985035179447
[2060]: https://riverclouds.net/p/3525412134
[2061]: https://zhuanlan.zhihu.com/p/680769942
[2062]: https://zhuanlan.zhihu.com/p/677203832
[2063]: https://zhuanlan.zhihu.com/p/462191421
[2064]: https://zhuanlan.zhihu.com/p/1993988455059963960
<!------------------------------- Author Links -------------------------------->
<!-- SSSä¸çŸ¥-é“ -->
[3001]: https://www.zhihu.com/people/sss-53-26
<!-- DefTruth -->
[3002]: https://www.zhihu.com/people/qyjdef
<!-- çŒ›çŒ¿ -->
[3003]: https://www.zhihu.com/people/lemonround
<!-- é£ä¹‹é­”æœ¯å¸ˆ -->
[3004]: https://www.zhihu.com/people/qian-lan-21-34
<!-- kaiyuan -->
[3005]: https://www.zhihu.com/people/xky7
<!-- The Ascend Team on vLLM -->
[3006]: https://github.com/vllm-project/vllm-ascend/graphs/contributors
<!-- BoLi2001 -->
[3007]: https://www.zhihu.com/people/Leeeee59
<!-- Jay Alammar -->
[3008]: https://jalammar.github.io/
<!-- é©¬é˜Ÿä¹‹å£° -->
[3009]: https://www.zhihu.com/people/ma-shang-xian-sheng-84
<!-- Zhang -->
[3010]: https://www.armcvai.cn/
<!-- å­£å¶ -->
[3011]: https://www.zhihu.com/people/hey-jey
<!-- Yibo Zhu -->
[3012]: https://www.zhihu.com/people/yibozhu
<!-- ä¸å½’ç‰›é¡¿ç®¡çš„ç†ŠçŒ« -->
[3013]: https://www.zhihu.com/people/russwong
<!-- å§œå¯Œæ˜¥ -->
[3014]: https://www.zhihu.com/people/jiang-fu-chun-73
<!-- Linuxå†…æ ¸åº“ -->
[3015]: https://www.zhihu.com/people/cheng-xu-yuan-mian-shi-zhi-nan
<!-- æœˆçƒå¤§å” -->
[3016]: https://www.zhihu.com/people/zhanghuaizheng
<!-- HuggingFace Blog -->
[3017]: https://huggingface.co/blog
<!-- CarryPls -->
[3018]: https://www.zhihu.com/people/song-yu-xing-14
<!-- vLLM Blog -->
[3019]: https://blog.vllm.ai/
<!-- Tommic -->
[3020]: https://www.zhihu.com/people/niu-nai-li-de-xi-gua
<!-- èƒ¡æ´¥é“­ -->
[3021]: https://www.zhihu.com/people/hu-jin-ming-31
<!-- NVIDIA Developer -->
[3022]: https://developer.nvidia.com/
<!-- Christopher Olah -->
[3023]: https://colah.github.io/about.html
<!-- å¾å® -->
[3024]: https://www.zhihu.com/people/henryhxu
<!-- Sunt -->
[3025]: https://www.zhihu.com/people/sunt-ing
<!-- ç‚¹å¤´å£«ä¹”æ²» -->
[3026]: https://www.zhihu.com/people/si-hou-yue-zi
<!-- momoå­¦æœ¯ç‰ˆ -->
[3027]: https://www.zhihu.com/people/faithwong-30
<!-- Jiangyun Zhu -->
[3028]: https://github.com/ZJY0516
<!----------------------------- Links: 2001~4000 ------------------------------>

## ğŸ“š Papers
<!--
| Title | Institute | Conference/Jounal | Date | GitHub/Codes | Note | Read/In-depth Reading |
| [<u>Title</u>][] | 20xx/xx | | | |
| [<u>link</u>][]  | âœ… |
-->

Refer to [**_How to Read a Paper_**](./Research/Notes/How_to_Read_Papers.pdf) to master a practical and efficient **three-pass** method for reading research papers.

Clarification for symbols in the following tables:

- âœ…: **The first pass** that gives you a general idea about the paper.
- âœ… âœ…: **The second pass** that lets you grasp the paper's content, but not its details.
- âœ… âœ… âœ…: **The third pass** that helps you understand the paper in depth.

### ğŸ“– LLM Backbone

| Title | Date | GitHub | Note | Read |
| :---- | :--- | :----- | :--- | :--- |
| [<u>Mamba: Linear-Time Sequence Modeling with Selective State Spaces</u>][86] | 2023/12 | [<u>Mamba</u>][87] | [<u>link</u>][88] | âœ… |
| [<u>Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality</u>][151] | 2024/05 | | | |

### ğŸ“– LLM Inference Survey

| Title | Date | GitHub | Note | Read |
| :---- | :--- | :----- | :--- | :--- |

### ğŸ“– Framework

| Title | Date | GitHub | Note | Read |
| :---- | :--- | :----- | :--- | :--- |
| [<u>Efficient Memory Management for Large Language Model Serving with PagedAttention</u>][1] | 2023/09 | [<u>vLLM</u>][2] | | âœ… âœ… âœ… |
| [<u>SGLang: Efficient Execution of Structured Language Model Programs</u>][6] | 2023/12 | [<u>SGLang</u>][7] | | âœ… |
| [<u>A Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency</u>][51] | 2025/05 | | | |

### ğŸ“– Schedule

| Title | Date | GitHub | Note | Read |
| :---- | :--- | :----- | :--- | :--- |

### ğŸ“– Speculative Decoding

| Title | Date | GitHub | Note | Read |
| :---- | :--- | :----- | :--- | :--- |
| [<u>Blockwise Parallel Decoding for Deep Autoregressive Models</u>][91] | 2018/11 | | | |
| [<u>Fast Inference from Transformers via Speculative Decoding</u>][136] | 2022/11 | | [<u>link</u>][92] | âœ… |
| [<u>Accelerating Large Language Model Decoding with Speculative Sampling</u>][141] | 2023/02 | | | |
| [<u>SpecInfer: Accelerating Generative Large Language Model Serving with Tree-based Speculative Inference and Verification</u>][146] | 2023/05| | [<u>link</u>][92] | âœ… |
| [<u>Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding</u>][131] | 2024/01 | | | |
| [<u>Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads</u>][111] | 2024/01 | | [<u>link</u>][92] | âœ… |
| [<u>EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty</u>][101] | 2024/01 | | [<u>link</u>][92] | âœ… |
| [<u>Break the Sequential Dependency of LLM Inference Using Lookahead Decoding</u>][126] | 2024/02 | | [<u>link</u>][92] | âœ… |
| [<u>Accelerating Production LLMs with Combined Token/Embedding Speculators</u>][116] | 2024/04 | | | |
| [<u>Better & Faster Large Language Models via Multi-token Prediction</u>][121] | 2024/04 | | | |
| [<u>Optimizing Speculative Decoding for Serving Large Language Models Using Goodput</u>][96] | 2024/06 | | | |
| [<u>Scaling Speculative Decoding with Lookahead Reasoning</u>][106] | 2025/06 | | | |

### ğŸ“– Guided Decoding

| Title | Date | GitHub | Note | Read |
| :---- | :--- | :----- | :--- | :--- |
| [<u>Robust Text-to-SQL Generation with Execution-Guided Decoding</u>][31] | 2018/07 | | | âœ… |
| [<u>Efficient Guided Generation for Large Language Models</u>][36] | 2023/07 | [<u>Outlines</u>][37] | | âœ… âœ… |
| [<u>XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models</u>][41] | 2024/11 | [<u>XGrammar</u>][42] | | |
| [<u>Pre<sup>3</sup>: Enabling Deterministic Pushdown Automata for Faster Structured LLM Generation</u>][46] | 2025/06 | | | |

### ğŸ“– Long Sequence Processing

| Title | Date | GitHub | Note | Read |
| :---- | :--- | :----- | :--- | :--- |

### ğŸ“– Memory Offloading

| Title | Date | GitHub | Note | Read |
| :---- | :--- | :----- | :--- | :--- |
| [<u>MoE-Infinity: Efficient MoE Inference on Personal Machines with Sparsity-Aware Expert Cache</u>][181] | 2024/01 | | | |
| [<u>ProMoE: Fast MoE-based LLM Serving using Proactive Caching</u>][176] | 2024/10 | | | âœ… |

### ğŸ“– Large Scale Serving

| Title | Date | GitHub | Note | Read |
| :---- | :--- | :----- | :--- | :--- |
| [<u>AccelGen: Heterogeneous SLO-Guaranteed High-Throughput LLM Inference Serving for Diverse Applications</u>][61] | 2025/03 | | | |
| [<u>Serving Large Language Models on Huawei CloudMatrix384</u>][66] | 2025/06 | | | |

### ğŸ“– Load Balancing

| Title | Date | GitHub | Note | Read |
| :---- | :--- | :----- | :--- | :--- |
| [<u>Lynx: Enabling Efficient MoE Inference through Dynamic Batch-Aware Expert Selection</u>][186] | 2024/11 | | | |
| [<u>Pro-Prophet: A Systematic Load Balancing Method for Efficient Parallel Training of Large-scale MoE Models</u>][191] | 2024/11 | | | |

### ğŸ“– KVCache Store

| Title | Date | GitHub | Note | Read |
| :---- | :--- | :----- | :--- | :--- |
| [<u>CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion</u>][26] | 2024/05 | [<u>LMCache</u>][27] | | |
| [<u>Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving</u>][21] | 2024/07 | [<u>Mooncake</u>][22] | | |

### ğŸ“– Disaggregated Architecture

| Title | Date | GitHub | Note | Read |
| :---- | :--- | :----- | :--- | :--- |
| [<u>Splitwise: Efficient generative LLM inference using phase splitting</u>][76] | 2023/11 | [<u>splitwise-sim</u>][78] | [<u>link</u>][77] | âœ… |
| [<u>DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving</u>][11] | 2024/01 | [<u>DistServe</u>][12] | | |
| [<u>MegaScale-Infer: Serving Mixture-of-Experts at Scale with Disaggregated Expert Parallelism</u>][56] | 2025/04 | | [<u>link</u>][57] | âœ… âœ… |
| [<u>Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding</u>][16] | 2025/07 | [<u>Step3</u>][17], [<u>StepMesh</u>][18] | [<u>link</u>][19] | âœ… âœ… âœ… |
| [<u>xDeepServe: Model-as-a-Service on Huawei CloudMatrix384</u>][81] | 2025/08 | | | |

### ğŸ“– Elasticity and Fault Tolerance

| Title | Date | GitHub | Note | Read |
| :---- | :--- | :----- | :--- | :--- |
| [<u>ServerlessLLM: Low-Latency Serverless Inference for Large Language Models</u>][71] | 2024/01 | [<u>ServerlessLLM</u>][72] | [<u>link</u>][73] | âœ… âœ… |
| [<u>Expert-as-a-Service: Towards Efficient, Scalable, and Robust Large-scale MoE Serving</u>][156] | 2025/09 | | [<u>link</u>][157] | âœ… âœ… |
| [<u>ElasWave: An Elastic-Native System for Scalable Hybrid-Parallel Training</u>][161] | 2025/10 | | [<u>link</u>][162] | âœ… âœ… |
| [<u>MoE-Prism: Disentangling Monolithic Experts for Elastic MoE Services via Model-System Co-Designs</u>][166] | 2025/10 | | [<u>link</u>][167] | âœ… âœ… |
| [<u>From Models to Operators: Rethinking Autoscaling Granularity for Large Generative Models</u>][171] | 2025/11 | | [<u>link</u>][172] | âœ… |

<!------------------------------- Links: 1~2000 ------------------------------->
<!-- Efficient Memory Management for Large Language Model Serving with PagedAttention -->
[1]: https://arxiv.org/abs/2309.06180
[2]: https://github.com/vllm-project/vllm
<!-- SGLang: Efficient Execution of Structured Language Model Programs -->
[6]: https://arxiv.org/abs/2312.07104
[7]: https://github.com/sgl-project/sglang
<!-- DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving -->
[11]: https://arxiv.org/abs/2401.09670
[12]: https://github.com/LLMServe/DistServe
<!-- Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding -->
[16]: https://arxiv.org/abs/2507.19427v1
[17]: https://github.com/stepfun-ai/Step3
[18]: https://github.com/stepfun-ai/StepMesh
[19]: ./Papers/Disaggregated_Architecture/Step3/Notes.md
<!-- Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving -->
[21]: https://arxiv.org/abs/2407.00079
[22]: https://github.com/kvcache-ai/Mooncake
<!-- CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion -->
[26]: https://arxiv.org/abs/2405.16444
[27]: https://github.com/LMCache/LMCache
<!-- Robust Text-to-SQL Generation with Execution-Guided Decoding -->
[31]: https://arxiv.org/abs/1807.03100
<!-- Efficient Guided Generation for Large Language Models -->
[36]: https://arxiv.org/abs/2307.09702
[37]: https://github.com/dottxt-ai/outlines
<!-- XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models -->
[41]: https://arxiv.org/abs/2411.15100
[42]: https://github.com/mlc-ai/xgrammar
<!-- Pre<sup>3</sup>: Enabling Deterministic Pushdown Automata for Faster Structured LLM Generation -->
[46]: https://arxiv.org/abs/2506.03887
<!-- A Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency -->
[51]: https://arxiv.org/abs/2505.01658
<!-- MegaScale-Infer: Serving Mixture-of-Experts at Scale with Disaggregated Expert Parallelism -->
[56]: https://arxiv.org/abs/2504.02263
[57]: ./Papers/Disaggregated_Architecture/MegaScale-Infer/Notes.md
<!-- AccelGen: Heterogeneous SLO-Guaranteed High-Throughput LLM Inference Serving for Diverse Applications -->
[61]: https://arxiv.org/abs/2503.13737
<!-- Serving Large Language Models on Huawei CloudMatrix384 -->
[66]: https://arxiv.org/abs/2506.12708
<!-- ServerlessLLM: Low-Latency Serverless Inference for Large Language Models -->
[71]: https://arxiv.org/abs/2401.14351
[72]: https://github.com/ServerlessLLM/ServerlessLLM
[73]: ./Papers/Elasticity/serverlessLLM/Notes.md
<!-- Splitwise: Efficient generative LLM inference using phase splitting -->
[76]: https://arxiv.org/abs/2311.18677
[77]: ./Papers/Disaggregated_Architecture/Splitwise/Notes.md
[78]: https://github.com/Mutinifni/splitwise-sim
<!-- xDeepServe: Model-as-a-Service on Huawei CloudMatrix384 -->
[81]: https://arxiv.org/abs/2508.02520
<!-- Mamba: Linear-Time Sequence Modeling with Selective State Spaces -->
[86]: https://arxiv.org/abs/2312.00752
[87]: https://github.com/state-spaces/mamba
[88]: ./Papers/LLM_Basic/Mamba/Mamba/Notes.md
<!-- Blockwise Parallel Decoding for Deep Autoregressive Models -->
[91]: https://arxiv.org/abs/1811.03115
[92]: ./Papers/Speculative_Decoding/Notes.md
<!-- Optimizing Speculative Decoding for Serving Large Language Models Using Goodput -->
[96]: https://arxiv.org/abs/2406.14066
<!-- EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty -->
[101]: https://arxiv.org/abs/2401.15077
<!-- Scaling Speculative Decoding with Lookahead Reasoning -->
[106]: https://arxiv.org/abs/2506.19830
<!-- Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads -->
[111]: https://arxiv.org/abs/2401.10774
<!-- Accelerating Production LLMs with Combined Token/Embedding Speculators -->
[116]: https://arxiv.org/abs/2404.19124
<!-- Better & Faster Large Language Models via Multi-token Prediction -->
[121]: https://arxiv.org/abs/2404.19737
<!-- Break the Sequential Dependency of LLM Inference Using Lookahead Decoding -->
[126]: https://arxiv.org/abs/2402.02057
<!-- Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding -->
[131]: https://arxiv.org/abs/2401.07851
<!-- Fast Inference from Transformers via Speculative Decoding -->
[136]: https://arxiv.org/abs/2211.17192
<!-- Accelerating Large Language Model Decoding with Speculative Sampling -->
[141]: https://arxiv.org/abs/2302.01318
<!-- SpecInfer: Accelerating Generative Large Language Model Serving with Tree-based Speculative Inference and Verification -->
[146]: https://arxiv.org/abs/2305.09781
<!-- Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality -->
[151]: https://arxiv.org/abs/2405.21060
<!-- Expert-as-a-Service: Towards Efficient, Scalable, and Robust Large-scale MoE Serving -->
[156]: https://arxiv.org/abs/2509.17863
[157]: ./Papers/Elasticity/EaaS/Notes.md
<!-- ElasWave: An Elastic-Native System for Scalable Hybrid-Parallel Training -->
[161]: https://arxiv.org/abs/2510.00606
[162]: ./Papers/Elasticity/ElasWave/Notes.md
<!-- MoE-Prism: Disentangling Monolithic Experts for Elastic MoE Services via Model-System Co-Designs -->
[166]: https://arxiv.org/abs/2510.19366
[167]: ./Papers/Elasticity/MoE-Prism/Notes.md
<!-- From Models to Operators: Rethinking Autoscaling Granularity for Large Generative Models -->
[171]: https://arxiv.org/abs/2511.02248
[172]: ./Papers/Elasticity/Autoscaling_Granularity/Notes.md
<!-- ProMoE: Fast MoE-based LLM Serving using Proactive Caching -->
[176]: https://arxiv.org/abs/2410.22134
<!-- MoE-Infinity: Efficient MoE Inference on Personal Machines with Sparsity-Aware Expert Cache -->
[181]: https://arxiv.org/abs/2401.14361
<!-- Lynx: Enabling Efficient MoE Inference through Dynamic Batch-Aware Expert Selection -->
[186]: https://arxiv.org/abs/2411.08982
<!-- Pro-Prophet: A Systematic Load Balancing Method for Efficient Parallel Training of Large-scale MoE Models -->
[191]: https://arxiv.org/abs/2411.10003
<!------------------------------- Links: 1~2000 ------------------------------->

## ğŸ“š Learning Projects
<!--
| [<u></u>][] | | [<u>@</u>][] | |
-->

| Project | Category | Author/Organization | About |
| :------ | :------- | :------------------ | :---- |
| [<u>llm-action</u>][4022] | LLM | [<u>@liguodongiot</u>][4021] | æœ¬é¡¹ç›®æ—¨åœ¨åˆ†äº«å¤§æ¨¡å‹ç›¸å…³æŠ€æœ¯åŸç†ä»¥åŠå®æˆ˜ç»éªŒï¼ˆå¤§æ¨¡å‹å·¥ç¨‹åŒ–ã€å¤§æ¨¡å‹åº”ç”¨è½åœ°ï¼‰|
| [<u>AI-Infra-from-Zero-to-Hero</u>][4002] | MLSys | [<u>@HuaizhengZhang</u>][4001] | ğŸš€ Awesome System for Machine Learning âš¡ï¸ AI System Papers and Industry Practice. âš¡ï¸ System for Machine Learning, LLM (Large Language Model), GenAI (Generative AI). ğŸ» OSDI, NSDI, SIGCOMM, SoCC, MLSys, etc. ğŸ—ƒï¸ Llama3, Mistral, etc. ğŸ§‘â€ğŸ’» Video Tutorials. |
| [<u>awesomeMLSys</u>][4012] | MLSys | [<u>@GPU MODE</u>][4011] | An ML Systems Onboarding list |
| [<u>resource-stream</u>][4013] | CUDA | [<u>@GPU MODE</u>][4011] | GPU programming related news and material links |

<!----------------------------- Links: 4001~6000 ------------------------------>
<!-- æœˆçƒå¤§å” -->
[4001]: https://github.com/HuaizhengZhang
[4002]: https://github.com/HuaizhengZhang/AI-Infra-from-Zero-to-Hero
<!-- GPU MODE -->
[4011]: https://github.com/gpu-mode
[4012]: https://github.com/gpu-mode/awesomeMLSys
[4013]: https://github.com/gpu-mode/resource-stream
<!-- liguodongiot -->
[4021]: https://github.com/liguodongiot
[4022]: https://github.com/liguodongiot/llm-action
<!----------------------------- Links: 4001~6000 ------------------------------>

## Â©ï¸ Citation

```bibtex
@misc{cs-self-learning@2023,
  title  = {cs-self-learning},
  url    = {https://github.com/shen-shanshan/cs-self-learning},
  note   = {Open-source software available at https://github.com/shen-shanshan/cs-self-learning},
  author = {shen-shanshan},
  year   = {2023}
}
```

## ğŸ“œ License

MIT License, find more details [<u>here</u>](./LICENSE).

## â­ Star History

[![Star History Chart](https://api.star-history.com/svg?repos=shen-shanshan/cs-self-learning&type=Timeline)](https://www.star-history.com/#shen-shanshan/cs-self-learning&Timeline)
