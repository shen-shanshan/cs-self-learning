# Papers

## 论文清单

|                                                              论文标题                                                               |   领域   | 类型 |          备注           | 是否已读 |
| :---------------------------------------------------------------------------------------------------------------------------------- | :------- | :--- | :---------------------- | :------- |
| [<u>A Survey of Large Language Models</u>](./Papers/LLM综述.pdf)                                                                    | 大模型   | 综述 |                         |          |
| [<u>Attention Is All You Need</u>](./Papers/Transformer.pdf)                                                                        | 模型结构 | 算法 | Transformer             |          |
| [<u>ROFORMER: ENHANCED TRANSFORMER WITH ROTARY POSITION EMBEDDING</u>](./Papers/RoPE.pdf)                                           | 模型结构 | 算法 | RoPE                    |          |
| [<u>Root Mean Square Layer Normalization</u>](./Papers/RMSNorm.pdf)                                                                 | 模型结构 | 算法 | RMSNorm                 |          |
| [<u>Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism</u>](./Papers/Megatron-LM.pdf)            | 训练     | 框架 |                         |          |
| [<u>Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning</u>](./Papers/PEFT综述.pdf)                                | 微调     | 综述 | PEFT                    | ✅        |
| [<u>Prefix-Tuning: Optimizing Continuous Prompts for Generation</u>](./Papers/Prefix-Tuning.pdf)                                    | 微调     | 算法 |                         | ✅        |
| [<u>LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS</u>](./Papers/LoRA.pdf)                                                      | 微调     | 算法 |                         | ✅        |
| [<u>QLORA: Efficient Finetuning of Quantized LLMs</u>](./Papers/QLoRA.pdf)                                                          | 微调     | 算法 |                         | ✅        |
| [<u>LONGLORA: EFFICIENT FINE-TUNING OF LONG-CONTEXT LARGE LANGUAGE MODELS</u>](./Papers/LongLoRA.pdf)                               | 微调     | 算法 |                         |          |
| [<u>PUNICA: MULTI-TENANT LORA SERVING</u>](./Papers/Punica.pdf)                                                                     | 微调     | 框架 |                         | ✅        |
| [<u>Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems</u>](./Papers/LLM推理综述-1.pdf) | 推理     | 综述 |                         |          |
| [<u>A Survey on Efficient Inference for Large Language Models</u>](./Papers/LLM推理综述-2.pdf)                                      | 推理     | 综述 |                         |          |
| [<u>LLM Inference Unveiled: Survey and Roofline Model Insights</u>](./Papers/LLM推理综述-3.pdf)                                     | 推理     | 综述 |                         |          |
| [<u>From Online Softmax to FlashAttention</u>](./Papers/FlashAttention-note.pdf)                                                    | 推理     | 算法 | FlashAttention 公式推导 |          |
| [<u>FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</u>](./Papers/FlashAttention-v1.pdf)                | 推理     | 算法 |                         |          |
| [<u>FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning</u>](./Papers/FlashAttention-v2.pdf)           | 推理     | 算法 |                         |          |
| [<u>Accelerating Large Language Model Decoding with Speculative Sampling</u>](./Papers/Speculative_Sampling.pdf)                    | 推理     | 算法 | 投机解码                |          |
| [<u>Efficient Memory Management for Large Language Model Serving with PagedAttention</u>](./Papers/vLLM.pdf)                        | 推理     | 框架 | vLLM                    | ✅        |
| [<u>SGLang: Efficient Execution of Structured Language Model Programs</u>](./Papers/SGLang.pdf)                                     | 推理     | 框架 |                         |          |

<!-- 是否已读：✅、✗ -->

## 更多论文

- [<u>LLM Research Papers: The 2024 List</u>](https://magazine.sebastianraschka.com/p/llm-research-papers-the-2024-list?utm_source=publication-search)
