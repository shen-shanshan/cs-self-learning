# ChatTTS æºç å­¦ä¹ 

## ç¯å¢ƒæ­å»º

ä¾èµ–åˆ—è¡¨ï¼š

```
numpy<2.0.0
numba
torch==2.1.0
torchaudio
tqdm
vector_quantize_pytorch
transformers>=4.41.1
vocos
IPython
gradio
pybase16384
pynini==2.1.5; sys_platform == 'linux'
WeTextProcessing==1.0.2; sys_platform == 'linux'
nemo_text_processing==1.0.2; sys_platform == 'linux'
av
pydub
pyyaml
setuptools>=65.5.1
torch-npu==2.1.0.post6
```

å®‰è£…ä¾èµ–ï¼š

```bash
pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
```

### å®‰è£… pynini æ—¶æ‰¾ä¸åˆ° openfst

å®‰è£… openfstï¼š

```bash
cd /usr/local
mkdir openfst
cd openfst
wget http://www.openfst.org/twiki/pub/FST/FstDownload/openfst-1.8.3.tar.gz
tar -zxvf openfst-1.8.3.tar.gz
cd openfst-1.8.3
./configure --prefix=/usr/local/openfst/openfst-1.6.7 <--enable-python> --enable-grm
make -j4
make install
ln -s /usr/local/openfst/openfst-1.6.7 /usr/local/openfst/openfst

export PATH=/home/sss/.local/fst/openfst-1.7.2/src/bin:$PATH
export PATH=/~/bin/miniconda/miniconda3/envs/chattts_2/lib:$PATH
echo $PATH
```

ç¯å¢ƒé…ç½®å†™å…¥ `.bashrc`ï¼š

```bash
# >>> conda initialize >>>
# !! Contents within this block are managed by 'conda init' !!
__conda_setup="$('/home/sss/bin/miniconda/miniconda3/bin/conda' 'shell.bash' 'hook' 2> /dev/null)"
if [ $? -eq 0 ]; then
    eval "$__conda_setup"
else
    if [ -f "/home/sss/bin/miniconda/miniconda3/etc/profile.d/conda.sh" ]; then
        . "/home/sss/bin/miniconda/miniconda3/etc/profile.d/conda.sh"
    else
        export PATH="/home/sss/bin/miniconda/miniconda3/bin:$PATH"
    fi
fi
unset __conda_setup
# <<< conda initialize <<<

export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:~/bin/miniconda/miniconda3/envs/chattts_2/lib/
#export LD_PRELOAD=$LD_PRELOAD:~/bin/miniconda/miniconda3/envs/chattts_2/lib/

export PATH=$PATH:/home/sss/.local/fst/openfst-1.7.2/src/bin:/~/bin/miniconda/miniconda3/envs/chattts_2/lib
```

æ‰‹åŠ¨å®‰è£… pyniniï¼ŒæŒ‡å®šå¤´æ–‡ä»¶è·¯å¾„ï¼š

```bash
cd /home/sss/bin/miniconda/miniconda3/envs/cann/lib/python3.10/site-packages/pynini

git clone https://github.com/kylebgorman/pynini.git
cd pynini
python setup.py build_ext --include-dirs=/home/sss/.local/fst/openfst-1.6.7

python setup.py build_ext --include-dirs="/home/sss/.local/include" --library-dirs="/home/sss/.local/lib" install

python setup.py build_ext --include-dirs=~/bin/miniconda/miniconda3/envs/chattts_2/lib/
```

> å‚è€ƒèµ„æ–™ï¼š
>
> - [<u>openfst ä»¥åŠå…¶ python æ‰©å±•å®‰è£…</u>](https://blog.csdn.net/qq_33424313/article/details/122293358)ï¼›
> - [<u>å®‰è£… openfst ä¾èµ–</u>](https://blog.csdn.net/weixin_53694631/article/details/128552804)ï¼›
> - [<u>openfst Installation issues</u>](https://github.com/kylebgorman/pynini/issues/16)ã€‚

## Python åŸºç¡€

### Python å…³é”®å­—

`async`ï¼š

å®šä¹‰å¼‚æ­¥å‡½æ•°ã€‚

> å‚è€ƒèµ„æ–™ï¼š[python å¼‚æ­¥ async/awaitï¼ˆè¿›é˜¶è¯¦è§£ï¼‰_python async-CSDNåšå®¢](https://blog.csdn.net/qq_43380180/article/details/111573642)ã€‚

`global`ï¼š

å®šä¹‰å…¨å±€å˜é‡ã€‚

`yield`ï¼š

å¸¦ `yield` çš„å‡½æ•°æ˜¯ä¸€ä¸ªç”Ÿæˆå™¨ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªå‡½æ•°ã€‚

ç”Ÿæˆå™¨æœ‰ä¸€ä¸ªæ˜¯ `next()` å‡½æ•°ï¼Œ`next()` å°±ç›¸å½“äºâ€œä¸‹ä¸€æ­¥â€ç”Ÿæˆå“ªä¸ªæ•°ï¼Œè¿™ä¸€æ¬¡çš„ `next()` å¼€å§‹çš„åœ°æ–¹æ˜¯æ¥ç€ä¸Šä¸€æ¬¡çš„ `next()` åœæ­¢çš„åœ°æ–¹æ‰§è¡Œçš„ï¼Œé‡åˆ° `yield` åï¼Œreturn å‡ºè¦ç”Ÿæˆçš„æ•°ï¼Œæ­¤æ­¥å°±ç»“æŸã€‚

> å‚è€ƒèµ„æ–™ï¼š[<u>python ä¸­ yield çš„ç”¨æ³•è¯¦è§£</u>](https://blog.csdn.net/mieleizhi0522/article/details/82142856/)ã€‚

`assert`ï¼š

ç”¨äºæ–­è¨€æµ‹è¯•ã€‚

ç¤ºä¾‹ï¼š

```python
assert expression, "é”™è¯¯ä¿¡æ¯"
```

- `expression` ä¸º `true`ï¼šä»€ä¹ˆä¹Ÿä¸å‘ç”Ÿï¼›
- `expression` ä¸º `false`ï¼šç¨‹åºæŠ¥é”™ã€‚

### Python æ³¨è§£

`@dataclass`ï¼š

åœ¨å®šä¹‰æ•°æ®ç±»æ—¶ï¼Œæˆ‘ä»¬é€šå¸¸éœ€è¦ç¼–å†™ä¸€äº›é‡å¤æ€§çš„ä»£ç ï¼Œå¦‚æ„é€ å‡½æ•°ã€å±æ€§è®¿é—®å™¨å’Œå­—ç¬¦ä¸²è¡¨ç¤ºç­‰ã€‚ä½¿ç”¨ `@dataclass` å¯ä»¥è‡ªåŠ¨ç”Ÿæˆè¿™äº›é€šç”¨æ–¹æ³•ï¼Œä»è€Œç®€åŒ–æ•°æ®ç±»çš„å®šä¹‰è¿‡ç¨‹ã€‚

å…·ä½“åœ°ï¼Œ`@dataclass` å¯ä»¥çœç•¥ `__init__()` å’Œ `__str__()` æ–¹æ³•ï¼Œé€‚åˆäºå±æ€§ç‰¹åˆ«å¤šã€ä¸“é—¨ç”¨äºå­˜å‚¨æ•°æ®çš„ç±»ï¼ˆç±»ä¼¼äºåç«¯å¼€å‘ä¸­çš„â€œå®ä½“ç±»â€ï¼‰ã€‚

```python
from dataclasses import dataclass, field
from typing import List


@dataclass
class Person1:
    name: str
    age: int
    salary: float
    gender: bool
    hobbies: List[str]


def gen_list():
    return ['guitar', 'badminton', 'programme']


@dataclass(frozen=True)  # ä½¿ç”¨ frozen=True å¯ä»¥å°†è¯¥ç±»çš„å±æ€§è®¾ç½®ä¸ºåªè¯»ï¼Œå¤–ç•Œæ— æ³•ä¿®æ”¹
class Person2:
    name: str = 'sss_2'  # å±æ€§å¯ä»¥è®¾ç½®é»˜è®¤å€¼
    age: int = 26
    salary: float = field(default=20000.000, repr=False)  # ä½¿ç”¨ repr å‚æ•°å¯ä»¥è®¾ç½®éšè—å­—æ®µï¼ˆä¸ä¼šè¢« print() æ–¹æ³•æ‰“å°ï¼‰
    gender: bool = True
    # hobbies: List[str] = ['guitar', 'badminton']
    """
    ValueError: mutable default <class 'list'> for field hobbies is not allowed: use default_factory
    æ³¨æ„ï¼šå¯¹äºå¯å˜ç±»å‹ï¼ˆå¦‚ listï¼‰çš„å±æ€§ï¼Œéœ€è¦ç”¨å·¥å‚æ–¹æ³•æ¥äº§ç”Ÿé»˜è®¤å€¼
    """
    hobbies: List[str] = field(default_factory=gen_list)


if __name__ == '__main__':
    p1 = Person1('sss', 18, 15000.000, True, ['guitar', 'badminton'])
    print(p1)  # Person1(name='sss', age=18, salary=15000.0, gender=True, hobbies=['guitar', 'badminton'])

    p2 = Person2()
    print(p2)  # Person2(name='sss_2', age=26, gender=True, hobbies=['guitar', 'badminton', 'programme'])

    p1.name = 'ma yun'
    print(p1.name)  # ma yun
    
    # p2.name = 'ma hua teng'
    """
    dataclasses.FrozenInstanceError: cannot assign to field 'name'
    Person2 çš„å±æ€§å·²è¢«å†»ç»“ï¼Œæ— æ³•ä¿®æ”¹
    """
```

> å‚è€ƒèµ„æ–™ï¼š[æŒæ¡pythonçš„dataclassï¼Œè®©ä½ çš„ä»£ç æ›´ç®€æ´ä¼˜é›… - wang_yb - åšå®¢å›­ (cnblogs.com)](https://www.cnblogs.com/wang_yb/p/18077397)ã€‚

`@metaclass`ï¼š

> å‚è€ƒèµ„æ–™ï¼š
>
> - [Pythonè¿›é˜¶â€”â€”è¯¦è§£å…ƒç±»ï¼Œmetaclassçš„åŸç†å’Œç”¨æ³• - çŸ¥ä¹ (zhihu.com)](https://zhuanlan.zhihu.com/p/149126959)ï¼›
> - [ä½¿ç”¨å…ƒç±» - Pythonæ•™ç¨‹ - å»–é›ªå³°çš„å®˜æ–¹ç½‘ç«™ (liaoxuefeng.com)](https://liaoxuefeng.com/books/python/oop-adv/meta-class/index.html)ã€‚

### Python æ–¹æ³•

`eval()`ï¼š

```python
class A(torch.nn.Module):
    # ...

# åˆå§‹åŒ–æ¨¡å‹
a = A().eval()
```

### `__init__.py`

- å°†ç›®å½•æ ‡è®°ä¸º Python åŒ…ï¼›
- åŒ…çº§åˆ«çš„åˆå§‹åŒ–ä»£ç ï¼ˆåŒ…è¢«å¯¼å…¥æ—¶æ‰§è¡Œï¼‰ï¼›
- å®šåˆ¶å¯¹å¤–çš„æ¥å£ï¼Œé—´æ¥å¯¼å…¥å‡½æ•°ã€ç±»æˆ–å…¶å®ƒæ¨¡å—ã€‚

> å‚è€ƒèµ„æ–™ï¼š[<u>python init æ–‡ä»¶åŸç†è§£æ</u>](https://blog.csdn.net/Bill_seven/article/details/104391208)ã€‚

### å­—ç¬¦æ“ä½œ

- `str.maketrans()`ï¼šæ„å»ºç¿»è¯‘è¡¨çš„æ˜ å°„å…³ç³»ï¼›
- `str.translate()`ï¼šæ‰§è¡Œç¿»è¯‘ï¼Œæ›¿æ¢ä¸ºæ˜ å°„åˆ°çš„è¯ï¼›
- `ord()`ï¼šPython çš„å†…ç½®å‡½æ•°ï¼Œè¿”å›å¯¹åº”çš„ ASCII æ•°å€¼ï¼Œæˆ–è€… Unicode æ•°å€¼ã€‚

### æ­£åˆ™è¡¨è¾¾å¼

> å‚è€ƒèµ„æ–™ï¼š[<u>æ­£åˆ™è¡¨è¾¾å¼-èœé¸Ÿæ•™ç¨‹</u>](https://www.runoob.com/python/python-reg-expressions.html)ã€‚

## web æ¡†æ¶

### FastAPI

å¸¸ç”¨ä¾èµ–ï¼š

- `from fastapi import FastAPI`ï¼šæä¾›æœåŠ¡ç«¯æ¥å£ï¼›
- `from pydantic import BaseModel`ï¼šç”¨äºæœåŠ¡ç«¯æ¥å£çš„å‚æ•°æ ¡éªŒï¼›
- `import requests`ï¼šç”¨äºå®¢æˆ·ç«¯å‘æœåŠ¡ç«¯å‘é€è¯·æ±‚ã€‚

å¸¸ç”¨æ³¨è§£ï¼š

- `on_event(â€œstartupâ€)`ï¼šå½“åç«¯æœåŠ¡å¯åŠ¨æ—¶æ‰§è¡Œè¯¥å‡½æ•°ï¼Œç”¨äºæœåŠ¡ç«¯çš„åˆå§‹åŒ–ï¼›
- `post(â€œ/â€¦â€)`ï¼šæ”¶åˆ°å¯¹åº” post è¯·æ±‚æ—¶æ‰§è¡Œè¯¥å‡½æ•°ï¼Œç›¸å½“ Java ä¸­çš„ `@Controller`ã€‚

### gradio

gradio ç”¨äºä¸º python åº”ç”¨æä¾› web UIã€‚

> å®˜æ–¹ç½‘ç«™ï¼š[<u>gradio</u>](https://www.gradio.app/)ã€‚

## PyTorch æ¡†æ¶

### å¸¸ç”¨æ³¨è§£

`@torch.no_grad()`ï¼š

æ¨ç†æ—¶ï¼Œä¸å­˜å‚¨è®¡ç®—å›¾ä¸­æ¯ä¸ªèŠ‚ç‚¹çš„æ¢¯åº¦ã€‚

`@torch.inference_mode()`ï¼š

æ˜¯ä¸€ä¸ªç±»ä¼¼äº `no_grad` çš„æ–°ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œè¯¥æ¨¡å¼ç¦ç”¨äº†è§†å›¾è·Ÿè¸ªå’Œç‰ˆæœ¬è®¡æ•°å™¨ï¼Œæ‰€ä»¥åœ¨æ­¤æ¨¡å¼ä¸‹è¿è¡Œä»£ç èƒ½å¤Ÿè·å¾—æ›´å¥½çš„æ€§èƒ½ï¼Œé€Ÿåº¦ä¹Ÿä¼šæ›´å¿«ã€‚

## æ€§èƒ½ä¼˜åŒ–

### numba

`numba`ï¼špython çš„ JITï¼ˆå³æ—¶ç¼–è¯‘ï¼‰åº“ã€‚

> å®˜æ–¹è¯´æ˜ï¼šNumba generates optimized machine code from pure Python code using the LLVM compiler infrastructure. With a few simple annotations, array-oriented and math-heavy Python code can be just-in-time optimized to performance similar as C, C++ and Fortran, without having to switch languages or Python interpreters.

JITï¼ˆå³æ—¶ç¼–è¯‘ï¼‰ï¼šå½“æŸæ®µä»£ç å³å°†ç¬¬ä¸€æ¬¡è¢«æ‰§è¡Œæ—¶è¿›è¡Œç¼–è¯‘ã€‚åŠ¨æ€åœ°å°†é«˜çº§è¯­è¨€ç¼–å†™çš„ä»£ç è½¬æ¢ä¸ºæœºå™¨ç ï¼Œå¯ä»¥ç›´æ¥ç”±è®¡ç®—æœºçš„å¤„ç†å™¨æ‰§è¡Œï¼Œè¿™æ˜¯åœ¨è¿è¡Œæ—¶å®Œæˆçš„ï¼Œä¹Ÿå°±æ˜¯ä»£ç æ‰§è¡Œä¹‹å‰ï¼Œå› æ­¤ç§°ä¸ºâ€œå³æ—¶â€ã€‚

ä¸¤ç§æ¨¡å¼ï¼š

- `nopython`ï¼šThe behaviour of the nopython compilation mode is to essentially compile the decorated function so that it will run entirely without the involvement of the Python interpreter. This is the recommended and best-practice way to use the Numba jit decorator as it leads to the best performanceï¼›
- `object`ï¼šThis achieved through using the `forceobj=True` key word argument to the `@jit` decorator. In this mode Numba will compile the function with the assumption that everything is a Python object and essentially run the code in the interpreter. Specifying `looplift=True` might gain some performance over pure object mode as Numba will try and compile loops into functions that run in machine code, and it will run the rest of the code in the interpreter. For best performance avoid using object mode mode in general.

ç¤ºä¾‹ï¼š

```python
from numba import jit

@jit
def function():
    # ...
```

numba åœ¨é¦–æ¬¡è°ƒç”¨å‡½æ•°çš„æ—¶å€™è¿›è¡Œäº†ç¼–è¯‘ï¼Œä½†å½“ç¼–è¯‘å‘ç”Ÿåï¼Œnumba ä¼šå°†è¯¥å‡½æ•°çš„æœºå™¨ç è¿›è¡Œç¼“å­˜ï¼Œå¦‚æœå†æ¬¡è°ƒç”¨è¯¥å‡½æ•°ï¼Œå®ƒä¼šç›´æ¥ä»ç¼“å­˜ä¸­åŠ è½½ï¼Œè€Œä¸éœ€è¦å†æ¬¡ç¼–è¯‘ã€‚

> å®˜æ–¹è¯´æ˜ï¼šonce the compilation has taken place Numba caches the machine code version of your function for the particular types of arguments presented. If it is called again with the same types, it can reuse the cached version instead of having to compile again.

Numba for CUDA GPUsï¼š

Numba èƒ½å¤Ÿæ”¯æŒ CUDA GPU ç¼–ç¨‹ï¼Œèƒ½å¤Ÿè‡ªåŠ¨åœ°åœ¨ host ä¸ device ä¹‹é—´ä¼ é€’ Numpy æ•°ç»„ã€‚

> å‚è€ƒèµ„æ–™ï¼š
>
> - [<u>Numba GitHub åœ°å€</u>](https://github.com/numba/numba)ï¼›
> - [<u>Numba documentation</u>](https://numba.readthedocs.io/en/stable/index.html)ï¼›
> - [<u>python è¿è¡ŒåŠ é€Ÿç¥å™¨â€”â€”numba åŸç†è§£æ</u>](https://blog.csdn.net/weixin_45977690/article/details/133886829)ï¼›
> - [<u>JIT å³æ—¶ç¼–è¯‘æŠ€æœ¯</u>](https://zhuanlan.zhihu.com/p/193035135)ã€‚

## TTS åŸç†

1. æ–‡æœ¬é¢„å¤„ç†ï¼›
2. é¢‘è°±ç”Ÿæˆï¼›
3. æ—¶åŸŸæ³¢å½¢ç”Ÿæˆã€‚

> å‚è€ƒèµ„æ–™ï¼š[<u>Text-to-Speech with Tacotron2</u>](https://pytorch.org/audio/stable/tutorials/tacotron2_pipeline_tutorial.html)ã€‚

## æ¨¡å‹æ¨ç†

### æ¨æµæµç¨‹

1. åŠ è½½æ¨¡å‹ï¼šé€‰æ‹© `device`ã€åˆå§‹åŒ–æ¨¡å‹æƒé‡ï¼›
2. text -> normalizerï¼ˆæ­£åˆ™åŒ–ï¼šæ›¿æ¢åŒä¹‰è¯ã€â€¦â€¦ï¼‰ï¼›
3. refine textï¼šè‡ªåŠ¨åŠ å…¥è¯­æ°”è¯ï¼Ÿï¼ˆå¯é€‰ï¼‰ï¼›
4. tokenizer -> encode -> embed -> gpt.generateï¼›
5. decode to wavsã€‚

### å¤šå¡æ¨ç†

> NPU é€‚é…ï¼Œæœç´¢å…³é”®è¯ï¼š`torch.cuda`ã€`gpu`ã€‚

`configs.py`ï¼š

**ParallelConfig**:
Configuration for the distributed execution.
    `max_parallel_loading_workers`ï¼šmax concurrent workers.

`llm_engine.py`ï¼š

**LLMEngine**ï¼š
This is the main class for the vLLM engine.
    The `LLM` class wraps this class for offline batched inference.
    The `AsyncLLMEngine` class wraps this class for online serving.
`__init__()`ï¼š
    Create the parallel GPU workers.
    `_init_workers_ray(placement_group)`ï¼šï¼Ÿ
        `_run_workers("init_model")`ï¼šï¼Ÿ
        `_run_workers("load_model", max_concurrent_workers)`ï¼šï¼Ÿ
    `_init_workers()`ï¼šï¼Ÿ
        `_run_workers("init_model")`ï¼šï¼Ÿ
        `_run_workers("load_model")`ï¼šï¼Ÿ
    `_run_workers(..., method, ...)`ï¼šRuns the given method on all workers.

åˆ†å¸ƒå¼è®¡ç®—æ¡†æ¶ Rayï¼š
    driver workerï¼šï¼Ÿ
    ray workerï¼šï¼Ÿ

`worker.py`ï¼š

ğŸŒŸ éœ€è¦é€‚é…çš„ APIï¼š
    `set_device()` âœ…
    `empty_cache()` âœ…
    `synchronize()`
    `mem_get_info()` âœ…
    `get_device_capability()`

`model_runner.py`ï¼š

`load_model()`ï¼šè°ƒç”¨ `get_model()` è·å– `model_config`ã€‚

ğŸŒŸ éœ€è¦é€‚é…çš„ APIï¼š
    `synchronize()`
    `CUDAGraph()`

`model_loader.py`ï¼š

`get_model()`ï¼šæ¶‰åŠ `torch.cuda.get_device_capability()`ã€‚

ğŸŒŸ éœ€è¦é€‚é…çš„ APIï¼š
    `get_device_capability()`

[torch_npu API æŸ¥è¯¢](https://www.hiascend.com/document/detail/zh/Pytorch/60RC2/apiref/apilist/ptaoplist_000655.html)ã€‚
