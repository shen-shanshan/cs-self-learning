# 分布式训练

- 流水线并行
- 数据并行
- 张量并行
- 序列并行

## 流水线并行

……

## 数据并行

……

## 张量并行

**朴素张量并行**：

与数据并行相反，张量并行将单个模型拆分到不同的设备上，而不是在每个设备上复制整个模型（具体来说，假设一个模型 m 包含 6 层：使用数据并行时，每个设备将拥有这 6 层的副本，而使用张量并行在两个设备上时，每个设备只拥有 3 层）。

朴素张量并行实现解决了模型过大无法放入单个设备的问题。然而，你可能已经注意到，如果模型能够放入单个设备，朴素张量并行将比在单个设备上运行更慢。这是因为在任何时候，只有一个设备在工作，而另一个设备处于空闲状态。

**张量并行：**

张量并行（Tensor Parallelism，TP）是一种更细粒度的模型并行方法，它将单层内部的参数和计算任务拆分到不同的设备上执行，这种方法特别适合于具有大量参数的大规模模型。它最初是在 Megatron-LM 论文中提出的，它是一种高效的模型并行技术，可用于训练大型 Transformer 模型。

> **Megatron-LM**: Training Multi-Billion Parameter Language Models Using **Model Parallelism**.

通过张量并行，可以将矩阵乘法等计算操作的矩阵按行或按列切分，然后在不同设备上并行执行部分计算，最后通过集合通信操作合并结果。张量并行可以分为 MatMul 并行、Transformer 并行、Embedding 并行、Cross Entropy Loss 并行。

张量并行的主要挑战在于如何切分参数和计算任务，以保证计算的一致性和通信的高效性。

“按行”和“按列”切分权重：在 Megatron-LM 中，权重的切分操作就是由这两个基础算子组合而成的。

**AllReduce**：GPU 间互相同步计算结果，分为两个阶段。

- Reduce-Scatter
- All-Gather

**Multi-head Attention 的计算方法**：

每个头上都可以独立计算，最后再将结果 concat 起来。也就是说，可以把每个头的参数放到一块 GPU 上。

对三个参数矩阵 Q/K/V 按照“列切割”，每个头放到一块 GPU 上，做并行计算。对线性层，按照“行切割”。

在实际应用中，并不一定按照一个 head 占用一块 GPU 来切割权重，我们也可以一个多个 head 占用一块 GPU（尽量保证 head 总数能被 GPU 个数整除）。

**输入层 Embedding**：

- word embedding：维度 `(v, h)`，其中 `v` 表示词表大小；
- positional embedding：维度 `(max_s, h)`，其中 `max_s` 表示模型允许的最大序列长度。

> 对于输入 X，计算 word embedding 就相当于是用 token 的序号去 word embedding 中查找对应词向量的过程。

positional embedding 的 `max_s` 一般比较小，对显存的压力不大；而 word embedding 的词表一般很大，需要拆分到各个 GPU 上。

当输入 X 在 GPU 上计算 word embedding 时，能找到的词，就正常返回词向量，找到不到就把词向量中的全部元素都置 0。按此方式查找完毕后，每块 GPU 上的数据做一次 AllReduce，就能得到最终的输入。

示例：

第一步：两块 GPU 独立计算 word embedding。

- 第一块 GPU 的查找结果为 `[A, 0, C, D]`；
- 第二块 GPU 的查找结果为 `[0, B, 0, 0]`。

第二步：GPU 间做 AllReduce。

将两个向量相加，得到完整的词向量：`[A, B, C, D]`。

**输出层 Embedding**：

输出层中，同样有一个 word embedding，把输入再映射回词表里，得到每一个位置的词。一般来说，输入层和输出层共用一个 word embeding。

**Cross-entropy 层**：

……

**总结：**

在实际应用中，对 Transformer 类的模型，最经典的方法是张量模型并行 + 数据并行，并在数据并行中引入 ZeRO 做显存优化。

- 模型权重按特征维度进行拆分（TP，张量并行）；
- 训练数据按批次进行拆分（DP，数据并行）。

一般我们在同一台机器（node）的 GPU 间做张量模型并行，在不同机器间做数据并行。

**计算 backward 的方式：**

- TP：从上一层往下一层做 backward 时，所有 GPU 间需要做一次 AllReduce；
- DP：本层算完梯度以后，就正常把本层的梯度发出去，和属于一个 DP 组的 GPU 做 AllReduce，同时继续往下一层做 backward。下一层也是同理。也就是在 DP 组中，下一层不依赖上一层的梯度聚合结果。

## 序列并行

……

## 参考资料

- [<u>图解大模型训练之：张量模型并行（TP）</u>](https://zhuanlan.zhihu.com/p/622212228)
- [<u>【AI系统】张量并行</u>](https://zhuanlan.zhihu.com/p/8375598222)
