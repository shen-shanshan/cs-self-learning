# TODO

## 输出博客

- [ ] 大模型 LoRA 微调的数学原理
- [ ] 大模型 LoRA/QLoRA/LongLoRA 微调原理与对比
- [ ] 基于 Ascend NPU & llama Factory 进行 LoRA 微调

## 遗留问题

- 为什么可以这样拆分？怎么拆？（SVD 分解）
- 这样拆为什么有效？（经实验证明可以）
- 为什么要这样初始化？（矩阵 A 和 B 只要有一个为 0 就行）
- 与其它竞品相比的优势？
