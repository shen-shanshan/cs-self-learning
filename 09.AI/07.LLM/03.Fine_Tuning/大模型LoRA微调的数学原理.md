# 大模型 LoRA 微调的数学原理

## 一、概述

**LoRA**（Low-Rank Adaptation，低秩适配器）是目前非常热门的大模型微调技术之一，网上已经有许多关于其原理的分析和讲解，本文将着重从 LoRA 背后的数学原理进行解读。

本文主要用于个人学习记录，如有错误欢迎指正。

## 二、背景介绍

### 2.1 基本概念

**大模型微调（Fine-tuning）**：基于已经训练好的预训练模型，针对特定的下游任务，在特定领域的数据集上进行二次训练，以提升模型在特定任务上的表现。

- **全量微调**：在下游任务的训练中，对预训练模型的每一个参数都做更新（训练代价昂贵）；
- **局部微调**：冻结（不更新）预训练模型的权重，只对部分增量权重进行训练，从而有效降低训练的代价（实用性更高）。

### 2.2 研究现状

在 LoRA 微调技术出现之前，现有的大模型微调技术存在以下缺点：

- **Adapter Tuning**：在模型中添加额外的 Adapter 层，并只针对这些 Adapter 的权重进行训练。这将导致模型整体的层数变深，从而使模型的训练/推理耗时增加（因为新增的 Adapter 层与模型中的其它层是串行的，无法充分利用硬件能力进行并行计算）；
- **Prefix Tuning**：对输入数据增加前缀（prefix），并只针对这些 prefix token 进行训练（prefix 的作用是引导模型提取输入中的特定信息，进而更好地生成结果）。这将导致输入数据中有效信息的减少，因为一般输入数据的长度是固定的，而增加的 prefix 将会占用原本输入数据的空间，从而使输入文字表达力下降。

## 三、LoRA 基本原理

### 3.1 LoRA 模型结构

LoRA 使用 $\mathbf{A}$ 和 $\mathbf{B}$ 两个与原模型并行的低秩矩阵来代替原本的增量权重矩阵 $\mathbf{\Delta W}$，从而可以在保证模型性能的同时，有效降低需要训练的参数量。

![LoRA基本原理](./images/LoRA.png)

对于输入 $\mathbf{x}$，模型的输出 $\mathbf{h}$ 为：

$$
\mathbf{h} = \mathbf{W}\mathbf{x} + \mathbf{\Delta W}\mathbf{x} \approx \mathbf{W}\mathbf{x} + \mathbf{B}\mathbf{A}\mathbf{x}
$$

> 其中：$\mathbf{W}、\mathbf{\Delta W} \in \mathbb{R}^{d*d}$；$\mathbf{A} \in \mathbb{R}^{r*d}$（初始化为正态分布）；$\mathbf{B} \in \mathbb{R}^{d*r}$（初始化为零）；r 为矩阵 $\mathbf{\Delta W}$ 的秩。

### 3.2 LoRA 的优点

- **节约内存**：在训练时，需要更新的权重数量从 `d*d` 下降到了 `2*d*r`，显著降低了模型微调对硬件性能的要求；
- **保证性能**：在推理时，可以将 LoRA 的权重直接合并到预训练权重中，从而可以保证推理的速度不受影响；
- **灵活易用**：针对不同的下游任务，用户可以训练不同的 LoRA，并且在微调完成后，只需要保存新增的这一部分权重（不同任务间共享预训练模型的权重），相比于存储整个模型的权重，只需要极少的内存。

## 四、LoRA 的数学原理与论文实验分析

简单介绍完了 LoRA 的基本原理，下面将针对以下几个问题进行分析和说明，这些问题也是我在刚开始学习 LoRA 时产生的疑惑。

- 为什么可以将 $\mathbf{\Delta W}$ 拆分为 $\mathbf{A}$ 和 $\mathbf{B}$？这样做为什么是有效的？
- r 作为一个超参数，它的取值是如何影响 LoRA 的表现的？
- $\mathbf{A}$ 和 $\mathbf{B}$ 为什么要这样初始化？

### 4.1 LoRA 的有效性分析

……

## 素材

$\mathbf{A}$
$\mathbf{B}$
$\mathbf{W}$
$\mathbf{\Delta W}$
