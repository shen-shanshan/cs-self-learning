# AI è®­ç»ƒ & æ¨ç†å¸¸ç”¨ä¾èµ–åº“

## PyTorch

### torch

â€¦â€¦

### torchaudio

`torchaudio` æ˜¯ PyTorch å®˜æ–¹ç”¨äºå¤„ç†éŸ³é¢‘æ•°æ®å’Œè¿›è¡ŒéŸ³é¢‘ç›¸å…³æ·±åº¦å­¦ä¹ ä»»åŠ¡çš„å·¥å…·åŒ…ï¼Œæä¾›äº†éŸ³é¢‘æ•°æ®çš„åŠ è½½å’Œä¿å­˜ã€é¢‘è°±åˆ†æã€é¢„è®­ç»ƒçš„éŸ³é¢‘æ¨¡å‹ï¼ˆæ”¯æŒéŸ³é¢‘åˆ†ç±»ã€è¯­éŸ³è¯†åˆ«ç­‰ä»»åŠ¡ï¼‰ã€ä¸ PyTorch çš„æ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨é›†æˆç­‰åŠŸèƒ½ã€‚

`torchaudio.load()` æ˜¯ `torchaudio` åº“ä¸­çš„ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºåŠ è½½éŸ³é¢‘æ–‡ä»¶å¹¶è¿”å›éŸ³é¢‘æ•°æ®åŠå…¶é‡‡æ ·ç‡ã€‚å®ƒå¯ä»¥æ–¹ä¾¿åœ°å°†éŸ³é¢‘æ–‡ä»¶åŠ è½½åˆ° PyTorch çš„å¼ é‡ä¸­ï¼Œä»¥ä¾¿è¿›è¡Œåç»­çš„éŸ³é¢‘å¤„ç†å’Œæ·±åº¦å­¦ä¹ ä»»åŠ¡ã€‚

å…·ä½“åŠŸèƒ½åŒ…æ‹¬ï¼š

- è¯»å–éŸ³é¢‘æ–‡ä»¶ï¼šæ”¯æŒå¤šç§éŸ³é¢‘æ ¼å¼ï¼ˆå¦‚ WAVã€MP3 ç­‰ï¼‰ï¼›
- è¿”å›æ•°æ®å’Œé‡‡æ ·ç‡ï¼šè¿”å›ä¸¤ä¸ªå€¼ï¼šéŸ³é¢‘ä¿¡å·çš„å¼ é‡è¡¨ç¤ºï¼ˆé€šå¸¸æ˜¯æµ®ç‚¹æ•°ï¼‰å’ŒéŸ³é¢‘çš„é‡‡æ ·ç‡ï¼ˆHzï¼‰ï¼Œæ–¹ä¾¿åç»­å¤„ç†å’Œåˆ†æã€‚

`torchaudio.functional.resample(y, orig_freq=xxx, new_freq=xxx)` æ˜¯ç”¨äºå¯¹éŸ³é¢‘ä¿¡å·è¿›è¡Œé‡é‡‡æ ·çš„å‡½æ•°ã€‚è¯¥å‡½æ•°é€‚ç”¨äºéœ€è¦è°ƒæ•´éŸ³é¢‘é‡‡æ ·ç‡çš„åœºæ™¯ï¼Œå¦‚åŒ¹é…ä¸åŒéŸ³é¢‘æºçš„é‡‡æ ·é¢‘ç‡æˆ–å‡†å¤‡éŸ³é¢‘æ•°æ®ä»¥ä¾›æ¨¡å‹è®­ç»ƒã€‚

å…·ä½“åŠŸèƒ½åŒ…æ‹¬ï¼š

- é‡é‡‡æ ·éŸ³é¢‘ä¿¡å·ï¼šå°†è¾“å…¥çš„éŸ³é¢‘å¼ é‡ `y` ä»åŸå§‹é‡‡æ ·é¢‘ç‡ `orig_freq` è½¬æ¢åˆ°æ–°çš„é‡‡æ ·é¢‘ç‡ `new_freq`ï¼›
- ä¿æŒéŸ³è´¨ï¼šé€šè¿‡é€‚å½“çš„ç®—æ³•ï¼Œå°½é‡å‡å°‘é‡é‡‡æ ·è¿‡ç¨‹ä¸­çš„å¤±çœŸï¼Œç¡®ä¿éŸ³è´¨å°½å¯èƒ½ä¿æŒã€‚

> å‚è€ƒèµ„æ–™ï¼š
>
> - [<u>TorchAudio Documentation</u>](https://pytorch.org/audio/stable/index.html)ï¼›
> - [<u>TorchAudio ç®€ä»‹</u>](https://datawhalechina.github.io/thorough-pytorch/%E7%AC%AC%E5%85%AB%E7%AB%A0/8.5%20%E9%9F%B3%E9%A2%91%20-%20torchaudio.html#)ã€‚

## NLP

### transformers

â€¦â€¦

> å‚è€ƒèµ„æ–™ï¼š
>
> - [<u>Transformers GitHub åœ°å€</u>](https://github.com/huggingface/transformers)ï¼›
> - [<u>ğŸ¤— Transformers</u>](https://huggingface.co/docs/transformers/en/index)ã€‚

## Audio

### vocos

`vocos` ä¸»è¦ç”¨äºéŸ³é¢‘å¤„ç†å’Œåˆæˆä»»åŠ¡ï¼Œå¯ä»¥æ”¯æŒå£°ç å™¨ï¼ˆvocoderï¼‰æ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†ã€‚

- å£°ç å™¨æ”¯æŒï¼šå®ç°äº†å¤šç§å£°ç å™¨æ¨¡å‹ï¼Œå¦‚ WaveNet å’Œ WaveRNNï¼Œèƒ½å¤Ÿå°†æ¢…å°”è°±ï¼ˆMel-spectrogramï¼‰è½¬æ¢ä¸ºéŸ³é¢‘æ³¢å½¢ï¼›
- éŸ³é¢‘å¤„ç†ï¼šæä¾›äº†ä¸€äº›éŸ³é¢‘é¢„å¤„ç†å’Œåå¤„ç†çš„å·¥å…·ï¼Œä»¥ä¾¿äºè¾“å…¥å’Œè¾“å‡ºéŸ³é¢‘ï¼›
- æ¨¡å‹è®­ç»ƒï¼šæ”¯æŒå£°å­¦æ¨¡å‹çš„è®­ç»ƒå’Œè¯„ä¼°ã€‚

`vocos` åŸºäº GAN è¿›è¡Œè®­ç»ƒï¼Œé’ˆå¯¹é¢‘è°±ç³»æ•°è¿›è¡Œå»ºæ¨¡ï¼Œèƒ½å¤Ÿæ ¹æ®é¢‘åŸŸçš„æ¢…å°”é¢‘è°±ï¼Œé€šè¿‡å‚…ç«‹å¶é€†å˜æ¢å¿«é€Ÿé‡å»ºæ—¶åŸŸéŸ³é¢‘ä¿¡å·ã€‚

> å‚è€ƒèµ„æ–™ï¼š
>
> - [<u>Vocos GitHub åœ°å€</u>](https://github.com/gemelo-ai/vocos)ã€‚

## æ€§èƒ½ä¼˜åŒ–

### numba

`numba`ï¼špython çš„ JITï¼ˆå³æ—¶ç¼–è¯‘ï¼‰åº“ã€‚

> å®˜æ–¹è¯´æ˜ï¼šNumba generates optimized machine code from pure Python code using the LLVM compiler infrastructure. With a few simple annotations, array-oriented and math-heavy Python code can be just-in-time optimized to performance similar as C, C++ and Fortran, without having to switch languages or Python interpreters.

JITï¼ˆå³æ—¶ç¼–è¯‘ï¼‰ï¼šå½“æŸæ®µä»£ç å³å°†ç¬¬ä¸€æ¬¡è¢«æ‰§è¡Œæ—¶è¿›è¡Œç¼–è¯‘ã€‚åŠ¨æ€åœ°å°†é«˜çº§è¯­è¨€ç¼–å†™çš„ä»£ç è½¬æ¢ä¸ºæœºå™¨ç ï¼Œå¯ä»¥ç›´æ¥ç”±è®¡ç®—æœºçš„å¤„ç†å™¨æ‰§è¡Œï¼Œè¿™æ˜¯åœ¨è¿è¡Œæ—¶å®Œæˆçš„ï¼Œä¹Ÿå°±æ˜¯ä»£ç æ‰§è¡Œä¹‹å‰ï¼Œå› æ­¤ç§°ä¸ºâ€œå³æ—¶â€ã€‚

ä¸¤ç§æ¨¡å¼ï¼š

- `nopython`ï¼šThe behaviour of the nopython compilation mode is to essentially compile the decorated function so that it will run entirely without the involvement of the Python interpreter. This is the recommended and best-practice way to use the Numba jit decorator as it leads to the best performanceï¼›
- `object`ï¼šThis achieved through using the `forceobj=True` key word argument to the `@jit` decorator. In this mode Numba will compile the function with the assumption that everything is a Python object and essentially run the code in the interpreter. Specifying `looplift=True` might gain some performance over pure object mode as Numba will try and compile loops into functions that run in machine code, and it will run the rest of the code in the interpreter. For best performance avoid using object mode mode in general.

ç¤ºä¾‹ï¼š

```python
from numba import jit

@jit
def function(): 
    # ...
```

numba åœ¨é¦–æ¬¡è°ƒç”¨å‡½æ•°çš„æ—¶å€™è¿›è¡Œäº†ç¼–è¯‘ï¼Œä½†å½“ç¼–è¯‘å‘ç”Ÿåï¼Œnumba ä¼šå°†è¯¥å‡½æ•°çš„æœºå™¨ç è¿›è¡Œç¼“å­˜ï¼Œå¦‚æœå†æ¬¡è°ƒç”¨è¯¥å‡½æ•°ï¼Œå®ƒä¼šç›´æ¥ä»ç¼“å­˜ä¸­åŠ è½½ï¼Œè€Œä¸éœ€è¦å†æ¬¡ç¼–è¯‘ã€‚

> å®˜æ–¹è¯´æ˜ï¼šonce the compilation has taken place Numba caches the machine code version of your function for the particular types of arguments presented. If it is called again with the same types, it can reuse the cached version instead of having to compile again.

Numba for CUDA GPUsï¼š

Numba èƒ½å¤Ÿæ”¯æŒ CUDA GPU ç¼–ç¨‹ï¼Œèƒ½å¤Ÿè‡ªåŠ¨åœ°åœ¨ host ä¸ device ä¹‹é—´ä¼ é€’ Numpy æ•°ç»„ã€‚

> å‚è€ƒèµ„æ–™ï¼š
>
> - [<u>Numba GitHub åœ°å€</u>](https://github.com/numba/numba)ï¼›
> - [<u>Numba Documentation</u>](https://numba.readthedocs.io/en/stable/index.html)ï¼›
> - [<u>python è¿è¡ŒåŠ é€Ÿç¥å™¨â€”â€”numba åŸç†è§£æ</u>](https://blog.csdn.net/weixin_45977690/article/details/133886829)ï¼›
> - [<u>JIT å³æ—¶ç¼–è¯‘æŠ€æœ¯</u>](https://zhuanlan.zhihu.com/p/193035135)ã€‚
