# v0.6.5 版本特性分析

---

## Hardware Support

### AMD ROCm GGUF quantization

**特性分析**：在 AMD ROCm 上支持 GGUF 模型量化。

**模块**：`vllm/_custom_ops.py`。

**昇腾影响**：无。

**代码链接**：`https://github.com/vllm-project/vllm/pull/10254`。

---

### ARM AARCH64 enablement

**特性分析**：支持 ARM AARCH64 架构（支持 CPU 推理）。

**模块**：`csrc/cpu/attention.cpp`。

**昇腾影响**：无。

**代码链接**：`https://github.com/vllm-project/vllm/pull/9228`。

---

### TPU prefix caching

**特性分析**：在 TPU 上支持 prefix caching。

**模块**：`vllm/worker/tpu_model_runner.py`、`vllm/worker/tpu_worker.py`。

**昇腾影响**：无。

**代码链接**：`https://github.com/vllm-project/vllm/pull/10307`。

---

### XPU AWQ/GPTQ

**特性分析**：在 XPU 上支持 AWQ/GPTQ。

**模块**：`vllm/model_executor/layers/quantization`。

**昇腾影响**：`ipex_quant.py/IPEXConfig/override_quantization_method()` 会判断平台类型，若不是 CPU 和 XPU，则直接返回。后续昇腾支持 AWQ/GPTQ 量化能力，需要修改这一块的代码。

**代码链接**：`https://github.com/vllm-project/vllm/pull/10107/files`。

---

### CPU/Gaudi/HPU enhancements

**特性分析**：在 CPU 上支持 chunked-prefill 和 prefix-caching。

**模块**：``。

**昇腾影响**：无。

**代码链接**：`https://github.com/vllm-project/vllm/pull/10355`。

---

**特性分析**：HPUAttentionBackend lack of `get_name` method。

**模块**：`vllm/attention/backends/hpu_attn.py`。

**昇腾影响**：检查 NPU 的 AttentionBackend 是否实现该方法。

**代码链接**：`https://github.com/vllm-project/vllm/pull/10667`。

---

**特性分析**：在 Intel Gaudi (HPU) 上支持 LoRA。

**模块**：`lora`、`punica_wrapper`。

**昇腾影响**：

`vllm/lora/layers.py` 中增加 `hpu` 硬编码（HPU needs special handling to prune out dummy samples）。

`vllm/lora/punica_wrapper/punica_selector.py` 中增加 `hpu` 硬编码，根据平台返回不同的 `PunicaWrapper` 类。

**代码链接**：`https://github.com/vllm-project/vllm/pull/10565`。

---

**特性分析**：在 HPU 上为模型的 decoder layer 增加 `mark_step`。

**模块**：`vllm/worker/hpu_model_runner.py`。

**昇腾影响**：无。

**代码链接**：`https://github.com/vllm-project/vllm/pull/10239`。

---

**特性分析**：升级 neuron 到 2.20.2。

**模块**：`vllm/utils.py`。

**昇腾影响**：无。

**代码链接**：`https://github.com/vllm-project/vllm/pull/11016`。

---

**特性分析**：Jetson 支持 non-NVML CUDA mode。

**模块**：`vllm/platforms/cuda.py`。

**昇腾影响**：无。

**代码链接**：`https://github.com/vllm-project/vllm/pull/9735`。

---

## Performance & Scheduling

### Prefix-cache aware scheduling

**特性分析**：增加 Prefix Cache Aware Scheduling，在 Scheduler 中提前将新 token 与 prefix cache 进行匹配，从而提高缓存命中率，并提高推理吞吐量提高。

**模块**：`vllm/core/scheduler.py`、`vllm/core/block_manager.py`。

**昇腾影响**：无。

**代码链接**：`https://github.com/vllm-project/vllm/pull/10128`。

---

### sliding window support

**特性分析**：Flashinfer 支持滑动窗口。

**模块**：`vllm/attention/backends/flashinfer.py`。

**昇腾影响**：无。

**代码链接**：`https://github.com/vllm-project/vllm/pull/10462`。

---

### disaggregated prefill enhancements

**特性分析**：支持 disagg prefill。

**模块**：`worker`、`kv_transfer`。

**昇腾影响**：无。

**代码链接**：`https://github.com/vllm-project/vllm/pull/10502`。

---

### evictor optimization

**特性分析**：evictor v1 和 v2 性能优化。

**模块**：`vllm/core/evictor.py`。

**昇腾影响**：无。

**代码链接**：`https://github.com/vllm-project/vllm/pull/7209`。

---

## 模板

**特性分析**：。

**模块**：``。

**昇腾影响**：。

**代码链接**：``。
