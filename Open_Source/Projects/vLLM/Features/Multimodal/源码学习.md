# vllm 多模态相关源码学习

## vllm/multimodal

`base.py` & `inputs.py`:

```python
class MultiModalPlaceholderMap:
    """
    Relates multi-modal embeddings to their corresponding placeholders.

    Note: This is only used in V0.
    """
    src_ranges: list[range]
    src_len: int
    dest_ranges: list[range]
    dest_len: int

    def from_seq_group(...):
        """
        Returns the multi-modal items that intersect with the portion of a
        prompt (``seq_group``) represented by ``positions``, as well as a
        ``MultiModalPlaceholderMap`` that relates the multi-modal embedding
        vectors to their corresponding placeholders.

        Examples:

        Prompt:    |AAAA BBBB What's in these images?|
        Positions: |.................................|

            images      = [A, B]
            src_ranges  = [(0, 4), (4, 8)]
            dest_ranges = [(0, 4), (5, 9)]

        Prompt:    |AAAA BBBB What's in these images?|
        Positions: |  .....                          |

            images      = [A, B]
            src_ranges  = [(2, 4), (4, 6)]
            dest_ranges = [(0, 2), (3, 5)]

        Prompt:    |AAAA BBBB What's in these images?|
        Positions: |     .........                   |

            images      = [B]
            src_ranges  = [(0, 4)]
            dest_ranges = [(0, 4)]

        Prompt:    |AAAA BBBB What's in these images?|
        Positions: |          .......................|

            images      = []
            src_ranges  = []
            dest_ranges = []

        个人理解：
        src_ranges：是在 position 范围内，当前位置在所有图像源文件（无缝衔接）中的索引；
        dest_ranges：是在 position 范围内，当前位置在整个序列（包含空格）中的索引。
        """
        seq_mm_data = seq_group.multi_modal_data  # MultiModalKwargs
        seq_mm_placeholders = seq_group.multi_modal_placeholders  # MultiModalPlaceholderDict

        for modality, placeholders in seq_mm_placeholders.items():
            placeholder_map = MultiModalPlaceholderMap()
            placeholder_map.append_items_from_seq_group(...)
            placeholder_maps[modality] = placeholder_map

        return seq_mm_data, placeholder_maps


# NOTE: UserDict is for V0 compatibility.
# V1 should access individual items via `get_item`.
class MultiModalKwargs(UserDict[str, NestedTensors]):
    """
    A dictionary that represents the keyword arguments to
    {meth}`~torch.nn.Module.forward`.

    The metadata `items` enables us to obtain the keyword arguments
    corresponding to each data item in {class}`MultiModalDataItems`, via
    {meth}`get_item` and {meth}`get_items`.
    """


MultiModalPlaceholderDict: TypeAlias = Mapping[str, Sequence[PlaceholderRange]]
    """
    A dictionary containing placeholder ranges for each modality.
    """

class PlaceholderRange:
    """
    Placeholder location information for multi-modal data.

    Example:

    Prompt: `AAAA BBBB What is in these images?`

    Images A and B will have:

    A: PlaceholderRange(offset=0, length=4)
    B: PlaceholderRange(offset=5, length=4)
    """
    offset: int
    length: int
```

SequenceGroupMetadata:

- multi_modal_data: Optional[MultiModalKwargs] = None
  - MultiModalKwargsItem
    - MultiModalFieldElem
- multi_modal_placeholders: Optional[MultiModalPlaceholderDict] = None`
  - key: modality (str)
  - value: placeholders (Sequence[PlaceholderRange])

MediaIO (`base.py`):

- `image.py`: ImageMediaIO / ImageEmbeddingMediaIO
- `vedio.py`: VideoMediaIO
- `audio.py`: AudioMediaIO

---

`parse.py`:

MultiModalDataItems:

- ModalityDataItems
  - ProcessorBatchItems
    - AudioProcessorItems
    - ImageProcessorItems
    - VideoProcessorItems
  - EmbeddingItems
    - AudioEmbeddingItems
    - ImageEmbeddingItems
    - VideoEmbeddingItems
  - DictEmbeddingItems

```python
class MultiModalDataItems(UserDict[str, ModalityDataItems[Any, Any]]):
    """
    As {data}`~vllm.multimodal.inputs.MultiModalDataDict`, but normalized
    such that each entry corresponds to a list.
    """


class ModalityDataItems(ABC, Generic[_T, _I]):
    """
    Represents data items for a modality in {class}`MultiModalDataItems`.
    """

    def __init__(self, data: _T, modality: str) -> None:
        super().__init__()

        self.data = data
        # ProcessorBatchItems: Sequence[HfAudioItem] / Sequence[HfImageItem] / Sequence[HfVideoItem]
        # EmbeddingItems: Union[torch.Tensor, list[torch.Tensor]]
        self.modality = modality  # "audio" / "image" / "video"
```

---

`processing.py`:

```python
class BaseMultiModalProcessor(ABC, Generic[_I]):
    """
    Abstract base class to process multi-modal inputs to be used in vLLM.

    Not to be confused with {class}`transformers.ProcessorMixin`.
    """


class EncDecMultiModalProcessor(BaseMultiModalProcessor[_I]):
    # ...
```

---

`registry.py`:

```python
class MultiModalRegistry:
    """
    A registry that dispatches data processing according to the model.
    """
```
