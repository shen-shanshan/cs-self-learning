# vllm 多模态相关源码学习

## vllm/multimodal

`base.py` & `inputs.py`:

```python
class MultiModalPlaceholderMap:
    """
    Relates multi-modal embeddings to their corresponding placeholders.

    Note: This is only used in V0.
    """
    src_ranges: list[range]
    src_len: int
    dest_ranges: list[range]
    dest_len: int

    def from_seq_group(...):
        """
        Returns the multi-modal items that intersect with the portion of a
        prompt (``seq_group``) represented by ``positions``, as well as a
        ``MultiModalPlaceholderMap`` that relates the multi-modal embedding
        vectors to their corresponding placeholders.

        Examples:

        Prompt:    |AAAA BBBB What's in these images?|
        Positions: |.................................|

            images      = [A, B]
            src_ranges  = [(0, 4), (4, 8)]
            dest_ranges = [(0, 4), (5, 9)]

        Prompt:    |AAAA BBBB What's in these images?|
        Positions: |  .....                          |

            images      = [A, B]
            src_ranges  = [(2, 4), (4, 6)]
            dest_ranges = [(0, 2), (3, 5)]

        Prompt:    |AAAA BBBB What's in these images?|
        Positions: |     .........                   |

            images      = [B]
            src_ranges  = [(0, 4)]
            dest_ranges = [(0, 4)]

        Prompt:    |AAAA BBBB What's in these images?|
        Positions: |          .......................|

            images      = []
            src_ranges  = []
            dest_ranges = []

        个人理解：
        src_ranges：是在 position 范围内，当前位置在所有图像源文件（无缝衔接）中的索引；
        dest_ranges：是在 position 范围内，当前位置在整个序列（包含空格）中的索引。
        """
        seq_mm_data = seq_group.multi_modal_data  # MultiModalKwargs
        seq_mm_placeholders = seq_group.multi_modal_placeholders  # MultiModalPlaceholderDict

        for modality, placeholders in seq_mm_placeholders.items():
            placeholder_map = MultiModalPlaceholderMap()
            placeholder_map.append_items_from_seq_group(...)
            placeholder_maps[modality] = placeholder_map

        return seq_mm_data, placeholder_maps


# NOTE: UserDict is for V0 compatibility.
# V1 should access individual items via `get_item`.
class MultiModalKwargs(UserDict[str, NestedTensors]):
    """
    A dictionary that represents the keyword arguments to
    {meth}`~torch.nn.Module.forward`.

    The metadata `items` enables us to obtain the keyword arguments
    corresponding to each data item in {class}`MultiModalDataItems`, via
    {meth}`get_item` and {meth}`get_items`.
    """


MultiModalPlaceholderDict: TypeAlias = Mapping[str, Sequence[PlaceholderRange]]
    """
    A dictionary containing placeholder ranges for each modality.
    """

class PlaceholderRange:
    """
    Placeholder location information for multi-modal data.

    Example:

    Prompt: `AAAA BBBB What is in these images?`

    Images A and B will have:

    A: PlaceholderRange(offset=0, length=4)
    B: PlaceholderRange(offset=5, length=4)
    """
    offset: int
    length: int
```

SequenceGroupMetadata:

- multi_modal_data: Optional[MultiModalKwargs] = None
  - MultiModalKwargsItem
    - MultiModalFieldElem
- multi_modal_placeholders: Optional[MultiModalPlaceholderDict] = None
  - key: modality (str)
  - value: placeholders (Sequence[PlaceholderRange])

MediaIO (`base.py`):

- `image.py`: ImageMediaIO / ImageEmbeddingMediaIO
- `vedio.py`: VideoMediaIO
- `audio.py`: AudioMediaIO

---

`parse.py`:

MultiModalDataItems:

- ModalityDataItems
  - ProcessorBatchItems
    - AudioProcessorItems
    - ImageProcessorItems
    - VideoProcessorItems
  - EmbeddingItems
    - AudioEmbeddingItems
    - ImageEmbeddingItems
    - VideoEmbeddingItems
  - DictEmbeddingItems

```python
class MultiModalDataItems(UserDict[str, ModalityDataItems[Any, Any]]):
    """
    As {data}`~vllm.multimodal.inputs.MultiModalDataDict`, but normalized
    such that each entry corresponds to a list.
    """


class ModalityDataItems(ABC, Generic[_T, _I]):
    """
    Represents data items for a modality in {class}`MultiModalDataItems`.
    """

    def __init__(self, data: _T, modality: str) -> None:
        super().__init__()

        self.data = data
        # ProcessorBatchItems: Sequence[HfAudioItem] / Sequence[HfImageItem] / Sequence[HfVideoItem]
        # EmbeddingItems: Union[torch.Tensor, list[torch.Tensor]]
        self.modality = modality  # "audio" / "image" / "video"
```

---

`processing.py`:

```python
class BaseMultiModalProcessor(ABC, Generic[_I]):
    """
    Abstract base class to process multi-modal inputs to be used in vLLM.

    Not to be confused with {class}`transformers.ProcessorMixin`.
    """
    def __call__(...):
        return self.apply(...)

    def apply(
        self,
        prompt: Union[str, list[int]],
        mm_data: MultiModalDataDict,
        hf_processor_mm_kwargs: Mapping[str, object],
        return_mm_hashes: bool = False,
    ) -> MultiModalInputs:
        """
        Process multi-modal inputs to be used in vLLM.

        The main steps are:

        1. Apply HF Processor on prompt text and multi-modal data together,
           outputting token IDs and processed tensors.
        2. Find and update sequences in the token IDs with placeholder tokens.
           The number of placeholder tokens equals the feature size of the
           multi-modal data outputted by the multi-modal encoder.
        3. Extract information about the placeholder tokens from the
           processed token IDs.
        """
        mm_items = self._to_mm_items(mm_data)

        (
            prompt_ids,
            mm_kwargs,
            mm_hashes,
            is_update_applied,
        ) = self._cached_apply_hf_processor(
            prompt,
            mm_items,
            hf_processor_mm_kwargs,
            return_mm_hashes=return_mm_hashes,
        )

        prompt_ids, prompt, mm_placeholders = self._maybe_apply_prompt_updates(
            mm_items=mm_items,
            hf_processor_mm_kwargs=hf_processor_mm_kwargs,
            prompt_ids=prompt_ids,
            mm_kwargs=mm_kwargs,
            is_update_applied=is_update_applied,
        )

        mm_placeholder_ranges = {
            modality: [item.to_range() for item in placeholders]
            for modality, placeholders in mm_placeholders.items()
        }

        return MultiModalInputs(
            type="multimodal",
            prompt=prompt,
            prompt_token_ids=prompt_ids,
            mm_kwargs=mm_kwargs,
            mm_hashes=mm_hashes,
            mm_placeholders=mm_placeholder_ranges,
        )
```

---

`registry.py`:

```python
class MultiModalRegistry:
    """
    A registry that dispatches data processing according to the model.
    """
    def __init__(self) -> None:
        self._processor_factories = ClassRegistry[nn.Module,
                                                  _ProcessorFactories]()

        self._processing_cache = ProcessingCache(VLLM_MM_INPUT_CACHE_GIB)

    def register_processor(
        self,
        processor: MultiModalProcessorFactory[_I],
        *,
        info: ProcessingInfoFactory[_I],
        dummy_inputs: DummyInputsBuilderFactory[_I],
    ):
        """
        Register a multi-modal processor to a model class. The processor
        is constructed lazily, hence a factory method should be passed.

        When the model receives multi-modal data, the provided function is
        invoked to transform the data into a dictionary of model inputs.

        Info:
            [mm-processing][]
        """

        def wrapper(model_cls: N) -> N:
            if self._processor_factories.contains(model_cls, strict=True):
                logger.warning(
                    "Model class %s already has a multi-modal processor "
                    "registered to %s. It is overwritten by the new one.",
                    model_cls, self)

            self._processor_factories[model_cls] = _ProcessorFactories(
                info=info,
                dummy_inputs=dummy_inputs,
                processor=processor,
            )

            return model_cls

        return wrapper

    def create_processor(
        self,
        model_config: "ModelConfig",
        *,
        tokenizer: Optional[AnyTokenizer] = None,
        disable_cache: Optional[bool] = None,
    ) -> BaseMultiModalProcessor[BaseProcessingInfo]:
        """
        Create a multi-modal processor for a specific model and tokenizer.

        Info:
            [mm-processing][]
        """
        if not model_config.is_multimodal_model:
            raise ValueError(f"{model_config.model} is not a multimodal model")

        if tokenizer is None:
            tokenizer = cached_tokenizer_from_config(model_config)
        if disable_cache is None:
            mm_config = model_config.get_multimodal_config()
            disable_cache = mm_config.disable_mm_preprocessor_cache

        model_cls = self._get_model_cls(model_config)
        factories = self._processor_factories[model_cls]

        ctx = InputProcessingContext(model_config, tokenizer)
        cache = None if disable_cache else self._processing_cache

        return factories.build_processor(ctx, cache=cache)
```

---

## vllm/model_executor/models

多模态模型注册：

```python
MULTIMODAL_REGISTRY = MultiModalRegistry()

@MULTIMODAL_REGISTRY.register_processor(
    Qwen2_5_VLMultiModalProcessor,
    info=Qwen2_5_VLProcessingInfo,
    dummy_inputs=Qwen2_5_VLDummyInputsBuilder)
class Qwen2_5_VLForConditionalGeneration(...): ...

class Qwen2_5_VLMultiModalProcessor(Qwen2VLMultiModalProcessor): ...
class Qwen2_5_VLProcessingInfo(Qwen2VLProcessingInfo): ...

# import Qwen2VLDummyInputsBuilder as Qwen2_5_VLDummyInputsBuilder
class Qwen2VLDummyInputsBuilder(BaseDummyInputsBuilder[Qwen2VLProcessingInfo]): ...

class Qwen2VLMultiModalProcessor(BaseMultiModalProcessor[Qwen2VLProcessingInfo]): ...

class Qwen2VLProcessingInfo(BaseProcessingInfo): ...

class BaseProcessingInfo:
    """Base class to provide the information necessary for data processing."""
    def __init__(self, ctx: InputProcessingContext) -> None:
        super().__init__()
        self.ctx = ctx

@dataclass(frozen=True)
class InputProcessingContext(InputContext):
    tokenizer: AnyTokenizer

@dataclass(frozen=True)
class InputContext:
    """Contains information about the model which may be used to modify the inputs."""
    model_config: "ModelConfig"
```

## vllm/v1/engine/processor.py

```python
# Process inputs, which includes:
# 1. Tokenize text prompt, with LoRA request if one exists.
# 2. For multimodal models with a merged preprocessor, preprocess
#    multimodal data and expand prompt token ids accordingly.
# 3. Apply prompt adapter to prompt token ids if one exists.
processed_inputs: ProcessorInputs = self.input_preprocessor.preprocess(...)
```

The problem here is that the parallelism is applied in `BaseMultiModalProcessor._call_hf_processor` which is many layers inside `BaseMultiModalProcessor.apply`.

---

## 关键 method

```python
# we process the text and multi-modal inputs separately, using dummy text to avoid HF errors. Since this skips HF's prompt updating code, we apply automatic prompt updating afterwards to keep the output tokens and multi-modal data consistent with each other
get_dummy_text()
_get_prompt_updates()
_apply_prompt_updates()
# our multi-modal processor can finally accept both text and token prompts with multi-modal data
_apply_hf_processor_main()

merge_multimodal_embeddings()
```
