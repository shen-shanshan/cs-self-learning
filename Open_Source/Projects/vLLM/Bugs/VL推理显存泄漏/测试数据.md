# 显存测试数据

启动服务：

```bash
watch -n 0.2 npu-smi info

VLLM_LOGGING_LEVEL=DEBUG \

vllm serve /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-8B-Instruct \
--tensor-parallel-size 2 \
--max-model-len 65536 \
--max-num-seqs 8 \
--max-num-batched-tokens 32768 \
--gpu-memory-utilization 0.95 \
--mm-processor-cache-gb 0
```

## 旧 profiling 方法

**gpu_xxx = 0.95 时：**

```bash
Loading model weights took 9.1948 GB

torch_peak after profiling is 13.11 G
non_torch_memory after profiling is 0.96 G
results of real profiling:
non_torch_memory after profiling is 1.26 G

Available memory: 43.40 GiB, total memory: 60.96 GiB
Available memory: 43.33 GiB, requested memory: 57.91 GiB (余量：3123.2 M)
```

**gpu_xxx = 0.9 时：**

```bash
torch_memory before profiling is 10.25 G
torch_peak before profiling is 9.45 G
non_torch_memory before profiling is 0.36 G

torch_memory after profiling is 9.75 G
torch_peak after profiling is 13.30 G
non_torch_memory after profiling is 0.91 G

# results of real profiling:
peak_memory after profiling is 13.30 G
non_torch_memory after profiling is 1.21 G  # updated: 

Available memory: 40.35 GiB, requested memory: 54.86 GiB
```

## 新 profiling 方法

**gpu_xxx = 0.95 时：**

服务启动前：3409 / 65536
服务启动后：59614 / 65536 (进程占用：56257)

```bash
torch_memory before init HCCL is 0.00 G
torch_peak before init HCCL is 0.00 G
non_torch_memory before init HCCL is 0.35 G

torch_memory after init HCCL is 0.00 G
torch_peak after init HCCL is 0.00 G
non_torch_memory after init HCCL is 0.35 G

# Loading weights took 4.29 seconds
Loading model weights took 9.1948 GB
torch_memory before profiling is 9.75 G
torch_peak before profiling is 9.45 G
non_torch_memory before profiling is 0.36 G
# Encoder cache will be initialized with a budget of 32768 tokens, and profiled with 2 image items of the maximum feature size. (2.25 G ？)
# Qwen3-VL dummy_mm_data target_width: 4096.
# Qwen3-VL dummy_mm_data target_height: 4096.

torch_memory after profiling multi-modal is 17.69 G
torch_peak after profiling multi-modal is 13.30 G
non_torch_memory after profiling multi-modal is 1.05 G

torch_memory after profiling, before empty_cache() is 19.13 G
torch_peak after profiling, before empty_cache() is 13.30 G
non_torch_memory after profiling, before empty_cache() is 1.07 G

torch_memory after profiling, after empty_cache() is 9.75 G
torch_peak after profiling, after empty_cache() is 13.30 G
non_torch_memory after profiling, after empty_cache() is 0.91 G
peak_activation_memory new is 4.11 G

Initial free memory: 60.61 GiB; Requested memory: 0.95 (util), 57.91 GiB
Free memory after profiling: 50.30 GiB (total), 47.60 GiB (within requested)

Total non KV cache memory: 13.61 GiB;  # 13.86 G -> 14.37 G
torch peak memory increase: 3.86 GiB;  # peak = 13.30, increase = 4.11 ?
non-torch forward increase memory: 0.56 GiB;  # 1.06 G
weights memory: 9.19 GiB.

Available KV cache memory: 44.30 GiB  # 44.05 G -> 43.54 G
GPU KV cache size: 645,120 tokens  # ... -> 633,984
# encoder_compute_budget: 32768
# encoder_cache_size: 32768
```

KV cache memory: **44.30** GiB + Total non KV cache memory: **13.61** GiB
= Requested memory **57.91** GiB
= Initial free memory: **60.61** GiB * **0.95**

Total non KV cache memory: **13.61** GiB
= torch peak memory increase: **3.86** GiB + non-torch forward increase memory: **0.56** GiB + weights memory: **9.19** GiB.

## 离线测试

29/60: OOM
Processing: MEITU_20251004_120005042.jpg

profiling:
mm_encoder_attention.py:141:forward_oot 峰值激活: 3.8 G
image_cache: 1 G (12.3 G)

> 问题：文本为 None，没考虑 _merge_multimodal_embeddings 操作。

image1:
kv_cache: 52.921 G
mm_encoder_attention.py:141:forward_oot: 1.884 G (54.805 G)
_merge_multimodal_embeddings: 1.908 G (54.829 G) 多了 24 M
cache: 384 M

image2:
peak: 1.8 G (54.9 G)
cache: 384 M

image3:
cache: 398.1 M
del image1 cache

OOM:

---

total_memory: 60.96 G
torch_memory: 59.38 G
torch_memory_peak: 54.77 G
non_torch_memory:

non_torch_memory: 1.15 G

---

offline:

```
torch_memory after profiling is 9.75 G
torch_peak after profiling is 13.30 G
non_torch_memory after profiling is 0.91 G
results of real profiling:
non_torch_memory after profiling is 1.21 G
Available memory: 43.39 GiB, requested memory: 57.91 GiB
```

---

```bash
# Upper limit of memory block splitting allowed (MB): Setting this parameter can prevent large memory blocks from being split.
export PYTORCH_NPU_ALLOC_CONF="max_split_size_mb:256"

# When operators on the communication stream have dependencies, they all need to be ended before being released for reuse. The logic of multi-stream reuse is to release the memory on the communication stream in advance so that the computing stream can be reused.
export PYTORCH_NPU_ALLOC_CONF="expandable_segments:True"

# Optimize operator delivery queue. This will affect the memory peak value, and may degrade if the memory is tight.
export TASK_QUEUE_ENABLE=2

# This will greatly improve the CPU bottleneck model and ensure the same performance for the NPU bottleneck model.
export CPU_AFFINITY_CONF=1
```

调整这3个参数
--mm-processor-cache-gb 0
--disable-mm-preprocessor-cache

--gpu-memory-utilization

--max-num-batched-tokens 32768 == 16384 * 2

if self.is_multimodal_model:
