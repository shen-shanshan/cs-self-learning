reset_mm_cache
clear_mm_cache

```python
class InputProcessor:
    self.mm_processor_cache = processor_cache_from_config(vllm_config, mm_registry)
    self.input_preprocessor = InputPreprocessor(
        self.model_config,
        tokenizer,
        mm_registry,
        mm_processor_cache=self.mm_processor_cache,
    )

class GPUModelRunner:
    # mm_hash -> encoder_output
    self.encoder_cache: dict[str, torch.Tensor] = {}

    def reset_mm_cache(self) -> None:
        if self.mm_budget:
            self.mm_budget.reset_cache()

class MultiModalBudget:
    self.cache = cache = processor_only_cache_from_config(model_config, mm_registry)
    self.cache.clear_cache()

class MultiModalProcessorOnlyCache:
    self._cache = MultiModalCache.get_lru_cache()

class LRUCache
```

---

MM Encoder Cache 相关代码：

- CPU 的预处理 Cache ？
- GPU 的特征 Cache ？

```python
class SchedulerConfig:
    def __post_init__():
        self.max_num_encoder_input_tokens = self.max_num_batched_tokens
        self.encoder_cache_size = self.max_num_batched_tokens

class Scheduler:
    def __init__():
        encoder_compute_budget, encoder_cache_size = compute_encoder_budget()  # -> compute_mm_encoder_budget()
        self.encoder_cache_manager = EncoderCacheManager(cache_size=encoder_cache_size)  # cache_size = 32768


class EncoderCacheManager:
    """Manages caching of encoder outputs for multimodal models in vLLM V1."""


class MultiModalBudget:
    def __init__():
        # Compute the encoder cache budget based on the model and scheduler configurations for a multimodal model.
        encoder_compute_budget, encoder_cache_size = compute_mm_encoder_budget(
            scheduler_config,
            max_tokens_by_modality,
        )

    def get_encoder_budget(self) -> int:
        return min(self.encoder_compute_budget, self.encoder_cache_size)


class GPUModelRunner:
    def __init__():
        # mm_hash -> encoder_output
        self.encoder_cache: dict[str, torch.Tensor] = {}  # GPU 的特征 Cache
        self.mm_budget = MultiModalBudget()
    
    def profile_run():
        mm_budget = self.mm_budget
        encoder_budget := mm_budget.get_encoder_budget()  # num_tokens
        # Create dummy batch of multimodal inputs.
        batched_dummy_mm_inputs = self._get_mm_dummy_batch()  # -> get_decoder_dummy_data()
        # Run multimodal encoder.
        dummy_encoder_outputs = self.model.embed_multimodal()
        for i, output in enumerate(dummy_encoder_outputs):
            self.encoder_cache[f"tmp_{i}"] = output
        self.encoder_cache.clear()
    
    def reset_mm_cache(self) -> None:
        if self.mm_budget:
            self.mm_budget.reset_cache()  # CPU 的预处理 Cache


class MultiModalProfiler:
    # Contains code for running memory profiling for multi-modal models.
    def get_decoder_dummy_data():
        mm_inputs = self._get_dummy_mm_inputs()

    def _get_dummy_mm_inputs():
        factory = self.dummy_inputs  # BaseDummyInputsBuilder
        processor_inputs = factory.get_dummy_processor_inputs()
        # Process multi-modal inputs to be used in vLLM.
        return self.processor.apply(
            prompt=processor_inputs.prompt,
            mm_data=processor_inputs.mm_data,
            hf_processor_mm_kwargs=processor_inputs.hf_processor_mm_kwargs,
            tokenization_kwargs=processor_inputs.tokenization_kwargs,
        )


class BaseDummyInputsBuilder:
    def __init__():
        self.info = info

    def get_dummy_processor_inputs():
        dummy_text = self.get_dummy_text()
        dummy_mm_data = self.get_dummy_mm_data()
        return ProcessorInputs(
            prompt=dummy_text,
            mm_data=dummy_mm_data,
            tokenization_kwargs=tokenization_kwargs,
        )

    def get_dummy_mm_data():
        # 抽象方法，不同模型去自定义自己的


class Qwen3VLDummyInputsBuilder:
    # self.info -> Qwen3VLProcessingInfo
    def get_dummy_mm_data():
        num_images = mm_counts.get("image", 0)
        num_videos = mm_counts.get("video", 0)
        # ...
        target_width, target_height = self.info.get_image_size_with_most_features()
        # ...
        return {
            "image": self._get_dummy_images(
                width=target_width,
                height=target_height,
                num_images=num_images,
                overrides=image_overrides,
            ),
            "video": self._get_dummy_videos(
                width=width,
                height=height,
                num_frames=target_num_frames,
                num_videos=num_videos,
            ),
        }


class Qwen3VLProcessingInfo(Qwen2VLProcessingInfo):
class Qwen2VLProcessingInfo:
    def get_image_size_with_most_features():
        # Qwen3-VL dummy_mm_data target_width: 4096.
        # Qwen3-VL dummy_mm_data target_height: 4096.
```

```
GPUModelRunner 多模态处理流程：

_preprocess()

    _execute_mm_encoder()
        curr_group_outputs: MultiModalEmbeddings
            A tensor of shape (num_items, feature_size, hidden_size)
        for ... group_mm_kwargs_by_modality:
            curr_group_outputs = model.embed_multimodal(**mm_kwargs_group)
        encoder_outputs.extend(curr_group_outputs)
        for mm_hash, output in zip(mm_hashes, encoder_outputs):
            self.encoder_cache[mm_hash] = output
    
    _gather_mm_embeddings()
```

---

profiling 流程：

```python
class EngineCore:
    def _initialize_kv_caches():
        available_gpu_memory = self.model_executor.determine_available_memory()
        self.available_gpu_memory_for_kv_cache = available_gpu_memory[0]  # 这里可以去掉 [0] ?
        ...
        kv_cache_configs = get_kv_cache_configs()
        generate_scheduler_kv_cache_config(kv_cache_configs)
        initialize_from_config(kv_cache_configs)

@dataclass
class MemorySnapshot:
    ...

@dataclass
class MemoryProfilingResult:
    ...

@contextlib.contextmanager
def memory_profiling():
    ...

class Worker():
    def determine_available_memory():
        ...

class GPUModelRunner():
    def profile_run():
        self._dummy_run()
    
    def _dummy_run():
        ...
```

EncoderCacheManager(cache_size=encoder_cache_size)  # cache_size = 32768 == max-num-batched-tokens

```python
class Scheduler():
    def __init__():
        encoder_compute_budget, encoder_cache_size = compute_encoder_budget()
        self.max_num_encoder_input_tokens = encoder_compute_budget
        self.encoder_cache_manager = EncoderCacheManager(cache_size=encoder_compute_budget)
    
    def schedule():
        # Encoder-related.
        scheduled_encoder_inputs: dict[str, list[int]] = {}
        encoder_compute_budget = self.max_num_encoder_input_tokens
        ...
        # First, schedule the RUNNING requests.
        req_index = 0
        while req_index < len(self.running) and token_budget > 0:
            request = self.running[req_index]
            ...
            encoder_inputs_to_schedule, num_new_tokens, new_encoder_compute_budget, ... = _try_schedule_encoder_inputs()
            ...
            # Encoder-related.
            if encoder_inputs_to_schedule:
                scheduled_encoder_inputs[request.request_id] = (
                    encoder_inputs_to_schedule
                )
                # Allocate the encoder cache.
                for i in encoder_inputs_to_schedule:
                    self.encoder_cache_manager.allocate(request, i)
                encoder_compute_budget = new_encoder_compute_budget

    def _try_schedule_encoder_inputs(..., encoder_compute_budget, ...):
        encoder_inputs_to_schedule: list[int] = []
        mm_features = request.mm_features
        # NOTE: since scheduler operates on the request level (possibly with
        # multiple encoder inputs per request), we need to create temporary
        # trackers for accounting at the encoder input level.
        mm_hashes_to_schedule = set()
        num_embeds_to_schedule = 0
        for i, mm_feature in enumerate(mm_features):
            num_encoder_tokens = mm_feature.mm_position.length
            num_encoder_embeds = mm_feature.mm_position.get_num_embeds
            ...
            self.encoder_cache_manager.check_and_update_cache(request, i)
            ...
            self.encoder_cache_manager.can_allocate(request, i, encoder_compute_budget, num_embeds_to_schedule)
            ...
            num_embeds_to_schedule += num_encoder_embeds
            encoder_compute_budget -= num_encoder_embeds
            mm_hashes_to_schedule.add(request.mm_features[i].identifier)  # hash
            encoder_inputs_to_schedule.append(i)  # index
```

---

```python
# torch peak memory usage
self.torch_peak = torch.cuda.memory_stats().get("allocated_bytes.all.peak", 0)

self.free_memory, self.total_memory = torch.cuda.mem_get_info()

self.cuda_memory = self.total_memory - self.free_memory

# memory that PyTorch gets from cuda
self.torch_memory = torch.cuda.memory_reserved()

self.non_torch_memory = self.cuda_memory - self.torch_memory
```

`@torch.inference_mode()` ?
