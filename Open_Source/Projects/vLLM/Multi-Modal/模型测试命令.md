# Multi-Modal Models Test

## Preparation

```bash
git fetch --tags
git tag -l
# vllm
git checkout -b v0.11.2 v0.11.2
# vllm-ascend
git checkout -b v0.11.0rc0 v0.11.0rc0

# vllm
# git reset --hard 17c540a993af88204ad1b78345c8a865cf58ce44
git rebase upstream/releases/v0.11.1
# vllm-ascend
main

tmux attach -t download
python download.py

eval "$(/root/miniconda3/bin/conda shell.bash hook)"
conda activate vllm

export HCCL_BUFFSIZE=1024
export VLLM_VERSION=""

def register_model(): pass
```

## Acc Test

```bash
pip install lm-eval

export VLLM_USE_MODELSCOPE=False
export HF_ENDPOINT="https://hf-mirror.com"
lm_eval \
--model vllm-vlm \
--model_args pretrained=/root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-7B-Instruct,max_model_len=4096 \
--tasks mmmu_val \
--batch_size auto
```

报错：

```bash
2025-10-20:09:40:15 INFO     [evaluator:574] Running generate_until requests
Adding requests:   0%|                                                                                                               | 0/900 [00:04<?, ?it/s]
Traceback (most recent call last):                                                                                                   | 0/900 [00:00<?, ?it/s]
  File "/root/miniconda3/envs/vllm/bin/lm_eval", line 7, in <module>
    sys.exit(cli_evaluate())
  File "/root/miniconda3/envs/vllm/lib/python3.10/site-packages/lm_eval/__main__.py", line 455, in cli_evaluate
    results = evaluator.simple_evaluate(
  File "/root/miniconda3/envs/vllm/lib/python3.10/site-packages/lm_eval/utils.py", line 456, in _wrapper
    return fn(*args, **kwargs)
  File "/root/miniconda3/envs/vllm/lib/python3.10/site-packages/lm_eval/evaluator.py", line 357, in simple_evaluate
    results = evaluate(
  File "/root/miniconda3/envs/vllm/lib/python3.10/site-packages/lm_eval/utils.py", line 456, in _wrapper
    return fn(*args, **kwargs)
  File "/root/miniconda3/envs/vllm/lib/python3.10/site-packages/lm_eval/evaluator.py", line 585, in evaluate
    resps = getattr(lm, reqtype)(cloned_reqs)
  File "/root/miniconda3/envs/vllm/lib/python3.10/site-packages/lm_eval/models/vllm_vlms.py", line 299, in generate_until
    cont = self._multimodal_model_generate(
  File "/root/miniconda3/envs/vllm/lib/python3.10/site-packages/lm_eval/models/vllm_vlms.py", line 154, in _multimodal_model_generate
    outputs = self.model.generate(
  File "/vllm-workspace/vllm/vllm/entrypoints/llm.py", line 393, in generate
    self._validate_and_add_requests(
  File "/vllm-workspace/vllm/vllm/entrypoints/llm.py", line 1516, in _validate_and_add_requests
    self._add_request(
  File "/vllm-workspace/vllm/vllm/entrypoints/llm.py", line 1569, in _add_request
    self.llm_engine.add_request(
  File "/vllm-workspace/vllm/vllm/v1/engine/llm_engine.py", line 230, in add_request
    prompt_str, request = self.processor.process_inputs(
  File "/vllm-workspace/vllm/vllm/v1/engine/processor.py", line 377, in process_inputs
    processed_inputs: ProcessorInputs = self.input_preprocessor.preprocess(
  File "/vllm-workspace/vllm/vllm/inputs/preprocess.py", line 644, in preprocess
    return self._process_decoder_only_prompt(
  File "/vllm-workspace/vllm/vllm/inputs/preprocess.py", line 614, in _process_decoder_only_prompt
    prompt_comps = self._prompt_to_llm_inputs(
  File "/vllm-workspace/vllm/vllm/inputs/preprocess.py", line 393, in _prompt_to_llm_inputs
    return self._process_text(
  File "/vllm-workspace/vllm/vllm/inputs/preprocess.py", line 343, in _process_text
    inputs = self._process_multimodal(
  File "/vllm-workspace/vllm/vllm/inputs/preprocess.py", line 242, in _process_multimodal
    mm_input = mm_processor.apply(
  File "/vllm-workspace/vllm/vllm/multimodal/processing.py", line 2045, in apply
    prompt_ids, prompt, mm_placeholders = self._maybe_apply_prompt_updates(
  File "/vllm-workspace/vllm/vllm/multimodal/processing.py", line 1997, in _maybe_apply_prompt_updates
    ) = self._apply_prompt_updates(
  File "/vllm-workspace/vllm/vllm/multimodal/processing.py", line 1919, in _apply_prompt_updates
    assert update_idx is not None, (
AssertionError: Failed to apply prompt replacement for mm_items['image'][0]
[ERROR] 2025-10-20-09:40:20 (PID:31615, Device:-1, RankID:-1) ERR99999 UNKNOWN applicaiton exception
```

## Qwen2-Audio-7B-Instruct

```python
from vllm.assets.audio import AudioAsset

audio_url = AudioAsset("winning_call").url
print(audio_url)
# https://vllm-public-assets.s3.us-west-2.amazonaws.com/multimodal_asset/winning_call.ogg
```

```bash
vllm serve /root/.cache/modelscope/models/Qwen/Qwen2-Audio-7B-Instruct \
--max_model_len 4096 \
--trust-remote-code \
--tensor-parallel-size 2 \
--limit-mm-per-prompt '{"audio":2}' \
--chat-template /home/sss/vllm/template_qwen2_audio.jinja \
--enforce-eager

curl -X POST http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "/root/.cache/modelscope/models/Qwen/Qwen2-Audio-7B-Instruct",
        "messages": [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": [
                {"type": "audio_url", "audio_url": {"url": "https://vllm-public-assets.s3.us-west-2.amazonaws.com/multimodal_asset/winning_call.ogg"}},
                {"type": "text", "text": "What is in this audio? How does it sound?"}
            ]}
        ],
        "max_tokens": 100
    }'

# output:
{"id":"chatcmpl-31f5f698f6734a4297f6492a830edb3f","object":"chat.completion","created":1761097383,"model":"/root/.cache/modelscope/models/Qwen/Qwen2-Audio-7B-Instruct","choices":[{"index":0,"message":{"role":"assistant","content":"The audio contains a background of a crowd cheering, a ball bouncing, and an object being hit. A man speaks in English saying 'and the o one pitch on the way to edgar martinez swung on and lined out.' The speech has a happy mood.","refusal":null,"annotations":null,"audio":null,"function_call":null,"tool_calls":[],"reasoning_content":null},"logprobs":null,"finish_reason":"stop","stop_reason":null,"token_ids":null}],"service_tier":null,"system_fingerprint":null,"usage":{"prompt_tokens":689,"total_tokens":743,"completion_tokens":54,"prompt_tokens_details":null},"prompt_logprobs":null,"prompt_token_ids":null,"kv_transfer_params":null}
```

## Qwen2-VL-7B-Instruct

```bash
vllm serve /root/.cache/modelscope/hub/models/Qwen/Qwen2-VL-2B \
--max_model_len 16384 \
--limit-mm-per-prompt '{"image":2}' \
--runner generate \
--enforce-eager

curl -X POST http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "/root/.cache/modelscope/hub/models/Qwen/Qwen2-VL-2B",
        "messages": [
            {"role": "user", "content": [
                {"type": "image_url", "image_url": {"url": "https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png"}},
                {"type": "text", "text": "What is the text in the illustrate? How does it look?"}
            ]}
        ],
        "max_tokens": 100
    }'

Failed to apply prompt replacement for mm_items['image'][0]
```

```python
from transformers import AutoProcessor
from vllm import LLM, SamplingParams
from qwen_vl_utils import process_vision_info

MODEL_PATH = "Qwen/Qwen2-VL-2B"

llm = LLM(
    model=MODEL_PATH,
    max_model_len=16384,
    limit_mm_per_prompt={"image": 10},
)

sampling_params = SamplingParams(
    max_tokens=512
)

image_messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {
        "role": "user",
        "content": [
            {
                "type": "image",
                "image": "https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png",
                "min_pixels": 224 * 224,
                "max_pixels": 1280 * 28 * 28,
            },
            {"type": "text", "text": "Please provide a detailed description of this image"},
        ],
    },
]

messages = image_messages

processor = AutoProcessor.from_pretrained(MODEL_PATH)
prompt = processor.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True,
)

image_inputs, _, _ = process_vision_info(messages, return_video_kwargs=True)

mm_data = {}
if image_inputs is not None:
    mm_data["image"] = image_inputs

llm_inputs = {
    "prompt": prompt,
    "multi_modal_data": mm_data,
}

outputs = llm.generate([llm_inputs], sampling_params=sampling_params)
generated_text = outputs[0].outputs[0].text

print(generated_text)
```

## Qwen2.5-VL-7B-Instruct

```bash
vllm serve /root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-7B-Instruct \
--max_model_len 16384 \
--max-num-batched-tokens 16384 \
--tensor-parallel-size 2 \
--enforce-eager

curl -X POST http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "/root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-7B-Instruct",
        "messages": [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": [
                {"type": "image_url", "image_url": {"url": "https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png"}},
                {"type": "text", "text": "What is the text in the illustrate? How does it look?"}
            ]}
        ],
        "max_tokens": 100
    }'

export VLLM_USE_MODELSCOPE=False
export HF_ENDPOINT="https://hf-mirror.com"
lm_eval \
--model vllm-vlm \
--model_args pretrained=/root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-7B-Instruct,max_model_len=4096 \
--tasks mmmu_val \
--batch_size auto
```

## Qwen3-VL-8B-Instruct

```bash
vllm serve /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-8B-Instruct \
--max_model_len 16384 \
--tensor-parallel-size 2 \
--enforce-eager

curl -X POST http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "/root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-8B-Instruct",
        "messages": [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": [
                {"type": "image_url", "image_url": {"url": "https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png"}},
                {"type": "text", "text": "What is the text in the illustrate? How does it look?"}
            ]}
        ],
        "max_tokens": 100
    }'
```

## Qwen3-VL-30B-A3B-Instruct

```bash
vllm serve /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-30B-A3B-Instruct \
--max_model_len 16384 \
--tensor-parallel-size 4 \
--enable-expert-parallel \
--enforce-eager

curl -X POST http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "/root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-30B-A3B-Instruct",
        "messages": [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": [
                {"type": "image_url", "image_url": {"url": "https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png"}},
                {"type": "text", "text": "What is the text in the illustrate? How does it look?"}
            ]}
        ],
        "max_tokens": 100
    }'

--max_model_len  # 手动设置的值不应超过模型本身的位置编码能力（max_position_embeddings）
"max_position_embeddings": 262144  # 这通常是模型在训练时见过的最长序列长度，也是该模型能保证效果的安全长度

vllm serve /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-30B-A3B-Instruct \
--max_model_len 262144 \
--tensor-parallel-size 4 \
--enable-expert-parallel \
--enforce-eager
```

## Qwen3-Omni-30B-A3B-Instruct

```bash
vllm serve /root/.cache/modelscope/hub/models/Qwen/Qwen3-Omni-30B-A3B-Instruct \
--max_model_len 16384 \
--tensor-parallel-size 4 \
--enable-expert-parallel \
--enforce-eager

curl -X POST http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "/root/.cache/modelscope/hub/models/Qwen/Qwen3-Omni-30B-A3B-Instruct",
        "messages": [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": [
                {"type": "image_url", "image_url": {"url": "https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png"}},
                {"type": "text", "text": "What is the text in the illustrate? How does it look?"}
            ]}
        ],
        "max_tokens": 100
    }'
```

## Qwen3-Omni-30B-A3B-Thinking

```bash
vllm serve /root/.cache/modelscope/hub/models/Qwen/Qwen3-Omni-30B-A3B-Thinking \
--max_model_len 32768 \
--tensor-parallel-size 2 \
--gpu-memory-utilization 0.9 \
--trust-remote-code \
--limit-mm-per-prompt '{"image": 5, "video": 2, "audio": 3}' \
--enforce-eager

curl http://localhost:8000/v1/chat/completions \
-X POST \
-H "Content-Type: application/json" \
-d '{
    "model": "/root/.cache/modelscope/hub/models/Qwen/Qwen3-Omni-30B-A3B-Thinking",
    "messages": [
        {
            "role": "user",
            "content": [
                {
                    "type": "image_url",
                    "image_url": {"url": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg"}
                },
                {
                    "type": "text",
                    "text": "Analyze this audio, image, and video together."
                }
            ]
        }
    ]
}'
                {
                    "type": "audio_url",
                    "audio_url": {"url": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav"}
                },
                {
                    "type": "video_url",
                    "video_url": {"url": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/draw.mp4"}
                },

curl http://localhost:8000/v1/chat/completions \
-H "Content-Type: application/json" \
-d '{
  "model": "/root/.cache/modelscope/hub/models/Qwen/Qwen3-Omni-30B-A3B-Thinking",
  "messages": [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": [
        {"type": "image_url", "image_url": {"url": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg"}},
        {"type": "audio_url", "audio_url": {"url": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav"}},
        {"type": "text", "text": "What can you see and hear? Answer in one sentence."}
    ]}
  ]
}'

pip install qwen_omni_utils

TypeError: process_mm_info() missing 1 required positional argument: 'use_audio_in_video'

audioread.ffdec.NotInstalledError
sudo apt update
sudo apt install ffmpeg -y
ffmpeg -version
```
