# 自定义算子 - 多模态

## 参考样例 - Rope 注册流程分析

vllm 侧：

```python
# .../base.py
from vllm.model_executor.custom_op import CustomOp

@CustomOp.register("rotary_embedding")
class RotaryEmbeddingBase(CustomOp):
    def __init__(self, ...):
        super().__init__()
    
    def forward_native(self, ...):
        pass

    def forward_cuda(self, ...):
        pass


# __init__.py
# 实现 selector 进行 dispatch
def get_rope(...):
    if ...:
        rotary_emb = XxxRotaryEmbedding(...)
    elif ...:
        rotary_emb = XxxRotaryEmbedding(...)
    else:
        rotary_emb = XxxRotaryEmbedding(...)


# xxx(model).py
class XxxAttention(nn.Module):
    def __init__(self, ...):
        self.rotary_emb = get_rope(...)
    
    def forward(self, ...):
        ... = self.rotary_emb(...)
```

vllm-ascend 侧：

```python
# .../ops/rope.py
from vllm.model_executor.layers.rotary_embedding import RotaryEmbedding

class AscendRotaryEmbedding(RotaryEmbedding):
    def __init__(self, ...):
        super().__init__(...)

    def forward_oot(self, ...):
        return _rope_forward_oot(self, ...)


def _rope_forward_oot(self, ...):
    pass


# utils.py
global REGISTERED_ASCEND_OPS
    REGISTERED_ASCEND_OPS = {
        "RotaryEmbedding": AscendRotaryEmbedding,
    }
```

---

```python
from vllm.model_executor.layers.multi_modal import get_vision_patch_embed

self.vision_patch_embed = get_vision_patch_embed()

self.vision_patch_embed = get_vision_patch_embed(
    patch_size=patch_size,
    temporal_patch_size=1,
    in_channels=in_channels,
    hidden_size=embed_dim,
    embed_type="linear",
)
```
