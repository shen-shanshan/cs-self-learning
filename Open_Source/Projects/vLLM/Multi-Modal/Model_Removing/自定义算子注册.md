# 自定义算子 - 多模态

## 参考样例 - Rope 注册流程分析

vllm 侧：

```python
# .../base.py
from vllm.model_executor.custom_op import CustomOp

@CustomOp.register("rotary_embedding")
class RotaryEmbeddingBase(CustomOp):
    def __init__(self, ...):
        super().__init__()
    
    def forward_native(self, ...):
        pass

    def forward_cuda(self, ...):
        pass


# __init__.py
# 实现 selector 进行 dispatch
def get_rope(...):
    if ...:
        rotary_emb = XxxRotaryEmbedding(...)
    elif ...:
        rotary_emb = XxxRotaryEmbedding(...)
    else:
        rotary_emb = XxxRotaryEmbedding(...)


# xxx(model).py
class XxxAttention(nn.Module):
    def __init__(self, ...):
        self.rotary_emb = get_rope(...)
    
    def forward(self, ...):
        ... = self.rotary_emb(...)
```

vllm-ascend 侧：

```python
# .../ops/rope.py
from vllm.model_executor.layers.rotary_embedding import RotaryEmbedding

class AscendRotaryEmbedding(RotaryEmbedding):
    def __init__(self, ...):
        super().__init__(...)

    def forward_oot(self, ...):
        return _rope_forward_oot(self, ...)


def _rope_forward_oot(self, ...):
    pass


# utils.py
global REGISTERED_ASCEND_OPS
    REGISTERED_ASCEND_OPS = {
        "RotaryEmbedding": AscendRotaryEmbedding,
    }
```

## Conv 算子注册

[nn.Conv2d](https://docs.pytorch.org/docs/2.8/generated/torch.nn.Conv2d.html) nn.Conv3d -> ConvLayer

```python
"""
torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)
"""
self.proj = nn.Conv2d(
    in_channels=config.num_channels,
    out_channels=config.hidden_size,
    kernel_size=(config.patch_size, config.patch_size),
    stride=(config.patch_size, config.patch_size),
    padding=0 | "valid",
    bias=False,
)

"""
torch.nn.Conv3d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)
"""

# linear
kernel_size = (temporal_patch_size, patch_size, patch_size)
self.proj = ReplicatedLinear(
    input_size=in_channels * math.prod(kernel_size),
    output_size=hidden_size,
    bias=bias,
    return_bias=False,
)


from vllm.model_executor.layers.multi_modal import get_conv_layer

self.proj = get_conv_layer(
            conv_type="conv2d",
            in_channels=config.num_channels,
            out_channels=config.hidden_size,
            kernel_size=(config.patch_size, config.patch_size),
            stride=(config.patch_size, config.patch_size),
        )
self.proj = get_conv_layer(
            conv_type="causal_conv2d",
            in_channels=config.num_channels,
            out_channels=config.hidden_size,
            kernel_size=(config.patch_size, config.patch_size),
            stride=(config.patch_size, config.patch_size),
        )
self.proj = get_conv_layer(
            conv_type="linear",
            input_size=
            output_size=
        )
```

TODO:

1. prefix param
