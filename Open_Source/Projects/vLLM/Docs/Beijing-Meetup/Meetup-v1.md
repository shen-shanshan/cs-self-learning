---
layout: post
title: "vLLM Beijing meetup: Innovation, Ecosystem and Cmmunity"
author: "vLLM / vLLM Ascend / verl / LLaMAFactory Team"
image: /assets/logos/vllm-logo-text-light.png
---

On March 16, 2025, the vLLM Meetup was successfully held in Beijing. The vLLM Meetup is a tradition we have maintained since the inception of the project. Prior to this, we have already hosted nine meetups with companies such as a16z, IBM, Roblox, Cloudflare & BentoML, AWS, NVIDIA, Snowflake, Google Cloud, and Meta. For this meetup, we invited engineers from companies like Huawei and ByteDance, as well as researchers from Tsinghua University and Beihang University, to share their contributions and development experiences within the vLLM community. Additionally, due to the widespread attention vLLM has garnered in both industry and academia, this meetup also attracted users from companies like Ant Group, who discussed their experiences with vLLM and their future needs.

<p align="center">
    <picture>
    <img src="/assets/figures/vllm-2025-beijing-meetup/0.png" width="45%">
    </picture>
</p>

It is worth noting that this meetup marked the first vLLM community exchange event held in China. It not only provided a great platform for communication among major Chinese enterprises and universities but also strengthened the connection between the vLLM community and Chinese developers. In the future, we hope that through the joint efforts of the vLLM community and Chinese users, vLLM can be further refined, made more efficient, and user-friendly.

## Talks

### vLLM Update

<p align="center">
    <picture>
    <img src="/assets/figures/vllm-2025-beijing-meetup/1.png" width="45%">
    </picture>
</p>

Zhang Chen, one of the maintainers of vLLM, shared the recent work within the vLLM community and the release plan for the upcoming v0.8.0 version. She also highlighted the new features and usage methods of the vLLM V1 Engine. Compared to the V0 version, the V1 Engine will be more concise and efficient, and in the future, V1 will become the default option for vLLM.

<p align="center">
    <picture>
    <img src="/assets/figures/vllm-2025-beijing-meetup/2.png" width="45%">
    </picture>
</p>

You Kaichao, another maintainer of vLLM, shared the current ecosystem development status of the vLLM project in the industry and the communication channels of the vLLM community in China, including Zhihu and WeChat official accounts, which facilitate better connections between the vLLM team and Chinese developers.

### vLLM Hardware Plugin Mechanism and Ascend Best Practices

<p align="center">
    <picture>
    <img src="/assets/figures/vllm-2025-beijing-meetup/3.png" width="45%">
    </picture>
</p>

Wang Xiyuan, an engineer at Huawei and a core maintainer of the vllm-ascend project, shared Huawei's work on the vLLM hardware plugin mechanism using the Ascend NPU as an example. He explained the technology behind vLLM's easy implementation of multi-device backend support. Additionally, he introduced Huawei's Ascend AI chips and the principles and features of the CANN computing architecture, as well as Huawei's future plans for the vLLM community.

### VeRL: A Hybrid Controller-based RLHF Framework

<p align="center">
    <picture>
    <img src="/assets/figures/vllm-2025-beijing-meetup/4.png" width="45%">
    </picture>
</p>

Zhang Chi, an engineer at ByteDance and a core developer of the VeRL project, shared ByteDance's research and work in the field of reinforcement learning fine-tuning frameworks. He focused on the pain points currently addressed by VeRL and the core working principles of the Hybrid Controller.

### Best Practices for Efficient Fine-Tuning Framework LLaMA-Factory with vLLM

<p align="center">
    <picture>
    <img src="/assets/figures/vllm-2025-beijing-meetup/5.png" width="45%">
    </picture>
</p>

Zheng Yaowei, a researcher at Beihang University and a core maintainer of the LLaMA-Factory project, shared the current development status in the field of large model fine-tuning and the latest graphical interface launched by LLaMA-Factory. He also introduced how LLaMA-Factory works with frameworks like vLLM to provide developers with ultimate ease of use.
