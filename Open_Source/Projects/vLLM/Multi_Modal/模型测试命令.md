# Multi-Modal Models Test

## Preparation

```bash
git fetch --tags
git tag -l | grep v0.14.1
# vllm
git checkout -b v0.14.0 v0.14.0
git checkout -b v0.14.0rc1-test v0.14.0rc1
git checkout -b v0.14.0-test v0.14.0
# vllm-ascend
git checkout -b v0.11.0 v0.11.0

# vllm
# git reset --hard 17c540a993af88204ad1b78345c8a865cf58ce44
git rebase upstream/releases/v0.11.1
# vllm-ascend
main

tmux attach -t download
python download.py

eval "$(/root/miniconda3/bin/conda shell.bash hook)"
conda activate vllm

export HCCL_BUFFSIZE=1024
export VLLM_VERSION="0.14.1"

def register_model(): pass
```

## Acc Test

```bash
pip install lm-eval

export VLLM_USE_MODELSCOPE=False
export HF_ENDPOINT="https://hf-mirror.com"
lm_eval \
--model vllm-vlm \
--model_args pretrained=/root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-7B-Instruct,max_model_len=4096 \
--tasks mmmu_val \
--batch_size auto
```

æŠ¥é”™ï¼š

```bash
2025-10-20:09:40:15 INFO     [evaluator:574] Running generate_until requests
Adding requests:   0%|                                                                                                               | 0/900 [00:04<?, ?it/s]
Traceback (most recent call last):                                                                                                   | 0/900 [00:00<?, ?it/s]
  File "/root/miniconda3/envs/vllm/bin/lm_eval", line 7, in <module>
    sys.exit(cli_evaluate())
  File "/root/miniconda3/envs/vllm/lib/python3.10/site-packages/lm_eval/__main__.py", line 455, in cli_evaluate
    results = evaluator.simple_evaluate(
  File "/root/miniconda3/envs/vllm/lib/python3.10/site-packages/lm_eval/utils.py", line 456, in _wrapper
    return fn(*args, **kwargs)
  File "/root/miniconda3/envs/vllm/lib/python3.10/site-packages/lm_eval/evaluator.py", line 357, in simple_evaluate
    results = evaluate(
  File "/root/miniconda3/envs/vllm/lib/python3.10/site-packages/lm_eval/utils.py", line 456, in _wrapper
    return fn(*args, **kwargs)
  File "/root/miniconda3/envs/vllm/lib/python3.10/site-packages/lm_eval/evaluator.py", line 585, in evaluate
    resps = getattr(lm, reqtype)(cloned_reqs)
  File "/root/miniconda3/envs/vllm/lib/python3.10/site-packages/lm_eval/models/vllm_vlms.py", line 299, in generate_until
    cont = self._multimodal_model_generate(
  File "/root/miniconda3/envs/vllm/lib/python3.10/site-packages/lm_eval/models/vllm_vlms.py", line 154, in _multimodal_model_generate
    outputs = self.model.generate(
  File "/vllm-workspace/vllm/vllm/entrypoints/llm.py", line 393, in generate
    self._validate_and_add_requests(
  File "/vllm-workspace/vllm/vllm/entrypoints/llm.py", line 1516, in _validate_and_add_requests
    self._add_request(
  File "/vllm-workspace/vllm/vllm/entrypoints/llm.py", line 1569, in _add_request
    self.llm_engine.add_request(
  File "/vllm-workspace/vllm/vllm/v1/engine/llm_engine.py", line 230, in add_request
    prompt_str, request = self.processor.process_inputs(
  File "/vllm-workspace/vllm/vllm/v1/engine/processor.py", line 377, in process_inputs
    processed_inputs: ProcessorInputs = self.input_preprocessor.preprocess(
  File "/vllm-workspace/vllm/vllm/inputs/preprocess.py", line 644, in preprocess
    return self._process_decoder_only_prompt(
  File "/vllm-workspace/vllm/vllm/inputs/preprocess.py", line 614, in _process_decoder_only_prompt
    prompt_comps = self._prompt_to_llm_inputs(
  File "/vllm-workspace/vllm/vllm/inputs/preprocess.py", line 393, in _prompt_to_llm_inputs
    return self._process_text(
  File "/vllm-workspace/vllm/vllm/inputs/preprocess.py", line 343, in _process_text
    inputs = self._process_multimodal(
  File "/vllm-workspace/vllm/vllm/inputs/preprocess.py", line 242, in _process_multimodal
    mm_input = mm_processor.apply(
  File "/vllm-workspace/vllm/vllm/multimodal/processing.py", line 2045, in apply
    prompt_ids, prompt, mm_placeholders = self._maybe_apply_prompt_updates(
  File "/vllm-workspace/vllm/vllm/multimodal/processing.py", line 1997, in _maybe_apply_prompt_updates
    ) = self._apply_prompt_updates(
  File "/vllm-workspace/vllm/vllm/multimodal/processing.py", line 1919, in _apply_prompt_updates
    assert update_idx is not None, (
AssertionError: Failed to apply prompt replacement for mm_items['image'][0]
[ERROR] 2025-10-20-09:40:20 (PID:31615, Device:-1, RankID:-1) ERR99999 UNKNOWN applicaiton exception
```

## Qwen2-Audio-7B-Instruct

```python
from vllm.assets.audio import AudioAsset

audio_url = AudioAsset("winning_call").url
print(audio_url)
# https://vllm-public-assets.s3.us-west-2.amazonaws.com/multimodal_asset/winning_call.ogg
```

```bash
vllm serve /root/.cache/modelscope/models/Qwen/Qwen2-Audio-7B-Instruct \
--max_model_len 4096 \
--trust-remote-code \
--tensor-parallel-size 2 \
--limit-mm-per-prompt '{"audio":2}' \
--chat-template /home/sss/vllm/template_qwen2_audio.jinja \
--enforce-eager

curl -X POST http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "/root/.cache/modelscope/models/Qwen/Qwen2-Audio-7B-Instruct",
        "messages": [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": [
                {"type": "audio_url", "audio_url": {"url": "https://vllm-public-assets.s3.us-west-2.amazonaws.com/multimodal_asset/winning_call.ogg"}},
                {"type": "text", "text": "What is in this audio? How does it sound?"}
            ]}
        ],
        "max_tokens": 100
    }'

# output:
{"id":"chatcmpl-31f5f698f6734a4297f6492a830edb3f","object":"chat.completion","created":1761097383,"model":"/root/.cache/modelscope/models/Qwen/Qwen2-Audio-7B-Instruct","choices":[{"index":0,"message":{"role":"assistant","content":"The audio contains a background of a crowd cheering, a ball bouncing, and an object being hit. A man speaks in English saying 'and the o one pitch on the way to edgar martinez swung on and lined out.' The speech has a happy mood.","refusal":null,"annotations":null,"audio":null,"function_call":null,"tool_calls":[],"reasoning_content":null},"logprobs":null,"finish_reason":"stop","stop_reason":null,"token_ids":null}],"service_tier":null,"system_fingerprint":null,"usage":{"prompt_tokens":689,"total_tokens":743,"completion_tokens":54,"prompt_tokens_details":null},"prompt_logprobs":null,"prompt_token_ids":null,"kv_transfer_params":null}
```

## Qwen2-VL-7B-Instruct

```bash
vllm serve /root/.cache/modelscope/hub/models/Qwen/Qwen2-VL-2B \
--max_model_len 16384 \
--limit-mm-per-prompt '{"image":2}' \
--runner generate \
--enforce-eager

curl -X POST http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "/root/.cache/modelscope/hub/models/Qwen/Qwen2-VL-2B",
        "messages": [
            {"role": "user", "content": [
                {"type": "image_url", "image_url": {"url": "https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png"}},
                {"type": "text", "text": "What is the text in the illustrate? How does it look?"}
            ]}
        ],
        "max_tokens": 100
    }'

Failed to apply prompt replacement for mm_items['image'][0]
```

```python
from transformers import AutoProcessor
from vllm import LLM, SamplingParams
from qwen_vl_utils import process_vision_info

MODEL_PATH = "Qwen/Qwen2-VL-2B"

llm = LLM(
    model=MODEL_PATH,
    max_model_len=16384,
    limit_mm_per_prompt={"image": 10},
)

sampling_params = SamplingParams(
    max_tokens=512
)

image_messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {
        "role": "user",
        "content": [
            {
                "type": "image",
                "image": "https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png",
                "min_pixels": 224 * 224,
                "max_pixels": 1280 * 28 * 28,
            },
            {"type": "text", "text": "Please provide a detailed description of this image"},
        ],
    },
]

messages = image_messages

processor = AutoProcessor.from_pretrained(MODEL_PATH)
prompt = processor.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True,
)

image_inputs, _, _ = process_vision_info(messages, return_video_kwargs=True)

mm_data = {}
if image_inputs is not None:
    mm_data["image"] = image_inputs

llm_inputs = {
    "prompt": prompt,
    "multi_modal_data": mm_data,
}

outputs = llm.generate([llm_inputs], sampling_params=sampling_params)
generated_text = outputs[0].outputs[0].text

print(generated_text)
```

## Qwen2.5-VL-7B-Instruct

```bash
vllm serve /root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-7B-Instruct \
--max_model_len 16384 \
--enforce-eager \

-compilation_config.custom_ops '["none"]'
--max-num-batched-tokens 16384 \
--tensor-parallel-size 2 \

curl -X POST http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "/root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-7B-Instruct",
        "messages": [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": [
                {"type": "image_url", "image_url": {"url": "https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png"}},
                {"type": "text", "text": "What is the text in the illustrate? How does it look?"}
            ]}
        ],
        "max_tokens": 100
    }'

export VLLM_USE_MODELSCOPE=False
export HF_ENDPOINT="https://hf-mirror.com"
lm_eval \
--model vllm-vlm \
--model_args pretrained=/root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-7B-Instruct,max_model_len=4096 \
--tasks mmmu_val \
--batch_size auto
```

## Qwen3-VL-8B-Instruct

```bash
vllm serve /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-8B-Instruct \
--max-model-len 16384 \
--max-num-batched-tokens 16384 \
--max-num-seqs 8 \
--enforce-eager

--max_model_len 16384 \
--enforce-eager \
--tensor-parallel-size 2 \
--allowed-local-media-path /media \

# image
curl -X POST http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "/root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-8B-Instruct",
        "messages": [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": [
                {"type": "image_url", "image_url": {"url": "https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png"}},
                {"type": "text", "text": "What is the text in the illustrate? How does it look?"}
            ]}
        ],
        "max_tokens": 100
    }'

# video
curl -X POST http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "/root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-8B-Instruct",
        "messages": [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": [
                {"type": "video_url", "video_url": {"url": "file:///media/test.mp4"}},
                {"type": "text", "text": "What is in this video?"}
            ]}
        ],
        "max_tokens": 100
    }'
```

Request script:

```python
import asyncio
import base64
import json
import mimetypes
import os
from pathlib import Path
from typing import Any, Dict, List

import aiohttp
from tqdm import tqdm

url = "http://localhost:8000/v1/chat/completions"
model_name = "/root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-8B-Instruct"
prompt = ""

data = None
with open("1122cases.json",'r') as f:
    data = json.load(f)

prompt_dict = {}
for item in data:
    prompt_dict[os.path.basename(Path(item['image_path'][0]).parent)] = [prompt, item['image_path']]

def encode_image_to_data_url(path: str) -> str:
    mime, _ = mimetypes.guess_type(path)
    if mime is None:
        mime = "image/jpeg"
    with open(path, "rb") as img_f:
        b64 = base64.b64encode(img_f.read()).decode("utf-8")
    return f"data:{mime};base64,{b64}"

def build_user_content(prompt_text: str, image_paths: List[str]) -> List[Dict[str, Any]]:
    content: List[Dict[str, Any]] = [{"type": "text", "text": prompt_text}]
    for p in image_paths:
        try:
            data_url = encode_image_to_data_url(p)
            content.append({
                "type": "image_url",
                "image_url": {"url": data_url}
            })
        except Exception as e:
            print(f"[warn] æ— æ³•ç¼–ç å›¾ç‰‡ï¼š{p}ï¼ŒåŸå› ï¼š{e}")
    return content

async def send_request(session, k_q, sem, pbar):
    prompt_text = k_q[1][0]
    image_paths = k_q[1][1]
    user_content = build_user_content(prompt_text, image_paths)
    
    payload = {
        "model": model_name,
        "messages": [
            {"role": "user", "content": user_content}
        ],
        "stream": False,
        "temperature": 1e-06,
        "top_p": 1.0,
        "max_tokens": 256,
    }
    headers = {"Content-Type": "application/json"}

    async with sem:
        try:
            async with session.post(url, json=payload, headers=headers) as response:
                if response.status == 200:
                    result = await response.json()
                    content = result['choices'][0]['message']['content']
                    with open('request_return.jsonl', "a") as f:
                        f.write(json.dumps({'image_path':image_paths, 'predict': content}, ensure_ascii=False) + "\n")
                else:
                    content = f"è¯·æ±‚å¤±è´¥ï¼š{response.status}"
        except Exception as e:
            content = f"å¼‚å¸¸ï¼š{str(e)}"
        finally:
            pbar.update(1)

async def main():
    sem = asyncio.Semaphore(6)
    async with aiohttp.ClientSession() as session:
        with tqdm(total=len(prompt_dict), desc="å¤„ç†è¿›åº¦", unit="ä¸ª", position=0, leave=True) as pbar:
            tasks = [send_request(session, k_q, sem, pbar) for k_q in prompt_dict.items()]
            await asyncio.gather(*tasks)

if __name__ == "__main__":
    asyncio.run(main())
```

## Qwen3-VL-30B-A3B-Instruct

```bash
vllm serve /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-30B-A3B-Instruct \
--max_model_len 16384 \
--max-num-batched-tokens 16384 \
--max-num-seqs 8 \
--tensor-parallel-size 2 \
--enable-expert-parallel \
--enforce-eager

curl -X POST http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "/root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-30B-A3B-Instruct",
        "messages": [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": [
                {"type": "image_url", "image_url": {"url": "https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png"}},
                {"type": "text", "text": "What is the text in the illustrate?"}
            ]}
        ],
        "max_tokens": 100
    }'

--max_model_len  # æ‰‹åŠ¨è®¾ç½®çš„å€¼ä¸åº”è¶…è¿‡æ¨¡å‹æœ¬èº«çš„ä½ç½®ç¼–ç èƒ½åŠ›ï¼ˆmax_position_embeddingsï¼‰
"max_position_embeddings": 262144  # è¿™é€šå¸¸æ˜¯æ¨¡å‹åœ¨è®­ç»ƒæ—¶è§è¿‡çš„æœ€é•¿åºåˆ—é•¿åº¦ï¼Œä¹Ÿæ˜¯è¯¥æ¨¡å‹èƒ½ä¿è¯æ•ˆæœçš„å®‰å…¨é•¿åº¦

vllm serve /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-30B-A3B-Instruct \
--max_model_len 262144 \
--tensor-parallel-size 4 \
--enable-expert-parallel \
--enforce-eager
```

è§†é¢‘æ¨ç†ï¼š

```bash
# By default, the timeout for fetching videos through HTTP URL is 30 seconds.
export VLLM_VIDEO_FETCH_TIMEOUT=100000

vllm serve /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-30B-A3B-Instruct \
--tensor-parallel-size 2 \
--enable-expert-parallel \
--max_model_len 40960 \
--allowed-local-media-path /media \
--enforce-eager

curl -X POST http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "/root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-30B-A3B-Instruct",
        "messages": [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": [
                {"type": "video_url", "video_url": {"url": "file:///media/test.mp4"}},
                {"type": "text", "text": "What is in this video?"}
            ]}
        ],
        "max_tokens": 100
    }'

curl -X POST http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "/root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-30B-A3B-Instruct",
        "messages": [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": [
                {"type": "video_url", "video_url": {"url": "file:///media/test.mp4"}},
                {"type": "text", "text": "What is in this video?"}
            ]}
        ],
        "max_tokens": 100
    }'

"enable_thinking": true,
"thinking_budget": true,
"stream": true,
```

æœ¬åœ°ç”Ÿæˆè§†é¢‘ï¼š

```bash
sudo apt-get install -y ffmpeg

# ç”Ÿæˆ 3 ç§’æµ‹è¯•è§†é¢‘ï¼ˆ224Ã—224ï¼Œ8 FPSï¼‰
ffmpeg -f lavfi -i testsrc=size=224x224:rate=8 -t 3 test.mp4
```

```
{"id":"chatcmpl-b9144bc88275eb2c","object":"chat.completion","created":1766566024,"model":"/root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-30B-A3B-Instruct","choices":[{"index":0,"message":{"role":"assistant","content":"The image contains the text \"8\". It is displayed in a white, bold, sans-serif font, positioned in the center of a black circular background. The text appears to be part of a test pattern, with vertical color bars (red, green, blue, yellow, cyan, magenta, and white) surrounding the circle. The number \"8\" is clearly visible and stands out against the dark background.","refusal":null,"annotations":null,"audio":null,"function_call":null,"tool_calls":[],"reasoning":null,"reasoning_content":null},"logprobs":null,"finish_reason":"stop","stop_reason":null,"token_ids":null}],"service_tier":null,"system_fingerprint":null,"usage":{"prompt_tokens":203,"total_tokens":287,"completion_tokens":84,"prompt_tokens_details":null},"prompt_logprobs":null,"prompt_token_ids":null,"kv_transfer_params":null}
```

## Qwen3-Omni-30B-A3B-Instruct

```bash
vllm serve /root/.cache/modelscope/hub/models/Qwen/Qwen3-Omni-30B-A3B-Instruct \
--max_model_len 16384 \
--tensor-parallel-size 4 \
--enable-expert-parallel \
--enforce-eager

curl -X POST http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "/root/.cache/modelscope/hub/models/Qwen/Qwen3-Omni-30B-A3B-Instruct",
        "messages": [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": [
                {"type": "image_url", "image_url": {"url": "https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png"}},
                {"type": "text", "text": "What is the text in the illustrate? How does it look?"}
            ]}
        ],
        "max_tokens": 100
    }'
```

## Qwen3-Omni-30B-A3B-Thinking

```bash
vllm serve /root/.cache/modelscope/hub/models/Qwen/Qwen3-Omni-30B-A3B-Thinking \
--tensor-parallel-size 2 \
--enable-expert-parallel \
--max_model_len 32768 \
--enforce-eager

--gpu-memory-utilization 0.9 \
--trust-remote-code \
--limit-mm-per-prompt '{"image": 5, "video": 2, "audio": 3}' \

curl http://localhost:8000/v1/chat/completions \
-X POST \
-H "Content-Type: application/json" \
-d '{
    "model": "/root/.cache/modelscope/hub/models/Qwen/Qwen3-Omni-30B-A3B-Thinking",
    "messages": [
        {
            "role": "user",
            "content": [
                {
                    "type": "image_url",
                    "image_url": {"url": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg"}
                },
                {
                    "type": "text",
                    "text": "Analyze this audio, image, and video together."
                }
            ]
        }
    ]
}'

curl http://localhost:8000/v1/chat/completions \
-X POST \
-H "Content-Type: application/json" \
-d '{
    "model": "/root/.cache/modelscope/hub/models/Qwen/Qwen3-Omni-30B-A3B-Thinking",
    "messages": [
        {
            "role": "user",
            "content": [
                {
                    "type": "image_url",
                    "image_url": {
                        "url": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg"
                    }
                },
                {
                    "type": "audio_url",
                    "audio_url": {
                        "url": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav"
                    }
                },
                {
                    "type": "video_url",
                    "video_url": {
                        "url": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/draw.mp4"
                    }

                },
                {
                    "type": "text",
                    "text":  "Analyze this audio, image, and video together."
                }
            ]
        }
    ],
    "max_tokens": 100
}'

curl http://localhost:8000/v1/chat/completions \
-H "Content-Type: application/json" \
-d '{
  "model": "/root/.cache/modelscope/hub/models/Qwen/Qwen3-Omni-30B-A3B-Thinking",
  "messages": [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": [
        {"type": "image_url", "image_url": {"url": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg"}},
        {"type": "audio_url", "audio_url": {"url": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav"}},
        {"type": "text", "text": "What can you see and hear? Answer in one sentence."}
    ]}
  ]
}'

pip install qwen_omni_utils

TypeError: process_mm_info() missing 1 required positional argument: 'use_audio_in_video'

audioread.ffdec.NotInstalledError
sudo apt update
sudo apt install ffmpeg -y
ffmpeg -version
```

## vllm-ascend UT

```bash
pytest -sv tests/e2e/singlecard/test_vlm.py
pytest -sv tests/e2e/singlecard/test_vlm.py::test_multimodal_vl[qwen-vl]
pytest -sv tests/e2e/singlecard/test_vlm.py::test_multimodal_vl[hunyuan-vl]
```

## vllm examples

```bash
source .venv/bin/activate

# def run_qwen2_5_vl:
export VLLM_USE_MODELSCOPE=True
# /home/sss/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-7B-Instruct
python examples/offline_inference/vision_language.py -m qwen2_5_vl

# def run_dots_ocr:
export VLLM_USE_MODELSCOPE=False
# /home/sss/.cache/huggingface/hub/models/dots_ocr
python examples/offline_inference/vision_language.py -m dots_ocr
```

## Benchmark

ğŸŒŸ vllm-ascend VL benchmark:

```bash
export VLLM_VERSION="main"
export VLLM_USE_MODELSCOPE=True
vllm serve /root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-7B-Instruct \
--dtype bfloat16 \
--limit-mm-per-prompt '{"image": 1}' \
--max-model-len 16384 \
--max-num-batched-tokens 16384

export VLLM_USE_MODELSCOPE=False
export HF_ENDPOINT="https://hf-mirror.com"
vllm bench serve \
--model /root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-7B-Instruct \
--backend openai-chat \
--endpoint /v1/chat/completions \
--dataset-name hf \
--hf-split train \
--dataset-path lmarena-ai/vision-arena-bench-v0.1 \
--num-prompts 10 \
--no-stream
# --dataset-path lmarena-ai/VisionArena-Chat \
```

doc: https://docs.vllm.ai/en/stable/benchmarking/cli/#multi-modal-benchmark

ä¸‹è½½æ•°æ®é›†ï¼š

```python
# uv pip install huggingface_hub -i https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple
# export VLLM_USE_MODELSCOPE=False
# export HF_ENDPOINT=https://hf-mirror.com

from huggingface_hub import hf_hub_download

path = hf_hub_download(
    repo_id="ddwang2000/MMSU",
    repo_type="dataset",
    filename="sharegpt4v_instruct_gpt4-vision_cap100k.json",
    local_dir="/home/sss/.cache/huggingface/hub/dataset/sharegpt4v",
    local_dir_use_symlinks=False,
)
```

```bash
wget http://images.cocodataset.org/zips/train2017.zip

cd /home/sss/.cache/huggingface/hub/dataset/sharegpt4v/images
# å¹¶è¡Œä¸‹è½½ï¼Œæ–­ç‚¹ç»­ä¼ 
# -x 16ï¼š16 ä¸ªå¹¶è¡Œè¿æ¥
# -s 16ï¼šåˆ† 16 æ®µä¸‹è½½
# -k 5Mï¼šæ¯æ®µ 5MB
# -cï¼šæ”¯æŒæ–­ç‚¹ç»­ä¼ 
sudo apt install aria2 -y
aria2c \
-x 16 \
-s 16 \
-k 5M \
-c \
http://images.cocodataset.org/zips/train2017.zip

# å†…å­˜ä¸å¤Ÿåˆ†æ
du -ah /home/sss | sort -h | tail -n 50
du -sh ./dots_ocr
find /home/sss -type f -exec du -h {} + | sort -h | tail -n 20
```

å¯åŠ¨æœåŠ¡ï¼š

```bash
export VLLM_USE_MODELSCOPE=False
export HF_ENDPOINT=https://hf-mirror.com

vllm serve /home/sss/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-7B-Instruct \
--dtype bfloat16 \
--limit-mm-per-prompt '{"image": 1}' \
--allowed-local-media-path /home/sss/.cache/huggingface/hub/dataset/sharegpt4v/images
```

å¼€å§‹æµ‹è¯•ï¼š

```bash
export VLLM_USE_MODELSCOPE=False
export HF_ENDPOINT=https://hf-mirror.com

vllm bench serve \
--backend openai-chat \
--model /home/sss/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-7B-Instruct \
--dataset-name sharegpt \
--dataset-path /home/sss/.cache/huggingface/hub/dataset/sharegpt4v/sharegpt4v_instruct_gpt4-vision_cap100k.json \
--num-prompts 100 \
--save-result \
--result-dir ~/vllm_benchmark_results \
--save-detailed \
--endpoint /v1/chat/completions
```
