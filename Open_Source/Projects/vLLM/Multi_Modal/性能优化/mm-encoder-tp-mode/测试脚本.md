```bash
vllm serve /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-30B-A3B-Instruct \
--max_model_len 16384 \
--max-num-batched-tokens 16384 \
--tensor-parallel-size 2 \
--enable-expert-parallel \
--limit-mm-per-prompt '{"image": 1}' \
--no-async-scheduling \
--mm-encoder-tp-mode data


vllm bench serve \
--model /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-30B-A3B-Instruct \
--backend openai-chat \
--endpoint /v1/chat/completions \
--dataset-name hf \
--hf-split train \
--dataset-path lmarena-ai/vision-arena-bench-v0.1 \
--num-prompts 100 \
--request-rate 100 \
--burstiness 0.5
```

```
============ Serving Benchmark Result ============
Successful requests:                     100       
Failed requests:                         0         
Request rate configured (RPS):           100.00    
Benchmark duration (s):                  24.90     
Total input tokens:                      16589     
Total generated tokens:                  11270     
Request throughput (req/s):              4.02      
Output token throughput (tok/s):         452.70    
Peak output token throughput (tok/s):    1380.00   
Peak concurrent requests:                100.00    
Total token throughput (tok/s):          1119.06   
---------------Time to First Token----------------
Mean TTFT (ms):                          10527.79  
Median TTFT (ms):                        10586.56  
P99 TTFT (ms):                           15155.24  
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          137.68    
Median TPOT (ms):                        106.90    
P99 TPOT (ms):                           753.13    
---------------Inter-token Latency----------------
Mean ITL (ms):                           110.71    
Median ITL (ms):                         67.79     
P99 ITL (ms):                            1816.15   
==================================================


--mm-encoder-tp-mode data

```

FAILED tests/e2e/singlecard/test_aclgraph_mem.py::test_aclgraph_mem_use[4-vllm-ascend/DeepSeek-V2-Lite-W8A8]
