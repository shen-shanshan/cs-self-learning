```bash
# Server
vllm serve /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-8B-Instruct \
--dtype bfloat16 \
--limit-mm-per-prompt '{"image": 1}' \
--max-model-len 16384 \
--max-num-batched-tokens 16384 \
--no-async-scheduling
# --max-num-seqs 32 \


# Client
conda activate vllm
export VLLM_USE_MODELSCOPE=False
export HF_ENDPOINT="https://hf-mirror.com"

vllm bench serve \
--model /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-8B-Instruct \
--backend openai-chat \
--endpoint /v1/chat/completions \
--dataset-name hf \
--hf-split train \
--dataset-path lmarena-ai/vision-arena-bench-v0.1 \
--max-concurrency 15 \
--num-prompts 800 \
--no-stream
# --request-rate 10 \
```

[参数设置说明](https://docs.vllm.ai/en/stable/benchmarking/cli/#load-pattern-configuration)

```bash
GPU KV cache size: 248,192 tokens
Maximum concurrency for 16,384 tokens per request: 15.15x
Oversampled requests to reach 800 total samples
```
