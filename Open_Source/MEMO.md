# MEMO

## Linux

```bash
--------------------------------------------------------------------------------
# å®‰è£… sudo
apt-get update
apt-get install sudo

# å®‰è£… curl
sudo apt update
sudo apt upgrade
sudo apt install curl
--------------------------------------------------------------------------------
# å°†è‡ªå·±çš„ SSH å…¬é’¥é…ç½®åˆ°è¿œç¨‹æœåŠ¡å™¨ä¸Š
ssh user@IP -p port
# å°† id_rsa.pub å…¬é’¥æ”¾åœ¨æœåŠ¡å™¨ä¸Šçš„ authorized_keys æ–‡ä»¶ä¸­
vim /root/.ssh/authorized_keys

# å®‰è£… SSH æœåŠ¡
sudo apt update
sudo apt install openssh-client

# å°† SSH å…¬é’¥é…ç½®åˆ° GitHub ä¸Š
ssh-keygen -t ed25519 -C "467638484@qq.com"
eval "$(ssh-agent -s)"
ssh-add ~/.ssh/id_ed25519
cat ~/.ssh/id_ed25519.pub  # Add it to your github
--------------------------------------------------------------------------------
```

## Docker

```bash
--------------------------------------------------------------------------------
# å¯åŠ¨å®¹å™¨ï¼ˆAscend 01 and 02ï¼‰
cd /data/disk3/sss/docker
docker-compose -p sss up -d
--------------------------------------------------------------------------------
# å¸¸ç”¨å‘½ä»¤
docker exec -it <å®¹å™¨åæˆ–ID> /bin/bash
docker stop <å®¹å™¨åæˆ–ID>
docker restart <å®¹å™¨åæˆ–ID>
docker commit <å®¹å™¨åæˆ–ID> <é•œåƒå>
docker rm <å®¹å™¨åæˆ–ID>
--------------------------------------------------------------------------------
```

## Git

```bash
--------------------------------------------------------------------------------
# åŸºæœ¬é…ç½®
git config --global user.email "467638484@qq.com"
git config --global user.name "shen-shanshan"
--------------------------------------------------------------------------------
# å¸¸ç”¨å‘½ä»¤
git chekcout <commit>
git stash
git stash pop
git stash drop [stash_id]
git stash clear
git clone -b åˆ†æ”¯å ä»“åº“åœ°å€
git cherry-pick <commitHash>
--------------------------------------------------------------------------------
# ä¸€é”®ä¸‹è½½ PR
vim ~/.gitconfig
# ä¿®æ”¹ [alias] section
pr = "!f() { git fetch -fu ${2:-$(git remote |grep ^upstream || echo origin)} refs/pull/$1/head:pr/$1 && git checkout pr/$1; }; f"
# å¿«é€Ÿå°† Pull Request ID ä¸º 736 çš„ä»£ç ä¸‹è½½åˆ°æœ¬åœ°
git pr 736
# å‚è€ƒé“¾æ¥ï¼šhttps://github.com/Yikun/yikun.github.com/issues/89
--------------------------------------------------------------------------------
# å®‰è£… git-lfs
curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash
sudo apt-get install git-lfs
git lfs install
# å®˜æ–¹ç½‘ç«™ï¼šhttps://git-lfs.com/
--------------------------------------------------------------------------------
```

## Pip

```bash
--------------------------------------------------------------------------------
pip config set global.index-url https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple
--------------------------------------------------------------------------------
```

## CANN

```bash
--------------------------------------------------------------------------------
# set env
source /home/sss/Ascend/ascend-toolkit/set_env.sh
source /home/sss/Ascend/nnal/atb/set_env.sh
source /usr/local/Ascend/ascend-toolkit/set_env.sh
source /usr/local/Ascend/nnal/atb/set_env.sh

# show env
cat /home/sss/Ascend/ascend-toolkit/latest/aarch64-linux/ascend_toolkit_install.info
--------------------------------------------------------------------------------
```

## Model

```bash
--------------------------------------------------------------------------------
/home/sss/.cache/modelscope/hub/Qwen/Qwen2.5-0.5B-Instruct
/home/sss/cache/modelscope/models/Qwen/Qwen2.5-7B-Instruct
/home/sss/cache/modelscope/models/deepseek-ai/DeepSeek-V2-Lite-Chat
/home/sss/.cache/modelscope/hub/models/LLM-Research/Meta-Llama-3.1-8B-Instruct
/root/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-Instruct
--------------------------------------------------------------------------------
```

## vLLM

```bash
--------------------------------------------------------------------------------
# ç¯å¢ƒå˜é‡
VLLM_USE_V1=xxx
VLLM_USE_MODELSCOPE=xxx
--------------------------------------------------------------------------------
# vllm-ascend format
yapf -i <file>
isort <file>
--------------------------------------------------------------------------------
# Clear process
ps -ef | grep vllm | cut -c 9-16 | xargs kill -9
ps -ef | grep python | cut -c 9-16 | xargs kill -9
--------------------------------------------------------------------------------
# Structured Output
pytest -sv \
tests/v1/entrypoints/llm/test_struct_output_generate.py::test_structured_output

vllm serve /root/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-Instruct \
--max_model_len 26240 \
--pipeline-parallel-size 2

curl http://localhost:8000/v1/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "/root/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-Instruct",
        "prompt": "Hello, my name is",
        "max_tokens": 7,
        "temperature": 0
    }'
--------------------------------------------------------------------------------
# Spec Decode
pytest -sv \
tests/long_term/spec_decode/e2e/test_v1_spec_decode.py::test_ngram_correctness

VLLM_USE_V1=0 pytest -sv \
tests/e2e/long_term/spec_decode/e2e/test_ngram_correctness.py::test_ngram_e2e_greedy_correctness

# Eagel Model
LLM-Research/Meta-Llama-3.1-8B-Instruct
vllm-ascend/EAGLE-LLaMA3.1-Instruct-8B
--------------------------------------------------------------------------------
```

## Open Source

```bash
--------------------------------------------------------------------------------
# å¼€æºç¤¾åŒºå¸¸ç”¨è¯æœ¯
The CI is finally passed and this PR can be merged.
I have rebased on the latest main and nothing changed.

# å¸¸ç”¨ç¬¦å·
ğŸ¯
--------------------------------------------------------------------------------
```

## VSCode

```bash
--------------------------------------------------------------------------------
# å¸¸ç”¨å¿«æ·é”®
æŠ˜å æ‰€æœ‰ï¼šCtrl/Cmd + K + 0
å±•å¼€æ‰€æœ‰ï¼šCtrl/Cmd + K + J
--------------------------------------------------------------------------------
```
