# 香港交流材料

模型上移（上游模型抽象 & 重构）：

- [RFC]: Reorganizing ViT Abstraction and Attention Selection Logic [#27821](https://github.com/vllm-project/vllm/issues/27821)

相关 PR：

- Refactor ROPE into module [#22192](https://github.com/vllm-project/vllm/pull/22192)
- Enable supports_torch_compile on generic nn.Module and demonstrate speedup on Qwen Vision model [#23207](https://github.com/vllm-project/vllm/pull/23207)
- Add FA3 in VIT [#24347](https://github.com/vllm-project/vllm/pull/24347)
- Decouple ViT backend from LM backend [#27061](https://github.com/vllm-project/vllm/pull/27061)
- Refactor mm encoder attention interface and support attention mask [#27147](https://github.com/vllm-project/vllm/pull/27147)
- Reorganizing ViT Attention Backend and decouple from Text Attention Backend [#27919](https://github.com/vllm-project/vllm/pull/27919)

ViT 支持 SP 并行

EPD：

- [Core] Encoder separation for Encode-Prefill-Decode Disaggregation [#21740](https://github.com/vllm-project/vllm/pull/21740)
- [Core] Encoder separation for Encode-Prefill-Decode Disaggregation [#25233](https://github.com/vllm-project/vllm/pull/25233#issuecomment-3404340863)
