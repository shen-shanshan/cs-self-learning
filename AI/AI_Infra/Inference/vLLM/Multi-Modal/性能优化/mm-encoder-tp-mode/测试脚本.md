```bash
export VLLM_VERSION="0.15.0"

vllm serve /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-8B-Instruct \
--max_model_len 16384 \
--max-num-batched-tokens 16384 \
--tensor-parallel-size 2 \
--limit-mm-per-prompt '{"image": 1}' \
--no-async-scheduling \
--mm-encoder-tp-mode data


conda activate vllm
export VLLM_USE_MODELSCOPE=False
export HF_ENDPOINT="https://hf-mirror.com"

vllm bench serve \
--model /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-8B-Instruct \
--backend openai-chat \
--endpoint /v1/chat/completions \
--dataset-name hf \
--hf-split train \
--dataset-path lmarena-ai/vision-arena-bench-v0.1 \
--num-prompts 100 \
--request-rate 100 \
--burstiness 0.5 \
--mm-encoder-tp-mode data

vllm bench latency \
--model /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-8B-Instruct \
--max_model_len 16384 \
--max-num-batched-tokens 16384 \
--tensor-parallel-size 2 \
--limit-mm-per-prompt '{"image": 1}' \
--no-async-scheduling \
--input-len 32 \
--output-len 1 \
--enforce-eager \
--load-format dummy
```

```
============ Serving Benchmark Result ============
Successful requests:                     100       
Failed requests:                         0         
Request rate configured (RPS):           100.00    
Benchmark duration (s):                  23.12     
Total input tokens:                      16589     
Total generated tokens:                  11988     
Request throughput (req/s):              4.33      
Output token throughput (tok/s):         518.56    
Peak output token throughput (tok/s):    2068.00   
Peak concurrent requests:                100.00    
Total token throughput (tok/s):          1236.14   
---------------Time to First Token----------------
Mean TTFT (ms):                          10937.75  
Median TTFT (ms):                        10313.85  
P99 TTFT (ms):                           16132.99  
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          116.14    
Median TPOT (ms):                        96.12     
P99 TPOT (ms):                           699.39    
---------------Inter-token Latency----------------
Mean ITL (ms):                           98.71     
Median ITL (ms):                         46.65     
P99 ITL (ms):                            1778.46   
==================================================
```

--mm-encoder-tp-mode data

```
```
