Run:

```bash
vllm serve /root/.cache/modelscope/hub/models/Qwen/Qwen3-Omni-30B-A3B-Thinking \
--tensor-parallel-size 2 \
--enable-expert-parallel \
--dtype bfloat16 \
--limit-mm-per-prompt '{"image": 1}' \
--max-model-len 16384 \
--max-num-batched-tokens 16384 \
--no-async-scheduling
```

**max-concurrency = 1**

```bash
vllm bench serve \
--model /root/.cache/modelscope/hub/models/Qwen/Qwen3-Omni-30B-A3B-Thinking \
--backend openai-chat \
--endpoint /v1/chat/completions \
--dataset-name hf \
--hf-split train \
--dataset-path lmarena-ai/vision-arena-bench-v0.1 \
--num-prompts 100 \
--max-concurrency 1 \
--no-stream
```

```
============ Serving Benchmark Result ============
Successful requests:                     100       
Failed requests:                         0         
Maximum request concurrency:             1         
Benchmark duration (s):                  486.29    
Total input tokens:                      16589     
Total generated tokens:                  12705     
Request throughput (req/s):              0.21      
Output token throughput (tok/s):         26.13     
Peak output token throughput (tok/s):    33.00     
Peak concurrent requests:                2.00      
Total token throughput (tok/s):          60.24     
---------------Time to First Token----------------
Mean TTFT (ms):                          502.85    
Median TTFT (ms):                        429.38    
P99 TTFT (ms):                           1592.76   
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          34.58     
Median TPOT (ms):                        33.92     
P99 TPOT (ms):                           37.40     
---------------Inter-token Latency----------------
Mean ITL (ms):                           34.35     
Median ITL (ms):                         34.29     
P99 ITL (ms):                            72.82     
==================================================
```

**max-concurrency = 2**

```bash
vllm bench serve \
--model /root/.cache/modelscope/hub/models/Qwen/Qwen3-Omni-30B-A3B-Thinking \
--backend openai-chat \
--endpoint /v1/chat/completions \
--dataset-name hf \
--hf-split train \
--dataset-path lmarena-ai/vision-arena-bench-v0.1 \
--num-prompts 100 \
--max-concurrency 2 \
--no-stream
```

```
============ Serving Benchmark Result ============
Successful requests:                     100       
Failed requests:                         0         
Maximum request concurrency:             2         
Benchmark duration (s):                  276.55    
Total input tokens:                      16589     
Total generated tokens:                  12738     
Request throughput (req/s):              0.36      
Output token throughput (tok/s):         46.06     
Peak output token throughput (tok/s):    70.00     
Peak concurrent requests:                4.00      
Total token throughput (tok/s):          106.05    
---------------Time to First Token----------------
Mean TTFT (ms):                          726.61    
Median TTFT (ms):                        527.96    
P99 TTFT (ms):                           10686.01  
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          37.90     
Median TPOT (ms):                        37.38     
P99 TPOT (ms):                           41.80     
---------------Inter-token Latency----------------
Mean ITL (ms):                           38.18     
Median ITL (ms):                         35.80     
P99 ITL (ms):                            129.65    
==================================================
```
