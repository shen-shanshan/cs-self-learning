# TODO

---

## LLM

- [ ] [MLA 结构代码实现及优化](https://www.armcvai.cn/2025-02-10/mla-code.html)
- [ ] [zhang's Blog - Transformer](https://www.armcvai.cn/categories.html)
- [ ] [多模态技术梳理：ViT 系列](https://zhuanlan.zhihu.com/p/26719287825)
- [ ] [阿里通义千问 Qwen3 系列模型正式发布，该模型有哪些技术亮点？ - 刘聪 NLP 的回答 - 知乎](https://www.zhihu.com/question/1900300358229652607/answer/1900452232018767979)

---

## 推理引擎

- [ ] [面向 ML 玩家的 Docker 零帧起手](https://zhuanlan.zhihu.com/p/1916764175230801287?share_code=FFpFk5rroxTE&utm_psn=1918221276146800528)
- [ ] [Speculative Decoding: 总结、分析、展望](https://zhuanlan.zhihu.com/p/1904881828906668879?share_code=hDIX8nBBfJOQ&utm_psn=1918275277408142518)
- [ ] [vLLM 推理引擎深入浅出 - 知乎专栏](https://www.zhihu.com/column/c_1397348083538153472)
- [ ] [图解 vLLM V1 系列 7：使用 AsyncLLM 做异步推理](https://zhuanlan.zhihu.com/p/1916187125931554299)
- [ ] [图解 vLLM V1 系列 6：KVCacheManager 与 PrefixCaching](https://zhuanlan.zhihu.com/p/1916181593229334390)
- [ ] [大语言模型所有算子逻辑](https://zhuanlan.zhihu.com/p/1909996866432668841?share_code=u4D2wlKwNjAp&utm_psn=1911112698021807580)
- [ ] [被问懵了！腾讯面试官让我手写 PagedAttention](https://zhuanlan.zhihu.com/p/1911455737118457997?share_code=9fRcELOowc4U&utm_psn=1912436101039226918)
- [ ] [vLLM调度器解密（上）：Continuous Batch 是如何工作的？](https://zhuanlan.zhihu.com/p/1117099341?share_code=3OZ9bBQsRAHV&utm_psn=1909578321869637005)
- [ ] [vLLM调度器解密（下）：chunked prefill是如何进一步优化的？](https://zhuanlan.zhihu.com/p/6144374775?share_code=w9CKto9eLSq2&utm_psn=1909578492246466702)
- [ ] [3 万字详细解析清华大学最新综述工作：大模型高效推理综述](https://mp.weixin.qq.com/s/U9ESiWehnoKc9SnDz7DVKg)
- [ ] [zhang's Blog - LLM Infer](https://www.armcvai.cn/categories.html)
- [ ] [vLLM 框架 V1 演进分析](https://zhuanlan.zhihu.com/p/1894423873145004335)
- [ ] [LLM (18)：LLM 的推理优化技术纵览](https://zhuanlan.zhihu.com/p/642412124?utm_psn=1897433318875693188)
- [ ] [图文详解 LLM inference：KV Cache](https://zhuanlan.zhihu.com/p/1893220743053030641?utm_psn=1897576305303721590)
- [ ] [大模型推理 - 为什么要 PD 分离？](https://zhuanlan.zhihu.com/p/1897270081664300462?utm_psn=1897629970966217092)
- [ ] [图解 vLLM Prefix Prefill Triton Kernel](https://zhuanlan.zhihu.com/p/695799736?share_code=Hz1PZDdfXLy7&utm_psn=1900943218725598209)
- [ ] [vLLM 的 prefix cache 为何零开销](https://zhuanlan.zhihu.com/p/1896927732027335111)
- [ ] [prefill 和 decode 该分离到不同的卡上么？](https://zhuanlan.zhihu.com/p/1280567902?share_code=z1ij3mzQpXAE&utm_psn=1902828634068226129)
- [ ] [vLLM PD 分离 KV cache 传递机制详解与演进分析](https://zhuanlan.zhihu.com/p/1906741007606878764?share_code=1m2xkCswqTA9N&utm_psn=1907185030842782292)
- [ ] [vLLM PD 分离方案浅析](https://zhuanlan.zhihu.com/p/1889243870430201414?utm_psn=1889596220076426760)

**已阅读：**

- [x] [vLLM V1 源码阅读](https://zhuanlan.zhihu.com/p/32045324831)（V1 全流程讲解，很详细）
- [x] [图解 vLLM V1 系列 1：整体流程 - 猛猿](https://zhuanlan.zhihu.com/p/1900126076279160869?share_code=18FtZ4wqQM3hR&utm_psn=1900940137866716878)
- [x] [图解 vLLM V1 系列 2：Executor-Workers 架构 - 猛猿](https://zhuanlan.zhihu.com/p/1900613601577899465)
- [x] [图解 vLLM V1 系列 3：KV Cache 初始化 - 猛猿](https://zhuanlan.zhihu.com/p/1900932850829730567)
- [x] [图解 vLLM V1 系列 4：加载模型权重（load_model）](https://zhuanlan.zhihu.com/p/1908151478557839879?share_code=RlCt8lDNStds&utm_psn=1912310198112059517)
- [x] [图解 vLLM V1 系列 5：调度器策略（Scheduler）](https://zhuanlan.zhihu.com/p/1908153627639551302?share_code=02jBOS1PfJxF&utm_psn=1912310252315079385)
- [x] [vLLM V1 Scheduler 的调度逻辑 & 优先级分析](https://zhuanlan.zhihu.com/p/1900957007575511876?share_code=o9ZDfDnEpemP&utm_psn=1901069245619635086)
- [x] [vLLM 模型权重加载：使用 setattr](https://zhuanlan.zhihu.com/p/714531623?utm_psn=1916989579635975888)
- [x] [vLLM 算子开发流程: "保姆级"详细记录](https://zhuanlan.zhihu.com/p/1892966682634473987?share_code=1lbfAKTh5A2Vr&utm_psn=1913354916832997933)（如何向 vLLM 提交一个优化 kernel 的 PR）
- [x] [vLLM 显存管理详解](https://zhuanlan.zhihu.com/p/1916529253169734444?share_code=aePDPg2VonBo&utm_psn=1917144770171606655)
- [x] [AI Infra 之模型显存管理分析](https://mp.weixin.qq.com/s/lNcszOFnGVktBRAAsHDVIA)（计算显存、推理时延评估方法）

---

## PyTorch

- [ ] [PyTorch internals](http://blog.ezyang.com/2019/05/pytorch-internals/)
- [ ] [PyTorch dispatcher](http://blog.ezyang.com/2020/09/lets-talk-about-the-pytorch-dispatcher/)
- [ ] [PyTorch 显存管理介绍与源码解析（一）](https://zhuanlan.zhihu.com/p/680769942)
- [ ] [PyTorch 显存管理介绍与源码解析（二）](https://zhuanlan.zhihu.com/p/681651660)
- [ ] [PyTorch 显存管理介绍与源码解析（三）](https://zhuanlan.zhihu.com/p/692614846)
- [ ] [PyTorch 显存可视化与 Snapshot 数据分析](https://zhuanlan.zhihu.com/p/677203832)

---

## CUDA

- [ ] [CUDA 内核优化策略](https://www.armcvai.cn/2024-08-25/cuda-kernel.html)
- [ ] [zhang's Blog - CUDA](https://www.armcvai.cn/categories.html)
- [ ] [CUDA 练手小项目 — Parallel Prefix Sum (Scan)](https://zhuanlan.zhihu.com/p/661460705?share_code=pseQOXxySVcl&utm_psn=1902627229709624968)
- [ ] [手撕 CUDA 算子：高频面试题汇总~](https://mp.weixin.qq.com/s/kSiQZGTumV1QkUhjQKB6Qg)
- [ ] [CUDA 性能分析工具](https://zhuanlan.zhihu.com/p/1911179137357419017)
- [ ] [CUDA 新手性能分析参考](https://zhuanlan.zhihu.com/p/1911511031525646518)
- [ ] [Cmake for CUDA](https://cliutils.gitlab.io/modern-cmake/chapters/packages/CUDA.html)

---

## 科研

- [ ] [写在 Ph.D 第 0 年：AI/CV 科研菜鸟的持续进阶之路](https://zhuanlan.zhihu.com/p/960781637?share_code=13GKbPaHvl60E&utm_psn=1904443459802206715)
