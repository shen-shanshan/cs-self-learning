# TODO

## 技术博客

- [Installing and Developing vLLM with Ease](https://blog.vllm.ai/2025/01/10/dev-experience.html)
- [如何从浅入深理解 Transformer？ - 胡津铭的回答 - 知乎](https://www.zhihu.com/question/471328838/answer/3554522339)
- [像教女朋友一样教你用 Cuda 实现 PyTorch 算子](https://mp.weixin.qq.com/s/UUYw_25TYC9HES4sNmVfKg)
- [为什么我还是无法理解 transformer？ - 等壹的回答 - 知乎](https://www.zhihu.com/question/596771388/answer/3263001703)
- [transformer 为什么使用 layer normalization，而不是其他的归一化方法？ - 苏剑林的回答 - 知乎](https://www.zhihu.com/question/395811291/answer/2431178353)
- [大模型推理加速技术的学习路线是什么? - 晨久已的回答 - 知乎](https://www.zhihu.com/question/591646269/answer/59681951197)
- [从零开始设计 SGLang 的 KV Cache](https://zhuanlan.zhihu.com/p/31160183506)
- [算子优化领域现状分析及未来展望](https://zhuanlan.zhihu.com/p/32760092441)
- [[LLM推理优化]🔥30+篇: LLM推理论文集-500页PDF💡](https://zhuanlan.zhihu.com/p/669777159)
- [3万字详细解析清华大学最新综述工作：大模型高效推理综述](https://mp.weixin.qq.com/s/U9ESiWehnoKc9SnDz7DVKg)

已看：

- [vllm近期更新的一些trick总结](https://mp.weixin.qq.com/s/R_X0qxSiA3X4FqhWzyQM1g)
- [AI Infra之模型显存管理分析](https://mp.weixin.qq.com/s/lNcszOFnGVktBRAAsHDVIA)
- - [0 Token 间间隔 100% GPU 利用率，百度百舸 AIAK 大模型推理引擎极限优化 TPS](https://mp.weixin.qq.com/s/ohpOginJF_H7RRDfAdRWEw)
