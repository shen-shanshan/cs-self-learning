# TODO

## 待看帖子

- [Installing and Developing vLLM with Ease](https://blog.vllm.ai/2025/01/10/dev-experience.html)
- [如何从浅入深理解 Transformer？ - 胡津铭的回答 - 知乎](https://www.zhihu.com/question/471328838/answer/3554522339)
- [像教女朋友一样教你用 Cuda 实现 PyTorch 算子](https://mp.weixin.qq.com/s/UUYw_25TYC9HES4sNmVfKg)
- [为什么我还是无法理解 transformer？ - 等壹的回答 - 知乎](https://www.zhihu.com/question/596771388/answer/3263001703)
- [transformer 为什么使用 layer normalization，而不是其他的归一化方法？ - 苏剑林的回答 - 知乎](https://www.zhihu.com/question/395811291/answer/2431178353)
- [大模型推理加速技术的学习路线是什么? - 晨久已的回答 - 知乎](https://www.zhihu.com/question/591646269/answer/59681951197)
- [从零开始设计 SGLang 的 KV Cache](https://zhuanlan.zhihu.com/p/31160183506)
- [算子优化领域现状分析及未来展望](https://zhuanlan.zhihu.com/p/32760092441)
- [[LLM推理优化]🔥30+篇: LLM推理论文集-500页PDF💡](https://zhuanlan.zhihu.com/p/669777159)
