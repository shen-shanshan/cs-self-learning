# 引言

## 1  机器学习的流程

1. 数据收集：
   - 训练数据：图片、声音、文本、数据；
   - 标签数据。
2. 数据清洗（格式化）；
3. 特征工程（向量化 -> 特征向量）；
4. 数据建模（算法 -> 模型）。

## 2  机器学习的分类

- 有监督学习：
  - 回归；
  - 分类。
- 无监督学习（训练数据没有标签）：
  - 聚类；
  - 降维。
- 强化学习。

## 3  预备知识

- 高等数学；
- 线性代数；
- 概率论；
- Python。

# 线性回归

## 1  基本概念

### 1.1  假设函数

假设函数（hypotheses function）：预测函数。
$$
h_{θ} = \sum_{i=0}^{n}θ_{i}x_{i} = θ^Tx
$$

> y = ax + b  ->  h(x):   b -> θ<sub>0</sub>x<sub>0</sub> (x<sub>0</sub> = 1);   a -> θ<sub>1</sub>;   x -> x<sub>1</sub>。

- 当 *X* 为二维时，假设函数为一条直线；
- 当 *X* 为三维时，假设函数为一个平面；
- 当 *X* 大于三维时，假设函数为一个超平面。

### 1.2  损失函数

损失函数（loss function）：预测值（*h*）与实际值（*y*）之间的差距。
$$
L(θ) = (h_{θ}(x)-y)^2
$$

### 1.3  代价函数

代价函数（cost function）：损失函数的均值。
$$
J(θ) = \frac{1}{2m}\sum_{i=1}^{m}(h_{θ}(x^{(i)})-y^{(i)})^2
$$

## 2  梯度下降

### 2.1  算法原理

*J*(*θ*) 的梯度（导数）为 ∇*J*(*θ*)：

- 随机初始化 *θ*；
- 设置步长 *α*；
- 设置迭代次数 *m*。

```
for i = 0 to m:
	θ = θ - α∇J(θ)
```

> 缺点：可能会出现只下降到局部最低，而达不到全局最低的情况。

### 2.2  代码实现

```python
import numpy as np
import matplotlib.pyplot as plt

x = np.linspace(-6, 4, 100)
y = x**2 + 2*x + 5

plt.plot(x, y)

# 初始化起点、步长、迭代次数
x = 3
alpha = 0.8
iteratorNum = 10

# 迭代
for i in range(iteratorNum):
    # y的导数为2x+2
    x = x - alpha*(2*x + 2)

print(x)
```

- 若 α 太小，则会导致迭代次数过多，下降速度慢；
- 若 α 太大，则会导致在最低点附近来回震荡，收敛速度慢。

## 3  线性回归

### 3.1  算法原理

使用梯度下降法求解线性回归问题：使代价函数的损失值最小。

迭代公式：

![image-20230910144123253](机器学习.assets/image-20230910144123253.png)

> - m 为样本数量；
> - j 为 n 维特征的其中一个。

### 3.2  代码实现

```python
# 梯度下降
def gradientDescent(X, y, theta, iterations, alpha):
    # 向原始数据手动添加一个全为1的列
    c = np.ones(X.shape[0]).transpose()
    X = np.insert(X, 0, values=c, axis=1)
    m = X.shape[0]  # 行数
    n = X.shape[1]  # 列数
    
    for num in range(iterations):
        for j in range(n):
            theta[j] = theta[j] + (alpha/m)*(np.sum((y-np.dot(X, theta))*X[:, j].reshape(-1, 1)))


# 特征归一化（减去平均值，除以方差）
def featureNormalize(X):
    mu = np.average(X, axis=0)
    sigma = np.std(X, axis=0, ddof=1)  # ddof=1，代表除的是n-1，否则除的是n
    X = (X-mu)/sigma
    return X, mu, sigma


X, y = loaddata()
X, mu, sigma = featureNormalize(X)
theta = np.zeros(X.shape[1]+1).reshape(-1, 1)
iterations = 400
alpha = 0.01
theta = gradientDescent(X, y, theta, iterations, alpha)
```

## 4  算法分类

### 4.1  批量梯度下降

每一次 *θ* 的更新，都会用到所有的数据进行计算。

### 4.2  随机梯度下降

每一次 *θ* 的更新，只选择其中一个样本进行计算。

> 缺点：效果不稳定，可能会产生震荡。

### 4.3  小批量梯度下降

每一次 *θ* 的更新，只选择样本中的一批（batch_size）数据进行计算。

## 5  评价指标

均方误差（MSE）：用每一个样本的实际值减去预测值，再对所有的样本取平均。

![image-20230910162351711](机器学习.assets/image-20230910162351711.png)

均方根误差（RMSE）：

![image-20230910162440635](机器学习.assets/image-20230910162440635.png)

平均绝对误差（MAE）：

![image-20230910162449513](机器学习.assets/image-20230910162449513.png)

## 6  正则化

### 6.1  岭回归

L2 范数正则化解决过拟合（Ridge Regression，岭回归）：

![image-20230910165601545](机器学习.assets/image-20230910165601545.png)

迭代公式：

![image-20230910165955770](机器学习.assets/image-20230910165955770.png)

λ 越大，对模型的惩罚就越大，模型就越简单（起主要作用的特征数量就越少），就越可以减少过拟合的风险。

### 6.2  LASSO 回归

L1 范数正则化解决过拟合（LASSO 回归）：

![image-20230910165613448](机器学习.assets/image-20230910165613448.png)

### 6.3  弹性网

L1 与 L2 结合解决过拟合（Elastic Net，弹性网）：

![image-20230910165627959](机器学习.assets/image-20230910165627959.png)

## 7  最小二乘法

$$
θ = (X^TX)^{-1}X^Ty
$$

> 常用公式：
>
> ![image-20230924110838971](机器学习.assets/image-20230924110838971.png)

# 逻辑回归

> 线性回归解决的是预测问题，逻辑回归解决的是二分类问题（通过扩展也可以解决多分类问题）。

## 1  基本概念

![image-20230924113433377](机器学习.assets/image-20230924113433377.png)

假设函数：预测值。

损失函数：

- 当 y（实际值）为 1 时，预测值越接近 1，损失函数越小；
- 当 y（实际值）为 0 时，预测值越接近 0，损失函数越小。

代价函数：对所有样本的损失函数取平均。

> 代价函数的值越小，分类的效果就越好。

## 2  迭代公式

![image-20230924115159799](机器学习.assets/image-20230924115159799.png)

## 3  代码实现

## 4  正则化

![image-20230924171227392](机器学习.assets/image-20230924171227392.png)

## 5  多分类

### 5.1  OvR（one over rest，一对多）

针对 k 个类别，训练 k 个分类器（预测函数），每次只区分是当前类别还是其它类别，最后取其中预测值最高的类别。

> 缺点：训练集有偏（不同类别的数据数量差别很大），对结果会造成影响。

### 5.2  OvO（one over one，一对一）

针对 k 个类别，训练 k(k-1)/2 个分类器（预测函数），每次只任意取其中两个类别来训练分类器。预测新数据时，将新数据带入每一个分类器，统计每个分类器的结果，出现次数最多的。

> - 缺点：训练开销大；
> - 优点：相对 OvR 准确率高。

### 5.3  MvM（multi over multi，多对多）

![image-20230924173433609](机器学习.assets/image-20230924173433609.png)

> 这里的距离为欧氏距离，即将每一维的坐标差分别平方，再求和并开根号。

# 决策树

## 1  基本概念

### 1.1  熵

![image-20230924174805141](机器学习.assets/image-20230924174805141.png)

### 1.2  条件熵

![image-20230924180423895](机器学习.assets/image-20230924180423895.png)

> 这里的 n 代表当前特征一共有 n 种取值，我们需要分别计算每种取值的熵，再加权求和，即为当前特征的条件熵。
>
> 每个特征下每种取值的熵，需要根据最后的类别去进行计算。

### 1.3  信息增益

![image-20230924181602323](机器学习.assets/image-20230924181602323.png)

> 若能根据某一个特征取值的不同，就能较为准确地区分出最后的类别，则说明该特征对最后的分类结果影响很大（与分类结果强相关），并且该特征下不同取值的熵会较小（对应最后的结果越统一），该特征对应的信息增益也越大，因此我们应选择信息增益较大的特征（分类效果最好）。

信息增益的计算方法：

![image-20230924181730633](机器学习.assets/image-20230924181730633.png)

## 2  构建算法

### 2.1  ID3 算法

![image-20230924182004385](机器学习.assets/image-20230924182004385.png)

### 2.2  C4.5 算法

![image-20230924225524160](机器学习.assets/image-20230924225524160.png)

### 2.3  基尼指数

![image-20230924225903034](机器学习.assets/image-20230924225903034.png)

> 每一个节点只划分出两个类别，选择一类，其余的类别都归为另一类。
>
> - 当该节点处只有两个类别时，只有一种划分方式；
> - 当该节点处有三个类别时，有三种划分方式。

示例：

![image-20230924230711331](机器学习.assets/image-20230924230711331.png)

## 3  代码实现

递归结束条件（满足其一即可）：

- 当前节点下的所有数据中，只剩一个类别，则该节点为该类别；
- 当前节点下的所有数据中，只剩一个特征，选择数量最多的类别作为当前节点的类别。

## 4  剪枝

### 4.1  预剪枝

![image-20230924233809848](机器学习.assets/image-20230924233809848.png)

### 4.2  后剪枝

![image-20230924233824977](机器学习.assets/image-20230924233824977.png)

## 5  处理连续值与缺失值

### 5.1  处理连续值

处理方法：将连续值离散化，通过计算信息增益来判断划分是否得当（取信息增益最大时的划分方法）。

![image-20230924233031814](机器学习.assets/image-20230924233031814.png)

### 5.2  处理缺失值

![image-20230924233725114](机器学习.assets/image-20230924233725114.png)

## 6  多变量决策树

![image-20230924234430469](机器学习.assets/image-20230924234430469.png)

![image-20230924234448821](机器学习.assets/image-20230924234448821.png)

# 朴素贝叶斯

## 1  基本概念

### 1.1  贝叶斯决策论

针对每个样本 X，选择能使后验概率 `P(某个类别|X)` 最大的类别作为 X 的类别。

### 1.2  条件概率

![image-20231003113100011](机器学习.assets/image-20231003113100011.png)

> 注意：分母的 p(x) 可以不用除，因为最后比较的是每种类别之间概率的相对大小，而分母的值对于某个样本来说是固定的。

### 1.3  极大似然估计

![image-20231003100913807](机器学习.assets/image-20231003100913807.png)

## 2  朴素贝叶斯模型

![image-20231003112618857](机器学习.assets/image-20231003112618857.png)

示例：

![image-20231003112833078](机器学习.assets/image-20231003112833078.png)

## 3  代码实现

记录条件概率的字典的 key 由三个值组装而成：y<sub>i</sub> 的取值（类别）+ x<sub>i</sub>（特征）+ x<sub>i</sub> 的取值（特征的类别）。

## 4  拉普拉斯修正

![image-20231003112453161](机器学习.assets/image-20231003112453161.png)

## 5  处理连续值

![image-20231003112113119](机器学习.assets/image-20231003112113119.png)

> 注意：这里是假设取值为连续值的特征的数据符合高斯分布，然后就可以通过计算概率密度来求得相应的条件概率。

# 支持向量机

## 1  基本概念

![image-20231003114126020](机器学习.assets/image-20231003114126020.png)

## 2  线性可分支持向量机

### 2.1  基本概念

![image-20231003114911022](机器学习.assets/image-20231003114911022.png)

![image-20231003123547699](机器学习.assets/image-20231003123547699.png)

> 注意：
>
> - 在求几何间隔时，由于 `w.x^(i)+b` 与 `y^(i)` 同号，因此可以去掉分子的求绝对值符号；
> - 通过缩放直线的系数，必然可以使相应的函数间隔为 1。

![image-20231003124012791](机器学习.assets/image-20231003124012791.png)

> 注意：`s.t.` 为约束条件。

### 2.2  拉格朗日乘子法

#### 2.2.1  等式约束

![image-20231003155416483](机器学习.assets/image-20231003155416483.png)

原理说明：

![image-20231003160236660](机器学习.assets/image-20231003160236660.png)

![image-20231003160047545](机器学习.assets/image-20231003160047545.png)

#### 2.2.2  不等式约束

##### 2.2.2.1  极值点在可行区域内

- 约束条件必须为 `g(x) < 0` 的形式；
- 此时约束条件不起作用，即相当于使 `α = 0`，只用对目标函数求梯度。

![image-20231003160610581](机器学习.assets/image-20231003160610581.png)

##### 2.2.2.2  极值点不在可行区域内

- 此时能取到的极值点必在可行区域的边界上，即相当于将不等式约束变为了等式约束 `g(x) = 0`；
- 此时 `α > 0`，即需要对 f(x) 和 g(x) 同时求梯度。

![image-20231003160908928](机器学习.assets/image-20231003160908928.png)

##### 2.2.2.3  综合方法

![image-20231003161150257](机器学习.assets/image-20231003161150257.png)

> 注意：`α` 作为拉格朗日乘子，必须满足 `α >= 0`。

### 2.3  目标函数求解

#### 2.3.1  目标函数

![image-20231003162942253](机器学习.assets/image-20231003162942253.png)

#### 2.3.2  公式推导

![image-20231003164628541](机器学习.assets/image-20231003164628541.png)

![image-20231003164235077](机器学习.assets/image-20231003164235077.png)

![image-20231003164416359](机器学习.assets/image-20231003164416359.png)

![image-20231003165103543](机器学习.assets/image-20231003165103543.png)

> 注意：
>
> - 对于每一条数据 `x^i`，都会有一个对应的 `α^i`；
> - `x^i` 为一条数据，`y^i` 为该数据对应的标签（1 或 -1）；
> - 这里的 `*` 表示是最优解。

#### 2.3.3  方法总结

![image-20231003170432388](机器学习.assets/image-20231003170432388.png)

#### 2.3.4  求解示例

![image-20231003195851020](机器学习.assets/image-20231003195851020.png)

![image-20231003195950671](机器学习.assets/image-20231003195950671.png)

![image-20231003200010492](机器学习.assets/image-20231003200010492.png)

![image-20231003194123127](机器学习.assets/image-20231003194123127.png)

![image-20231003194519186](机器学习.assets/image-20231003194519186.png)

![image-20231003195709970](机器学习.assets/image-20231003195709970.png)

![image-20231003195812694](机器学习.assets/image-20231003195812694.png)

> - x1 和 x3 所在的虚线就是「间隔平面」；
> - x1 和 x3 之间的实线就是「分离平面」。

## 3  线性支持向量机

### 3.1  基本概念

「线性支持向量机」相比于「线性可分支持向量机」，具有一定的容忍度，能够允许出现误分的情况。

![image-20231003201954528](机器学习.assets/image-20231003201954528.png)

### 3.2  目标函数优化

![image-20231003203441601](机器学习.assets/image-20231003203441601.png)

![image-20231003204323764](机器学习.assets/image-20231003204323764.png)

![image-20231003204602821](机器学习.assets/image-20231003204602821.png)

## 4  非线性支持向量机

### 4.1  基本概念

在低维空间线性不可分的数据，可以通过特征映射到更高维的空间，并通过一个超平面来进行分隔，从而变为线性可分的数据。

![image-20231003205237215](机器学习.assets/image-20231003205237215.png)

将低维数据（原始空间）映射到高维的操作计算量很大，因此应找到一个核函数，使我们可以只用在低维空间进行计算，并达到与在高维计算同样的效果。

![image-20231003210554342](机器学习.assets/image-20231003210554342.png)

> 注意：在上图中，采用不同的映射函数，最后得到的核函数却都相同。

### 4.2  目标函数

「非线性支持向量机」的目标函数与「线性支持向量机」的目标函数相比，只需要把 x^(i)^·x^(j)^ 替换为核函数。

![image-20231003220726821](机器学习.assets/image-20231003220726821.png)

> 总结：非线性支持向量机是对前两种支持向量机的优化。

核函数：

![image-20231003221734995](机器学习.assets/image-20231003221734995.png)

> 注意：第二种核函数叫做高斯核函数，需要传入 σ 参数。

## 5  SMO算法

### 5.1  推导结果

![image-20231003223158900](机器学习.assets/image-20231003223158900.png)

### 5.2  代码实现

## 6  总结

![image-20231003231502595](机器学习.assets/image-20231003231502595.png)

# K-means

## 1  聚类

### 1.1  基本概念

聚类是一种无监督学习算法，可以把无分类（标签）的数据分为 k 类。

![image-20231006110928233](机器学习.assets/image-20231006110928233.png)

### 1.2  距离计算

![image-20231006111542936](机器学习.assets/image-20231006111542936.png)

### 1.3  代码实现

## 2  层次聚类

### 2.1  基本概念

![image-20231006113624202](机器学习.assets/image-20231006113624202.png)

### 2.2  距离计算

![image-20231006113956700](机器学习.assets/image-20231006113956700.png)

### 2.3  示例

![image-20231006114416670](机器学习.assets/image-20231006114416670.png)

### 2.4  代码实现

## 3  密度聚类

### 3.1  基本概念

![image-20231006115812491](机器学习.assets/image-20231006115812491.png)

> 概念说明：
>
> - 核心对象：若 x<sub>2</sub> 的 ε 邻域内的样本点数 >= Minpts，则 x<sub>2</sub> 是一个核心对象；
> - 密度直达：若 x<sub>2</sub> 是一个核心对象，且 x<sub>3</sub> 在 x<sub>2</sub> 的 ε 邻域内，则 x<sub>3</sub> 可由 x<sub>2</sub> 密度直达；
> - 密度可达：若 x<sub>3</sub> 可由 x<sub>1</sub> 密度直达，且 x<sub>1</sub> 可由 x<sub>2</sub> 密度直达，则 x<sub>3</sub> 从 x<sub>2</sub> 是密度可达的。

### 3.2  算法原理

![image-20231006121134054](机器学习.assets/image-20231006121134054.png)

### 3.3  代码实现

## 4  高斯混合模型

### 4.1  基本概念

![image-20231006160046282](机器学习.assets/image-20231006160046282.png)

![image-20231006160125818](机器学习.assets/image-20231006160125818.png)

### 4.2  参数估计

![image-20231006161236294](机器学习.assets/image-20231006161236294.png)

### 4.3  代码实现

## 5  主成分分析（PCA）

### 5.1  基本概念

![image-20231006163241900](机器学习.assets/image-20231006163241900.png)

![image-20231006163751392](机器学习.assets/image-20231006163751392.png)

### 5.2  分解算法

### 5.3  代码实现

# 集成学习

## 1  基本概念

![image-20231006164804282](机器学习.assets/image-20231006164804282.png)

集成学习的分类：

![image-20231006164844309](机器学习.assets/image-20231006164844309.png)

## 2  Voting

### 2.1  算法原理

![image-20231006165216656](机器学习.assets/image-20231006165216656.png)

### 2.2  硬投票分类器

![image-20231006165641174](机器学习.assets/image-20231006165641174.png)

### 2.3  软投票分类器

![image-20231006165958668](机器学习.assets/image-20231006165958668.png)

![image-20231006165928570](机器学习.assets/image-20231006165928570.png)

### 2.4  代码实现

## 3  Bagging

### 3.1  算法原理

![image-20231006170746972](机器学习.assets/image-20231006170746972.png)

> 优点：多个基分类器可以并行进行训练。

### 3.2  随机森林

当所有的基分类器为决策树时，此时的 Bagging 就是随机森林。

![image-20231006171001557](机器学习.assets/image-20231006171001557.png)

### 3.3  代码实现

## 4  Boosting

### 4.1  算法原理

![image-20231006171954437](机器学习.assets/image-20231006171954437.png)

### 4.2  AdaBoost

#### 4.2.1  算法原理

![image-20231006172358028](机器学习.assets/image-20231006172358028.png)

![image-20231006173013208](机器学习.assets/image-20231006173013208.png)

> 说明：m 代表训练样本的个数，D<sub>1</sub> 代表是第一轮训练，w 为样本的权重。

#### 4.2.2  示例

![image-20231006174004753](机器学习.assets/image-20231006174004753.png)

![image-20231006174043332](机器学习.assets/image-20231006174043332.png)

> 注意：这里样本 3 预测错误了，则在下一轮中会增加其所占的权重。

![image-20231006174122933](机器学习.assets/image-20231006174122933.png)

#### 4.2.3  代码实现

### 4.3  Boosting

#### 4.3.1  提升树

![image-20231006175935989](机器学习.assets/image-20231006175935989.png)

![image-20231006180158974](机器学习.assets/image-20231006180158974.png)

#### 4.3.2  梯度提升树

![image-20231006182942036](机器学习.assets/image-20231006182942036.png)

#### 4.3.2  代码实现

### 4.4  XGBoost

#### 4.4.1  算法原理

![image-20231006204213886](机器学习.assets/image-20231006204213886.png)

![image-20231006204521321](机器学习.assets/image-20231006204521321.png)

![image-20231006204640572](机器学习.assets/image-20231006204640572.png)

#### 4.4.2  求解

![image-20231006210149000](机器学习.assets/image-20231006210149000.png)

![image-20231006210214606](机器学习.assets/image-20231006210214606.png)

![image-20231006210619271](机器学习.assets/image-20231006210619271.png)

#### 4.4.3  树结构生成

![image-20231006210956048](机器学习.assets/image-20231006210956048.png)

![image-20231006211533719](机器学习.assets/image-20231006211533719.png)

#### 4.4.4  代码实现

## 5  Stacking

### 5.1  基本概念

![image-20231006213144297](机器学习.assets/image-20231006213144297.png)

### 5.2  代码实现
